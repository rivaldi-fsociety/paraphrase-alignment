[
{"type":"header","version":"4.9.4","comment":"Export to JSON plugin for PHPMyAdmin"},
{"type":"database","name":"parafras_dataset_corpus"},
{"type":"table","name":"view_hasil_rivaldi","database":"parafras_dataset_corpus","data":
[
{"id_case":"32","class":"","sentence1":"Recently, there has been a successful attempt to harmonize the linguistic principles behind the coding systems MSD and KR (Farkas et al., 2010).","sentence2":"Recently, there has been a successful attempt to harmonize the coding systems MSD and KR (Farkas et al., 2010)."},
{"id_case":"33","class":"","sentence1":"The second algorithm, denoted GloTr, is the Chu-Liu- Edmonds algorithm for maximal spanning tree implemented in the MSTParser (McDonald, 2006).","sentence2":"The second algorithm, denoted GloTr, is the Chu-Liu-Edmonds algorithm for maximal spanning tree implemented in the MSTParser (McDonald, 2006)."},
{"id_case":"34","class":"","sentence1":"The first one is the WS-353 dataset (Finkelstein et al., 2001) containing 353 pairs of English words that have been assigned similarity ratings by humans.","sentence2":"The first one is the WS-353 dataset (Finkelstein et al., 2001), which contains 353 pairs of English words that have been assigned similarity ratings by humans."},
{"id_case":"1201","class":"","sentence1":"We use Stanford parser (de Marneffe et al., 2006) to obtain parse trees and dependency relations.","sentence2":"We first use a dependency parser (de Marneffe et al., 2006) to parse each sentence and extract the set of dependency relations associated with the sentence ."},
{"id_case":"36","class":"","sentence1":"We perform bootstrap resampling with bounds estimation as described in (Koehn, 2004).","sentence2":"We perform bootstrap resampling with bounds estimation as described by (Koehn, 2004)."},
{"id_case":"37","class":"","sentence1":"It is used to support semantic analyses in HPSG English Resource  grammar ERG (Copestake and Flickinger, 2000), but also in other grammar formalisms like LFG.","sentence2":"It is used to support semantic analyses in the HPSG English Resource Grammar  (Copestake and Flickinger, 2000), but also in other grammar formalisms like LFG."},
{"id_case":"38","class":"","sentence1":"It is used to support semantic analyses in the English HPSG grammar ERG (Copestake and Flickinger, 2000), but also in other grammar formalisms like LFG.","sentence2":"It is used to support semantic analyses in the HPSG English Resource Grammar (Copestake and Flickinger, 2000), but also in other grammar formalisms like LFG."},
{"id_case":"39","class":"","sentence1":"It is used to support semantic analyses in HPSG English grammar ERG (Copestake and Flickinger, 2000), but also in other grammar formalisms like LFG.","sentence2":"It is used to support semantic analyses in the HPSG English Resource Grammar (Copestake and Flickinger, 2000), but also in other grammar formalisms like LFG."},
{"id_case":"40","class":"","sentence1":"It is used to support semantic analyses in the English HPSG grammar ERG (Copestake and Flickinger, 2000), but also in other grammar formalisms like LFG.","sentence2":"It is used to support semantic analyses in the HPSG English Resource Grammar  (Copestake and Flickinger, 2000), but also in other grammar formalisms like LFG."},
{"id_case":"42","class":"","sentence1":"Phrase-based SMT systems extend basic word-by-word SMT by splitting the translation process into 3 steps: the input source sentence is segmented into \" phrases \" or multiword units;","sentence2":"Phrase-based SMT systems extend the basic SMT word-by-word approach by splitting the translation process into 3 steps: the input source sentence is segmented into \" phrases \" or multi-word units; thes"},
{"id_case":"43","class":"","sentence1":"We build upon our previous Markov Logic based approach for joint concept disambiguation and clustering (Fahrni and Strube, 2012).","sentence2":"We build upon our previous approach for joint concept disambiguation and clustering (Fahrni and Strube, 2012)."},
{"id_case":"44","class":"","sentence1":"Details about SVM and KFD can be found in (Taylor and Cristianini, 2004).","sentence2":"Details about SVM and KRR can be found in (Taylor and Cristianini, 2004)."},
{"id_case":"45","class":"","sentence1":"Therefore, POS taggers for English tweets have been developed such as ARK, T-Pos and GATE TwitIE which reaches 92.8%, 88.4% and 89.37% accuracy respectively (Derczynski et al., 2013).","sentence2":"Therefore , POS taggers for English tweets have been developed such as ARK, T-Pos and GATE TwitIE which reach 92.8%, 88.4% and 89.37% accuracy respectively (Derczynski et al., 2013)."},
{"id_case":"46","class":"","sentence1":"We learn the parameters using a quasi-Newton procedure with L 1 (lasso) regularization (Andrew and Gao, 2007).","sentence2":"We learn the parameters  using a quasi-Newton (QN) procedure with l 1 (lasso) regularization (Andrew and Gao, 2007)."},
{"id_case":"47","class":"","sentence1":"We use the SCFG decoder cdec (Dyer et al., 2010) 4 and build grammars using its implementation of the suffix array extraction method described in Lopez (2007).","sentence2":"For direct translation, we use the SCFG decoder cdec (Dyer et al., 2010) 4 and build grammars using its implementation of the suffix array extraction method described in Lopez (2007)."},
{"id_case":"48","class":"","sentence1":"This is known as the Distributional Hypothesis (Harris, 1968).","sentence2":"This is known as the Distributional Hypothesis in linguistics (Harris, 1968)."},
{"id_case":"49","class":"","sentence1":"All our models , as well as the parser described in (Henderson, 2003), are run only once.","sentence2":"The models , as well as the parser described in (Henderson, 2003), are run only once."},
{"id_case":"51","class":"","sentence1":"For strings, many such kernel functions exist with various applications in computational biology and computational linguistics (Taylor and Cristianini, 2004).","sentence2":"For strings, a lot of such kernel functions exist with many applications in computational biology and computational linguistics (Taylor and Cristianini, 2004)."},
{"id_case":"53","class":"","sentence1":"The estimation of the semantically Smoothed Partial Tree Kernel (SPTK) is made available by an extended version of SVM-LightTK software 5 (Moschitti, 2006).","sentence2":"The estimation of the semantically Smoothed Partial Tree Kernel (SPTK), is made available by an extended version of SVM-LightTK software 7 (Moschitti, 2006)"},
{"id_case":"54","class":"","sentence1":"In the i2b2 2012 temporal challenge, all top performing teams used a combination of supervised classification and rule-based methods for extracting temporal information and relations (Sun et al., 2013).","sentence2":"In I2b2 2012 temporal challenge, all top performing teams used a combination of supervised classification and rule based methods for extracting temporal information and relations (Sun et al., 2013)."},
{"id_case":"56","class":"","sentence1":"In our experimental study, we use the freely available implementations in Weka (Witten and Frank, 2005).","sentence2":"In our experimental study, we use the freely available implementation of SVM in Weka (Witten and Frank, 2005)."},
{"id_case":"58","class":"","sentence1":"More recently, (Carpineto and Romano, 2010) showed that the characteristics of the outputs returned by SRC algorithms suggest the adoption of a meta clustering approach.","sentence2":"OPTIMSRC: (Carpineto and Romano, 2010) showed that the characteristics of the outputs returned by PRC algorithms suggest the adoption of a meta clustering approach."},
{"id_case":"59","class":"","sentence1":"They are based on the distributional hypothesis (Harris, 1968) and by looking at a set of event expressions whose argument fillers have a similar distribution, they try to recognize synonymous event.","sentence2":"These methods rely on the distributional hypothesis (Harris, 1968), and by looking at a set of event expressions whose argument fillers have a similar distribution, try to recognize synonymous event"},
{"id_case":"60","class":"","sentence1":"Word alignment is performed using GIZA++ (Och and Ney, 2003).","sentence2":"Word alignment is done using GIZA++ (Och and Ney, 2003)."},
{"id_case":"61","class":"","sentence1":"Word alignment is performed using GIZA++ (Och and Ney, 2003).","sentence2":"Word alignment is done using GIZA++ (Och and Ney, 2003)."},
{"id_case":"62","class":"","sentence1":"We used Mallet software (McCallum, 2002) for CRF experiments.","sentence2":"We used Mallet software (McCallum, 2002) for SSCRF experiments."},
{"id_case":"63","class":"","sentence1":"The thesaurus consists of a hierarchy of 2,710 semantic classes, defined for over 264,312 nouns, with a maximum depth of twelve (Ikehara et al., 1997).","sentence2":"The ontology, GoiTaikei, consists of a hierarchy of 2,710 semantic classes, defined for over 264,312 nouns, with a maximum depth of 12 (Ikehara et al., 1997)."},
{"id_case":"64","class":"","sentence1":"The detailed discussion is provided in the longer version of the paper (Kim et al., 2013).","sentence2":"A detailed discussion on the results is provided in the longer version of the paper (Kim et al., 2013)."},
{"id_case":"65","class":"","sentence1":"can be evaluated by maximizing the pseudo-likelihood on a training corpus see (Malouf, 2002).","sentence2":"can be evaluated by maximizing the pseudo-likelihood on a training corpus (see(Malouf, 2002))."},
{"id_case":"66","class":"","sentence1":"The first one is the WS- 353 3 dataset (Finkelstein et al., 2001) containing 353 pairs of English words that have been assigned similarity ratings by humans.","sentence2":"The first one is the WS-353 dataset (Finkelstein et al., 2001), which contains 353 pairs of English words that have been assigned similarity ratings by humans."},
{"id_case":"67","class":"","sentence1":"Following Blitzer et al. (2006) , we consider pivot features that appear more than 50 times in all the domains for SCL and mDA.","sentence2":"Following Blitzer et al. (2006) , we consider pivot features that appear more than 50 times in all the domains for SCL and mDA."},
{"id_case":"68","class":"","sentence1":"A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006), but like human evaluation , this is also a time consuming task.","sentence2":"A framework for human error analysis has been proposed in (Vilar et al., 2006), but as every human evaluation, this is also a time consuming task."},
{"id_case":"69","class":"","sentence1":"MaxEnt classifier is a good example of this group (Mani et al., 2006).","sentence2":"MaxEnt classifier is an example of this group (Mani et al., 2006)."},
{"id_case":"70","class":"","sentence1":"Among these media, blog is one of the communicative and informative repository of text based emotional contents in the Web 2.0 (Lin et al., 2007).","sentence2":"Blog is one of the crucial, communicative and informative repository of text based emotional contents in the Web 2.0 (Lin et al., 2007)."},
{"id_case":"71","class":"","sentence1":"For the gold preprocessing and all 5k settings, we refer the reader to the Shared Task overview paper (Seddah et al., 2013).","sentence2":""},
{"id_case":"73","class":"","sentence1":"For preprocessing, we used MADA (Morphological Analysis and Disambiguation for Arabic) (Habash et al., 2009) which is one of the most accurate Arabic preprocessing toolkits.","sentence2":"For this purpose, we use MADA (Morphological Analysis and Disambiguation for Arabic) (Habash et al., 2009) which is one of the most accurate Arabic preprocessing toolkits."},
{"id_case":"74","class":"","sentence1":"We trained a 5-gram language model on the Xinhua section of the English Gigaword corpus (306 million words) using the SRILM toolkit (Stolcke, 2002) with the modified Kneser-Ney smoothing.","sentence2":"Our 5-gram language model was trained on the Xinhua section of the English Gigaword corpus (306 million words) using the SRILM toolkit (Stolcke, 2002)  with modified Kneser-Ney smoothing."},
{"id_case":"75","class":"","sentence1":"To determine semantic type and subtype, we train two SVM multiclass classifiers using SVM multiclass (Tsochantaridis et al., 2004).","sentence2":"To determine semantic types and subtypes, we train two SVM multiclass classifiers using SVM multiclass (Tsochantaridis et al., 2004)."},
{"id_case":"76","class":"","sentence1":"We use the scikit implementation of Random Forest (Pedregosa et al., 2011).","sentence2":"We use the scikit implementation of SVM (Pedregosa et al., 2011)."},
{"id_case":"78","class":"","sentence1":"Each term in the input text will be represented by its stem and POS tag, in the following format (stem:POS) using Buckwalter transliteration (Buckwalter, 2002).","sentence2":"Each term in the input text is represented by its stem and POS tag using Buckwalter transliteration (Buckwalter, 2002)."},
{"id_case":"79","class":"","sentence1":"An algorithm, the Kuhn-Munkres method (Kuhn, 1955), can find solutions to the optimum assignment problem in polynomial time.","sentence2":"An algorithm, the Kuhn-Munkres method (Kuhn, 1955), has been proposed that can find a solution to the optimum assignment problem in polynomial time."},
{"id_case":"81","class":"","sentence1":"We use Collapsed Gibbs Sampling (Griffiths and Steyvers, 2004) to infer the parameters of the model and the latent violent categories and topics assignments for tweets, given observed data D. Gibbs.","sentence2":"We use collapsed Gibbs sampling (Griffiths and Steyvers, 2004) to infer the parameters of the model , given observed data D. Gibbs sampling is a Markov chain Monte Carlo method which allows us repeat."},
{"id_case":"82","class":"","sentence1":"We use the Stanford dependency parser (Marneffe et al., 2006).","sentence2":"We use the Stanford parser with Stanford dependencies (de Marneffe et al., 2006)."},
{"id_case":"83","class":"","sentence1":"Filter weights are initialized using Glorot-Bengio strategy (Glorot and Bengio, 2010).","sentence2":"Weights are initialized using Glorot-Bengio strategy (Glorot and Bengio, 2010)."},
{"id_case":"85","class":"","sentence1":"Automatic sentence alignment of the training data was provided by Ulrich German, and the hand alignments of the words in the test data were created by Franz Och and Hermann Ney (Och and Ney, 2003).","sentence2":"Automatic sentence alignment of the training data was provided by Ulrich German, and the hand alignments of the words in the trial and test data were created by Franz Och and Hermann Ney (Och and Ney, 2003)."},
{"id_case":"86","class":"","sentence1":"We use the AdaGrad optimizer (Duchi et al., 2011)  with initial learning rate set to 0.1.","sentence2":"We use AdaGrad (Duchi et al., 2011)  with the initial learning rate set to 0.5."},
{"id_case":"87","class":"","sentence1":"Then we did word alignment using GIZA++ (Och and Ney, 2003) with the default grow-diagfinal-and alignment symmetrization method.","sentence2":"We performed word alignment using GIZA++ (Och and Ney, 2003) with the default grow-diagfinal-and alignment symmetrization method."},
{"id_case":"88","class":"","sentence1":"For example, DIRT (Lin and Pantel, 2001) aims to discover different representations of the same semantic relation, i.e. similar dependency paths.","sentence2":"For example, DIRT (Lin and Pantel, 2001) aims to discover different representations of the same semantic relation using distributional similarity of dependency paths."},
{"id_case":"89","class":"","sentence1":"The annotation was performed manually using the brat annotation tool (Stenetorp et al., 2012).","sentence2":"The annotation was performed using the BRAT 2 tool (Stenetorp et al., 2012)."},
{"id_case":"91","class":"","sentence1":"System proposed by (Li et al., 2006), uses a semantic-vector approach to measure sentence similarity.","sentence2":"A similar semantic similarity measure, proposed by (Li et al., 2006), uses a semantic-vector approach to measure sentence similarity."},
{"id_case":"92","class":"","sentence1":"This corpus contains around 11,000 NPs annotated for information status including 663 bridging NPs and their antecedents in 50 texts taken from the WSJ portion of the OntoNotes corpus  (Weischedel et al., 2011).","sentence2":"It consists of 50 texts taken from the WSJ portion of the OntoNotes corpus (Weischedel et al., 2011) with almost 11,000 NPs annotated for information status including 663 bridging NPs and their antecedent."},
{"id_case":"93","class":"","sentence1":"All modules take as input the corpus documents preprocessed with a part-of-speech tagger 4 and shallow parser 5 (Punyakanok and Roth, 2001).","sentence2":"All components take as input the corpus documents preprocessed with a part-of-speech tagger 2 and shallow parser 3 (Punyakanok and Roth, 2001)."},
{"id_case":"95","class":"","sentence1":"From the pioneering work of (Rapp, 1995), contextual similarity has been used for BLE for a long time.","sentence2":"From the pioneering work of (Rapp, 1995), BLE from comparable corpora has been studied for a long time."},
{"id_case":"96","class":"","sentence1":"Europarl (Koehn, 2005) is a multilingual parallel corpus extracted from the proceedings of the European Parliament.","sentence2":"We run our experiments on Europarl (Koehn, 2005), a multilingual parallel corpus extracted from the proceedings of the European Parliament."},
{"id_case":"100","class":"","sentence1":"For building the baseline SMT system, we used the open-source SMT toolkit Moses (Koehn et al., 2007), in its standard setup.","sentence2":"For building our SMT systems, the open-source SMT toolkit Moses (Koehn et al., 2007) was used in its standard setup."},
{"id_case":"102","class":"","sentence1":"Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, represents texts by labeled hierarchical structures, called Discourse Trees (DTs).","sentence2":"Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, posits a tree representation of a discourse, known as a Discourse Tree (DT)."},
{"id_case":"103","class":"","sentence1":"In addition, the fix-discount method in (Foster et al., 2006) for phrase table smoothing is also used.","sentence2":"In addition, the fix-discount method (Foster et al., 2006) for phrase table smoothing was also used."},
{"id_case":"105","class":"","sentence1":"We calculate statistical significance of performance differences using stratified shuffling (Yeh, 2000).","sentence2":"We tested the significance of differences using stratified shuffling (Yeh, 2000)."},
{"id_case":"106","class":"","sentence1":"For instance, machine translation (MT) systems can benefit from training on sentences extracted from parallel or comparable documents retrieved from the Web (Munteanu and Marcu, 2005).","sentence2":"In addition, machine translation (MT) systems can be improved by training on sentences extracted from parallel or comparable documents mined from the Web (Munteanu and Marcu, 2005)."},
{"id_case":"109","class":"","sentence1":"1 with 2 -regularization using AdaGrad (Duchi et al., 2011).","sentence2":"1 -regularization using AdaGrad (Duchi et al., 2011)."},
{"id_case":"111","class":"","sentence1":"Why does the lr model outperform Berkeley 13 The MUC (Vilain et al., 1995) score is the minimum number of links between mentions to be inserted or deleted when mapping the output to a gold standard.","sentence2":"The MUC score (Vilain et al., 1995) counts the minimum number of links between mentions to be inserted or deleted when mapping a system response to a gold standard key set."},
{"id_case":"112","class":"","sentence1":"In the context of this paper we will be focusing on the SubSet Tree (SST) kernel described in (Collins and Duffy, 2002), which relies on a fragment definition that does not allow to break production.","sentence2":"We will focus on the syntactic tree kernel described in (Collins and Duffy, 2002), which relies on a fragment definition that does not allow to break production rules."},
{"id_case":"113","class":"","sentence1":"Europarl (Koehn, 2005) is a multilingual parallel corpus extracted from the proceedings of the European Parliament.","sentence2":"The Europarl corpus (Koehn, 2005) is built from the proceedings of the European Parliament."},
{"id_case":"114","class":"","sentence1":"We have used Foma, a free software tool to specify finite-state automata and transducers (Hulden, 2009).","sentence2":"The module was implemented using Foma, a free software tool to specify finite-state automata and transducers (Hulden, 2009)."},
{"id_case":"117","class":"","sentence1":"The Penn Discourse Treebank (PDTB, Prasad et al., 2008) is a large corpus annotated with discourse relations, covering the Wall Street Journal part of the Penn Treebank.","sentence2":"Penn Discourse Treebank The Penn Discourse Treebank (PDTB) is a corpus of Wall Street Journal articles annotated with discourse relations (Prasad et al., 2008)."},
{"id_case":"118","class":"","sentence1":"We used the implementation of the scikit-learn 2 module (Pedregosa et al., 2011).","sentence2":"We used a GB implementation of the scikit-learn package (Pedregosa et al., 2011)."},
{"id_case":"119","class":"","sentence1":"Since the commonly used word similarity datasets contain a small number of word pairs we also use the MEN dataset (Bruni et al., 2012) of 3,000 word pairs sampled from words that occur at least 700 times in a large web corpus.","sentence2":"The second is the MEN dataset (Bruni et al., 2012) of 3,000 words pairs sampled from words that occur at least 700 times in a large web corpus."},
{"id_case":"120","class":"","sentence1":"The training data of the shared task is the NUCLE corpus (Dahlmeier et al., 2013), which contains essays written by learners of English .","sentence2":"The training data released by the task organizers comes from the NUCLE corpus (Dahlmeier et al., 2013), which contains essays written by learners of English as a foreign language."},
{"id_case":"121","class":"","sentence1":"RTE is instead the task of deciding whether a long text T entails a shorter text, typically a single sentence, called hypothesis H. It has been often seen as a classification task  (Dagan et al., 2013).","sentence2":"RTE is the task of deciding whether a long text T entails a shorter text, typically a single sentence , called hypothesis H. It has been often seen as a classification task (see (Dagan et al., 2013))."},
{"id_case":"122","class":"","sentence1":"All system implementation was done using Python and the open-source machine learning toolkit scikit-learn (Pedregosa et al., 2011).","sentence2":"All of the machine learning was done using scikit-learn (Pedregosa et al., 2011)."},
{"id_case":"123","class":"","sentence1":"It has been shown that a diverse set of predictions can be used to help improve decoder accuracy for various problems in NLP (Henderson and Brill, 1999).","sentence2":"It has been long identified in NLP that a diverse set of solutions from a decoder can be reranked or recombined in order to improve the accuracy in various problems (Henderson and Brill, 1999)."},
{"id_case":"124","class":"","sentence1":"In the 2013 system, we had used SentiStrength lexicon (Thelwall et al., 2010).","sentence2":"In our system, we used the sentiment lexicon provided by SentiStrength (Thelwall et al., 2010)."},
{"id_case":"125","class":"","sentence1":"Finally, we also compare the quality of the candidate phrase embeddings with word embeddings (Dhillon et al., 2011) by adding them as features in a CRF based sequence tagger.","sentence2":"We also compared the quality of the candidate phrase embeddings with the word-level embeddings by adding them as features (Dhillon et al., 2011) along with the baseline features in the CRF tagger."},
{"id_case":"126","class":"","sentence1":"These methods are based on the distributional hypothesis which states that words appearing in the same contexts tend to have similar meaning (Harris, 1954).","sentence2":"Corpus-based VSMs follow the standard distributional hypothesis, which states that words appearing in the same contexts tend to have similar meaning (Harris, 1954)."},
{"id_case":"127","class":"","sentence1":"All annotations were done using the BRAT rapid annotation tool (Stenetorp et al., 2012).","sentence2":"The annotations were made using the BRAT rapid annotation tool (Stenetorp et al., 2012)."},
{"id_case":"128","class":"","sentence1":"Compared to WordNet (Fellbaum, 1998), there are similarities but also significant differences.","sentence2":"Compared to WordNet (Fellbaum, 1998), there are similarities as well as considerable differences."},
{"id_case":"131","class":"","sentence1":"For training, we use Adam (Kingma and Ba, 2015) for optimization with an initial learning rate of 0.001.","sentence2":"We use Adam (Kingma and Ba, 2015) for optimization with initial learning rate of 0.001."},
{"id_case":"132","class":"","sentence1":"For our classifier, we use SVMs, specifically the LIBLINEAR SVM software package (Fan et al., 2008), which is well-suited to text classification tasks with large numbers of features and large number","sentence2":"Specifically, we use the LIBLINEAR SVM package (Fan et al., 2008) as it is well-suited to text classification tasks with large numbers of features and texts."},
{"id_case":"133","class":"","sentence1":"The first two experiments concern the prediction of the sentiment of movie reviews in the Stanford Sentiment Treebank (Socher et al., 2013).","sentence2":"The first two experiments involve predicting the sentiment of movie reviews (Socher et al., 2013)."},
{"id_case":"136","class":"","sentence1":"We used only the non-ensembled left-to-right run (i.e. no right-to-left rescoring as done by Sennrich et al., 2016a) with beam size of 5, 5 taking just the single-best output.","sentence2":"We used only the non-ensembled left-to-right run (i.e. no right-to-left rescoring as done by Sennrich et al., 2016a) with beam size of 12 (default value)."},
{"id_case":"137","class":"","sentence1":"We used only the non-ensembled left-to-right run (Sennrich et al., 2016a) with beam size of 5, 5 taking just the single-best output.","sentence2":"We used only the non-ensembled left-to-right run (Sennrich et al., 2016a) with beam size of 12 (default value)."},
{"id_case":"138","class":"","sentence1":"The MSD morphological coding system was developed for a bunch of languages including Hungarian (Erjavec, 2004).","sentence2":"The MSD morphological coding system is a positional coding system developed for several languages (Erjavec, 2004)."},
{"id_case":"139","class":"","sentence1":"The phrase tables were generated by means of symmetrised word alignments obtained with GIZA++ (Och and Ney, 2003).","sentence2":"The phrase table was generated employing symmetrised word alignments obtained with GIZA++ (Och and Ney, 2003)."},
{"id_case":"140","class":"","sentence1":"Word alignment is performed using GIZA++ (Och and Ney, 2003).","sentence2":"Word alignment is performed with Giza++ (Och and Ney, 2003)."},
{"id_case":"141","class":"","sentence1":"We experimented with several levels of cluster granularity using development data, and following Koo et al. (2008).","sentence2":"Following Koo et al. (2008) , we also experimented with using two sets of cluster labels with different levels of granularity."},
{"id_case":"144","class":"","sentence1":"We built a modified Kneser-Ney smoothed 5-gram language model using the English side of the training data and performed querying with KenLM (Heafield, 2011).","sentence2":"Training and querying of a modified Kneser-Ney smoothed 5- gram language model are done on the English side of the training data using KenLM (Heafield, 2011) ."},
{"id_case":"146","class":"","sentence1":"A formal PAC-style analysis can be found in (Ando and Zhang, 2004).","sentence2":"The formal derivation can be found in (Ando and Zhang, 2004)."},
{"id_case":"148","class":"","sentence1":"The SMT systems were built using the Moses toolkit (Koehn et al., 2007).","sentence2":"The baseline systems are built with the opensource phrase-based SMT toolkit Moses (Koehn et al., 2007)."},
{"id_case":"149","class":"","sentence1":"The classifier experiments were carried out using the SVM-light software (Joachims, 1999) available at http:\/\/svmlight.joachims.org\/ with a polynomial kernel 2 (degree=3).","sentence2":"The classifier evaluations were carried out using the SVM-light software (Joachims, 1999) available at http:\/\/svmlight.joachims.org\/ with the default linear kernel for the standard feature evaluation."},
{"id_case":"150","class":"","sentence1":"The training data of the shared task is the NUCLE corpus (Dahlmeier et al., 2013), which contains essays written by learners of English.","sentence2":"The training data for the task is from the NUCLE corpus (Dahlmeier et al., 2013), an error-tagged collection of essays written by non-native learners of English."},
{"id_case":"151","class":"","sentence1":"SALDO (Borin et al., 2013) is the largest freely available lexical resource for Swedish.","sentence2":"SALDO (Borin et al., 2013) is the most comprehensive open lexical resource for Swedish."},
{"id_case":"152","class":"","sentence1":"For the phrase-based SMT system, we adopted the Moses toolkit (Koehn et al., 2007).","sentence2":"For our SMT experiments, we use the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"153","class":"","sentence1":"However, those string-to-tree systems run slowly in cubic time (Huang et al., 2006).","sentence2":"However such string-to-tree systems run slowly in cubic time (Huang et al., 2006)."},
{"id_case":"154","class":"","sentence1":"The 5-gram target language model was trained using KenLM (Heafield, 2011).","sentence2":"We trained an English 5-gram language model using KenLM (Heafield, 2011)."},
{"id_case":"155","class":"","sentence1":"For all experiments, we used the Moses SMT system (Koehn et al., 2007).","sentence2":"For our SMT experiments, we use the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"156","class":"","sentence1":"We evaluate our method on the following data sets: @BULLET OntoNotes-Dev: Development set of the OntoNotes data provided by the CoNLL2012 shared task (Pradhan et al., 2012).","sentence2":"@BULLET OntoNotes-Test: Test set of the OntoNotes data provided by the CoNLL2012 shared task (Pradhan et al., 2012)."},
{"id_case":"157","class":"","sentence1":"We use the Moses phrase-based translation system (Koehn et al., 2007) to implement our models.","sentence2":"We use the state-of-the-art phrase-based machine translation system Moses (Koehn et al., 2007) to perform our machine translation experiments."},
{"id_case":"159","class":"","sentence1":"The BLEU score measures the precision of n-grams (over all n to 4 in our case) with respect to a reference translation with a penalty for short translations (Papineni et al., 2001).","sentence2":"BLEU score: This score measures the precision of unigrams, bigrams, trigrams and fourgrams with respect to a whole set of reference translations with a penalty for too short sentences (Papineni et al., 2001)."},
{"id_case":"160","class":"","sentence1":"The training data of the shared task is the NUCLE corpus (Dahlmeier et al., 2013), which contains essays written by learners of English.","sentence2":"The training data provided for the task is a subset of the NUCLE v2.3 corpus (Dahlmeier et al., 2013), which comprises essays written in English by students at the National University of Singapore ."},
{"id_case":"161","class":"","sentence1":"Test data was drawn from the Open American National Corpus (Ide and Suderman, 2004 , OANC) across a variety of genres and from both the spoken and written portions of the corpus.","sentence2":"We selected the dataset of Jurgens and Klapaftis (2013), which was drawn from the Open American National Corpus (OANC) (Ide and Suderman, 2004) across a variety of genres and from both the spoken and written portions of the corpus."},
{"id_case":"162","class":"","sentence1":"We measure statistical significance using 95% confidence intervals computed with paired bootstrap resampling (Koehn, 2004).","sentence2":"The statistical significance tests using 95% confidence interval are measured with paired bootstrap resampling (Koehn, 2004)."},
{"id_case":"163","class":"","sentence1":"We use the English portion of the ACE 2005 relation extraction dataset (Walker et al., 2006).","sentence2":"We evaluate our relation extraction system on the English portion of the ACE 2005 corpus (Walker et al., 2006)."},
{"id_case":"164","class":"","sentence1":"This data was collected for the 2014 SemEval competition (Marelli et al., 2014) and consists of 9,927 sentence pairs, with 4,500 for training, 500 as a development set.","sentence2":"Sentences Involving Compositional Knowledge (SICK) is from Task 1 of the 2014 SemEval competition (Marelli et al., 2014) and consists of 9,927 annotated sentence pairs, with 4,500 for training, 500 as a development set."},
{"id_case":"164","class":"","sentence1":"This data was collected for the 2014 SemEval competition (Marelli et al., 2014) and consists of 9,927 sentence pairs, with 4,500 for training, 500 as a development set.","sentence2":"Sentences Involving Compositional Knowledge (SICK) is from Task 1 of the 2014 SemEval competition (Marelli et al., 2014) and consists of 9,927 annotated sentence pairs, with 4,500 for training, 500 as a development set."},
{"id_case":"165","class":"","sentence1":"The parsing model used for intra-sentential parsing is a Dynamic Conditional Random Field (DCRF) (Sutton et al., 2007) shown in Figure 7 .","sentence2":"Our novel parsing model is the Dynamic Conditional Random Field (DCRF) (Sutton et al., 2007) shown in Figure 2 ."},
{"id_case":"166","class":"","sentence1":"Latent Dirichlet Allocation (LDA) is a generative model which considers a document model (Salton, 1989) as a mixture probability of latent topics .","sentence2":"Combination of latent topics ent Dirichlet Allocation (LDA) is a generative model which considers a document model (seen as a bag of words (Salton, 1989)) as a mixture probability of latent topics."},
{"id_case":"167","class":"","sentence1":"Distributional hypothesis theory (Harris, 1954) indicates that words that occur in the same context tend to have similar meanings.","sentence2":"It therefore follows the distributional hypothesis (Harris, 1954) which states that words that occur in the same contexts tend to have similar meanings."},
{"id_case":"168","class":"","sentence1":"Distributional hypothesis theory (Harris, 1954) indicates that words that occur in the same context tend to have similar meanings.","sentence2":"It therefore follows the distributional hypothesis (Harris, 1954) which states that words that occur in the same contexts tend to have similar meanings."},
{"id_case":"169","class":"","sentence1":"In 2009, Yefang Wang (Wang et al., 2009) used cascading classifiers on manually annotated data which fetched F-score of 0.832.","sentence2":"In 2009, Yefang Wang (Wang et al., 2009) used cascading classifiers on manually annotated data and achieved around 83.2% accuracy."},
{"id_case":"170","class":"","sentence1":"We built a 5-gram language model on the English side of QCA-train using KenLM (Heafield, 2011).","sentence2":"We built a modified Kneser-Ney smoothed 5-gram language model using the English side of the training data and performed querying with KenLM (Heafield, 2011)."},
{"id_case":"171","class":"","sentence1":"The remaining three models are all Naive Bayes classifiers trained on the Google Web 1T 5-gram corpus (henceforth, Google corpus , (Brants and Franz, 2006)).","sentence2":"The other models are trained on native English data, the Google Web 1T 5-gram corpus (henceforth, Google, (Brants and Franz, 2006)) with the Naive Bayes (NB) algorithm."},
{"id_case":"172","class":"","sentence1":"We apply bootstrapping (Kozareva et al., 2008) on the word graphs by manually selecting 10 seeds for concrete and abstract words (see Table 10).","sentence2":"We then apply bootstrapping (Kozareva et al., 2008) on the noun and adjective graphs by selecting 10 seeds for visual and non-visual nouns and adjectives (see Table 1)."},
{"id_case":"173","class":"","sentence1":"These methods are based on the distributional hypothesis which states that words appearing in the same contexts tend to have similar meaning (Harris, 1954).","sentence2":"It therefore follows the distributional hypothesis (Harris, 1954) which states that words that occur in the same contexts tend to have similar meanings."},
{"id_case":"175","class":"","sentence1":"RG-65: (Rubenstein and Goodenough, 1965) has 65 word pairs.","sentence2":"RG-65: (Rubenstein and Goodenough, 1965) is set of 65 word pairs."},
{"id_case":"176","class":"","sentence1":"The significance tests were performed using the bootstrap resampling method (Koehn, 2004).","sentence2":"Statistical significance tests are performed using bootstrap resampling (Koehn, 2004)."},
{"id_case":"177","class":"","sentence1":"The default Phrasal search algorithm is cube pruning (Huang and Chiang, 2007).","sentence2":"The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007)."},
{"id_case":"178","class":"","sentence1":"In-domain data is mainly used to solve the problem of data sparseness (Sun and Xu, 2011).","sentence2":"In-domain data only solves the problem of data sparseness (Sun and Xu, 2011)."},
{"id_case":"179","class":"","sentence1":"All experiments were carried out using the open-source SMT toolkit Moses (Koehn et al., 2007).","sentence2":"All the experiments are carried out in Moses toolkit (Koehn et al., 2007)."},
{"id_case":"180","class":"","sentence1":"First, we apply heuristics to determine number and gender based on word lists, WordNet (Miller, 1990) and part-of-speech tags.","sentence2":"We then apply heuristics to determine number and gender for the characters based on word lists, Word- Net (Miller, 1990) and POS tags."},
{"id_case":"181","class":"","sentence1":"Rank SVM (Joachims, 2002) is a method based on Support Vector Machines (SVMs) for which we use only linear kernels to keep complexity low.","sentence2":"For this task we use RankSVM (Joachims, 2002) which is a method based on Support Vector Machines (SVMs)."},
{"id_case":"182","class":"","sentence1":"A more detailed description of the task can be found in (Nakov et al., 2017).","sentence2":"A precise description of the corpus and metrics can be found in Task3 description paper (Nakov et al., 2017)."},
{"id_case":"184","class":"","sentence1":"A Tree Kernel function is a convolution kernel (Haussler, 1999) defined over pairs of trees.","sentence2":"Tree Kernel (TK) functions are convolution kernels (Haussler, 1999) defined over pairs of trees."},
{"id_case":"185","class":"","sentence1":"We build upon our previous approach for joint concept disambiguation and clustering (Fahrni and Strube, 2012).","sentence2":"For disambiguation and clustering we build upon our previous work (Fahrni and Strube, 2012)."},
{"id_case":"186","class":"","sentence1":"ROUGE-2 metric (Lin, 2004) is used for the evaluation .","sentence2":"We used the ROUGE-1 evaluation metric (Lin, 2004)."},
{"id_case":"187","class":"","sentence1":"We used Mallet software (McCallum, 2002) for CRF experiments.","sentence2":"We used Mallet toolkit (McCallum, 2002) for CRF implementation ."},
{"id_case":"188","class":"","sentence1":"We exploit a transition-based framework with global learning and beam-search decoding to implement the joint model (Zhang and Clark, 2011).","sentence2":"Our joint parsing model exploits a transition-based framework with global learning and beam-search decoding (Zhang and Clark, 2011), extended from a arc-standard transition-based parsing model."},
{"id_case":"189","class":"","sentence1":"The annotation was performed using the BRAT 2 tool (Stenetorp et al., 2012).","sentence2":"The annotations were made using the BRAT rapid annotation tool (Stenetorp et al., 2012)."},
{"id_case":"190","class":"","sentence1":"For building the word alignment models we use MGIZA++ (Gao and Vogel, 2008).","sentence2":"To build the word alignment models we used the MGIZA++ package (Gao and Vogel, 2008)."},
{"id_case":"191","class":"","sentence1":"The reliability of the annotation was evaluated using the kappa statistic (Carletta, 1996).","sentence2":"We evaluated annotation reliability by using the Kappa statistic (Carletta, 1996)."},
{"id_case":"192","class":"","sentence1":"It therefore follows the distributional hypothesis (Harris, 1954) which states that words that occur in the same contexts tend to have similar meanings.","sentence2":"Distributional models of meaning follow the distributional hypothesis (Harris, 1954), which states that two words that occur in similar contexts have similar meanings."},
{"id_case":"194","class":"","sentence1":"The significance tests were performed using the bootstrap resampling method (Koehn, 2004).","sentence2":"A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004)."},
{"id_case":"195","class":"","sentence1":"We use the standard alignment tool Giza++ (Och and Ney, 2003) to word align the parallel data.","sentence2":"We use the Giza++ tool (Och and Ney, 2003) to align words in our parallel corpora."},
{"id_case":"196","class":"","sentence1":"We use the standard alignment tool Giza++ (Och and Ney, 2003) to word align the parallel data.","sentence2":"We use the Giza++ tool (Och and Ney, 2003) to align words in our parallel corpora."},
{"id_case":"198","class":"","sentence1":"The kernels are combined using Gaussian process regression (GPR) (Rasmussen and Williams, 2006).","sentence2":"Gaussian process regression (GPR) (Rasmussen and Williams, 2006)."},
{"id_case":"200","class":"","sentence1":"We measure statistical significance using 95% confidence intervals computed with paired bootstrap resampling (Koehn, 2004).","sentence2":"We calculated significance using paired bootstrap resampling (Koehn, 2004)."},
{"id_case":"701","class":"","sentence1":"We use the Stanford dependency parser (Chen and Manning, 2014) at this stage, and have not experimented with alternatives .","sentence2":"In this work, we use the Stanford neural dependency parser (Chen and Manning, 2014)."},
{"id_case":"702","class":"","sentence1":"Next, a tweet was tokenized and fed into MADAMIRA (Pasha et al., 2014), a morphological analysis tool for Arabic text.","sentence2":"MADAMIRA (Pasha et al., 2014) is a morphological analysis and disambiguation tool of Arabic ."},
{"id_case":"703","class":"","sentence1":"(Yarowsky ,(1995)) has proposed a bootstrapping method for word sense disambiguation .","sentence2":"Yarowsky (1995) proposed such a method for word sense disambiguation, which we refer to as monolingual bootstrapping."},
{"id_case":"704","class":"","sentence1":"We also list the previous state-of-the-art performance from a conventional SMT system (Durrani et al., 2014) with the BLEU of 37.0.","sentence2":"We also list the results from SMT model (Durrani et al., 2014) as a comparison."},
{"id_case":"705","class":"","sentence1":"We used Adam (Kingma and Ba, 2014) with a learning rate of 0.0002.","sentence2":"We use the Adam (Kingma and Ba, 2014) algorithm to minimize the sum of the loss for p asv and p a,F , with a learning rate of 10."},
{"id_case":"706","class":"","sentence1":"Distributional semantics is based on the idea that (Firth, 1957) in other words, the meaning of a word is related to the contexts it appears in.","sentence2":"Word co-occurence statistics \" You shall know a word by the company it keeps \" (Firth, 1957)."},
{"id_case":"707","class":"","sentence1":"In order to estimate the basic lexical similarity function employed in the SUM, SSC and SPTK operators, a co-occurrence Word Space is acquired through the distributional analysis of the UkWaC corpus (Baroni et al., 2009).","sentence2":"The co-occurrence Word Space is acquired through the distributional analysis of the UkWaC corpus (Baroni et al., 2009)."},
{"id_case":"709","class":"","sentence1":"We used the implementation of the scikit-learn 2 module (Pedregosa et al., 2011).","sentence2":"We used the scikit-learn toolkit to train our classifiers (Pedregosa et al., 2011)."},
{"id_case":"710","class":"","sentence1":"The translation model was trained by GIZA++ (Och and Ney, 2003), and the trigram was trained by the CMU-Cambridge Statistical Language Modeling Toolkit v2 (Clarkson and Rosenfeld, 1997).","sentence2":"The probability p(E) is computed using a simple trigram language model that was trained using the CMU Language Modeling Toolkit (Clarkson and Rosenfeld, 1997)."},
{"id_case":"711","class":"","sentence1":"This is a generalization of the operator Id in (Kaplan and Kay, 1994).","sentence2":"This is similar to the operator Intro in (Kaplan and Kay, 1994)."},
{"id_case":"712","class":"","sentence1":"We used standard classifiers available in scikit-learn package (Pedregosa et al., 2011).","sentence2":"We used a GB implementation of the scikit-learn package (Pedregosa et al., 2011)."},
{"id_case":"714","class":"","sentence1":"We used the implementation of the scikit-learn 2 module (Pedregosa et al., 2011).","sentence2":"For the linear logistic regression implementation we used scikit-learn (Pedregosa et al., 2011)."},
{"id_case":"715","class":"","sentence1":"For the phrase-based SMT system, we adopted the Moses toolkit (Koehn et al., 2007).","sentence2":"Finally, we used Moses toolkit as phrase-based reference (Koehn et al., 2007)."},
{"id_case":"716","class":"","sentence1":"4 Word alignments are created by aligning the data in both directions with GIZA++ 5 and symmetrizing the two trained alignments (Och and Ney, 2003).","sentence2":"Baseline word alignments were obtained by running GIZA++ in both directions and symmetrizing using the grow-diag-final-and heuristic (Och and Ney, 2003;)."},
{"id_case":"717","class":"","sentence1":"The perplexity achieved by the 6- gram NN LM in the Spanish news-test08 development set was 116, versus 94 obtained with a standard 6-gram language model with interpolation and Kneser-Ney smoothing (Kneser and Ney, 1995).","sentence2":"The language model is a 5-gram with interpolation and Kneser-Ney smoothing (Kneser and Ney, 1995)."},
{"id_case":"718","class":"","sentence1":"We used standard classifiers available in scikit-learn package (Pedregosa et al., 2011).","sentence2":"We used the scikit-learn toolkit to train our classifiers (Pedregosa et al., 2011)."},
{"id_case":"719","class":"","sentence1":"Statistical machine translation is typically performed using phrase-based systems (Koehn et al., 2007).","sentence2":"It is a standard phrase-based machine translation model (Koehn et al., 2007)."},
{"id_case":"720","class":"","sentence1":"WordNet (Miller et al., 1990) is an on-line hierarchical lexical database which contains semantic information about English words.","sentence2":"The WordNet on-line lexical database (Miller et al., 1990)."},
{"id_case":"721","class":"","sentence1":"We use the scikit implementation of Random Forest (Pedregosa et al., 2011).","sentence2":"We used a GB implementation of the scikit-learn package (Pedregosa et al., 2011)."},
{"id_case":"724","class":"","sentence1":"We lemmatise the head of each constituent with TreeTagger (Schmid, 1994).","sentence2":"We used the Stuttgart TreeTagger (Schmid, 1994) to lemmatise constituent heads."},
{"id_case":"725","class":"","sentence1":"Our text processing uses the Natural Language Toolkit (NLTK) (Bird et al., 2009).","sentence2":"Part-of-speech tagging was accomplished using the Natural Language Toolkit (NLTK) (Bird et al., 2009)."},
{"id_case":"726","class":"","sentence1":"They used the Web-based annotation tool brat (Stenetorp et al., 2012) for the annotation .","sentence2":"The annotation was performed manually using the brat annotation tool(Stenetorp et al., 2012)."},
{"id_case":"729","class":"","sentence1":"Our system participated in SemEval-2013 Task 2: Sentiment Analysis in Twitter (Wilson et al., 2013).","sentence2":"We participated in both subtask A and B of SemEval-2013 Task 2: Sentiment Analysis in Twitter (Wilson et al., 2013) with an adaptation of our existing system."},
{"id_case":"730","class":"","sentence1":"The English text was tokenized using the word tokenize routine from NLTK (Bird et al., 2009).","sentence2":"We tokenise the text using the default tokeniser from NLTK (Bird et al., 2009)."},
{"id_case":"731","class":"","sentence1":"The webpages were parsed using the Stanford CoreNLP software (Manning et al., 2014).","sentence2":"In addition, the data was tokenized, lemmatized, and parsed using Stanford CoreNLP (Manning et al., 2014)."},
{"id_case":"732","class":"","sentence1":"Statistical machine translation is typically performed using phrase-based systems (Koehn et al., 2007).","sentence2":"We built phrase-based machine translation systems using the open software toolkit Moses (Koehn et al., 2007)."},
{"id_case":"733","class":"","sentence1":"The English side was tokenized using the Moses toolkit (Koehn et al., 2007).","sentence2":"The corpora are tokenised and truecased using scripts from the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"734","class":"","sentence1":"We trained an English 5-gram language model using KenLM (Heafield, 2011).","sentence2":"We built a trigram language model with Kneser-Ney smoothing using KenLM toolkit (Heafield, 2011)."},
{"id_case":"735","class":"","sentence1":"All of the text data from Reddit was tokenized using the NLTK tokenizer (Bird et al., 2009).","sentence2":"We tokenise the text using the default tokeniser from NLTK (Bird et al., 2009)."},
{"id_case":"736","class":"","sentence1":"Our machine translation systems are trained using Moses 3 (Koehn et al., 2007).","sentence2":"We built phrase-based machine translation systems using the open software toolkit Moses (Koehn et al., 2007)."},
{"id_case":"737","class":"","sentence1":"For the phrase-based SMT system, we adopted the Moses toolkit (Koehn et al., 2007).","sentence2":"For training the translation model and for decoding we used the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"738","class":"","sentence1":"We build upon our previous approach for joint concept disambiguation and clustering (Fahrni and Strube, 2012).","sentence2":"Scope-ignorant (Disambig.): Our previous MLN-based approach for concept disambiguation (Fahrni and Strube, 2012)."},
{"id_case":"739","class":"","sentence1":"We used Adam (Kingma and Ba, 2014) with a learning rate of 0.0002 ().","sentence2":"We use ADAM (Kingma and Ba, 2014) with a learning rate of 4e?4 and a batch size of 32."},
{"id_case":"740","class":"","sentence1":"For all experiments, we used the Moses SMT system (Koehn et al., 2007).","sentence2":"For training the translation model and for decoding we used the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"741","class":"","sentence1":"The SMT systems were built using the Moses toolkit (Koehn et al., 2007).","sentence2":"The corpora are tokenised and truecased using scripts from the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"742","class":"","sentence1":"For training the translation model and for decoding we used the Moses toolkit (Koehn et al., 2007).","sentence2":"For our SMT experiments, we use the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"743","class":"","sentence1":"For training the translation model and for decoding we used the Moses toolkit (Koehn et al., 2007).","sentence2":"First, we used the Moses toolkit (Koehn et al., 2007) for statistical machine translation."},
{"id_case":"744","class":"","sentence1":"For the phrase-based SMT system, we adopted the Moses toolkit (Koehn et al., 2007).","sentence2":"The baseline systems are built with the opensource phrase-based SMT toolkit Moses (Koehn et al., 2007)."},
{"id_case":"745","class":"","sentence1":"We develop translation models using the phrase-based Moses (Koehn et al., 2007) SMT system.","sentence2":"We built phrase-based machine translation systems using the open software toolkit Moses (Koehn et al., 2007)."},
{"id_case":"746","class":"","sentence1":"We used TnT (Brants, 2000), trained on the Negra training set.","sentence2":"We employed the TnT tagger (Brants, 2000) which was trained on the spective CoNLL training data."},
{"id_case":"747","class":"","sentence1":"The webpages were parsed using the Stanford CoreNLP software (Manning et al., 2014).","sentence2":"In addition, the data was tokenized, lemmatized, and parsed using Stanford CoreNLP (Manning et al., 2014)."},
{"id_case":"748","class":"","sentence1":"We develop translation models using the phrase-based Moses (Koehn et al., 2007) SMT system.","sentence2":"We build a state of the art phrase-based SMT system using Moses (Koehn et al., 2007)."},
{"id_case":"750","class":"","sentence1":"The term frequency count is normalized with the inverse document frequency in the test collection (Salton and Buckley, 1988).","sentence2":"TF-IDF is a standard statistical method that combines the frequency of a term in a particular document with its inverse document frequency in general use (Salton and Buckley, 1988)."},
{"id_case":"751","class":"","sentence1":"We assessed the statistical significance of differences in score with an approximate randomization test 8 (Noreen, 1989), indicating a significant impact in bold font.","sentence2":"We assess statistical significance of the difference in F 1 score for two approaches via an approximate randomization test (Noreen, 1989)."},
{"id_case":"752","class":"","sentence1":"A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out.","sentence2":"A framework for human error analysis has been proposed in (Vilar et al., 2006), but as every human evaluation, this is also a time consuming task."},
{"id_case":"753","class":"","sentence1":"For example, Chang et al. (2009) found that the probability of held-out documents is not always a good predictor of human judgments.","sentence2":"Chang et al. (2009) stated that one reason is that the objective function of topic models does not always correlate well with human judgments."},
{"id_case":"755","class":"","sentence1":"conducted using the Moses phrase-based decoder (Koehn et al., 2007).","sentence2":"We build a state of the art phrase-based SMT system using Moses (Koehn et al., 2007)."},
{"id_case":"756","class":"","sentence1":"conducted using the Moses phrase-based decoder (Koehn et al., 2007).","sentence2":"We built phrase-based machine translation systems using the open software toolkit Moses (Koehn et al., 2007)."},
{"id_case":"757","class":"","sentence1":"We conducted baseline experiments for phrasebased machine translation using the Moses toolkit (Koehn et al., 2007).","sentence2":"conducted using the Moses phrase-based decoder (Koehn et al., 2007)."},
{"id_case":"759","class":"","sentence1":"Then the processed data was performed for tokenization, POS tagging, parsing, stemming and lemmatization using Stanford CoreNLP (Manning et al., 2014).","sentence2":"In addition, the data was tokenized, lemmatized, and parsed using Stanford CoreNLP (Manning et al., 2014)."},
{"id_case":"760","class":"","sentence1":"We applied bootstrap resampling (Koehn, 2004) to measure statistical significance , p < 0.05, of our models compared to a baseline.","sentence2":"We measure significance of results using bootstrap resampling at p < 0.05 (Koehn, 2004)."},
{"id_case":"762","class":"","sentence1":"The language model is a 5-gram KenLM (Heafield, 2011) model, trained using lmplz, with modified Kneser-Ney smoothing and no pruning.","sentence2":"We built a trigram language model with Kneser-Ney smoothing using KenLM toolkit (Heafield, 2011)."},
{"id_case":"763","class":"","sentence1":"Weighted Finite State Transducers (FSTs) used in our model are constructed with OpenFst (Allauzen et al., 2007).","sentence2":"The decoder is implemented with Weighted Finite State Transducers (WFSTs) using standard operations available in the OpenFst libraries (Allauzen et al., 2007)."},
{"id_case":"764","class":"","sentence1":"Weighted Finite State Transducers (FSTs) used in our model are constructed with OpenFst (Allauzen et al., 2007).","sentence2":"The decoder is implemented with Weighted Finite State Transducers (WFSTs) using standard operations available in the OpenFst libraries (Allauzen et al., 2007)."},
{"id_case":"765","class":"","sentence1":"Then the processed data was performed for tokenization, POS tagging, parsing, stemming and lemmatization using Stanford CoreNLP (Manning et al., 2014).","sentence2":"In addition, the data was tokenized, lemmatized, and parsed using Stanford CoreNLP (Manning et al., 2014)."},
{"id_case":"767","class":"","sentence1":"We use Scikit-learn (Pedregosa et al., 2011), the machine learning library for Python, for implementing the different approaches.","sentence2":"We used the Scikit-learn machine learning library (Pedregosa et al., 2011) for both implementing our classification models and performing statistical feature selection."},
{"id_case":"769","class":"","sentence1":"The CRF is trained using decisions from the following underlying components: @BULLET MADAMIRA: is a publicly available tool for morphological analysis and disambiguation of EDA and MSA text (Pasha et al., 2014)","sentence2":"MADAMIRA (Pasha et al., 2014) is a morphological analysis and disambiguation tool of Arabic ."},
{"id_case":"770","class":"","sentence1":"For language modeling, we trained a separate 5-gram Kneser-Ney smoothed LM model on the target (i.e. English) side of the training bi-text using KenLM (Heafield, 2011).","sentence2":"We built a modified Kneser- Ney smoothed 5-gram language model using the English side of the training data and performed querying with KenLM (Heafield, 2011) 7 ."},
{"id_case":"771","class":"","sentence1":"with the training script of the Moses toolkit (Koehn et al., 2007).","sentence2":"We preprocessed the training corpora with scripts included in the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"772","class":"","sentence1":"The word alignment was trained using GIZA++ (Och and Ney, 2003) with the configuration grow-diag-final-and alignment symmetrization method.","sentence2":"We performed word alignment using GIZA++ (Och and Ney, 2003) with the default grow-diagfinal-and alignment symmetrization method."},
{"id_case":"773","class":"","sentence1":"We use linear SVMs from LIBLINEAR and SVMs with RBF kernel from LIBSVM (Chang and Lin, 2011).","sentence2":"As our learner, we use LIBSVM with a linear kernel (Chang and Lin, 2011)."},
{"id_case":"774","class":"","sentence1":"We specify the hierarchical aligner in terms of a deduction system (Shieber et al., 1995).","sentence2":"We specify our dynamic programming algorithm as a deduction system (Shieber et al., 1995)."},
{"id_case":"775","class":"","sentence1":"We use Stanford parser (de Marneffe et al., 2006) to obtain parse trees and dependency relations.","sentence2":"We use the Stanford dependency parser (de Marneffe et al., 2006) for extracting dependency path and part-of-speech features."},
{"id_case":"776","class":"","sentence1":"In this work, we use the Stanford neural dependency parser (Chen and Manning, 2014).","sentence2":"In order to detect the object pronouns, we employ Stanford parser (Chen and Manning, 2014)."},
{"id_case":"778","class":"","sentence1":"We use Stanford parser (de Marneffe et al., 2006) to obtain parse trees and dependency relations.","sentence2":"We also obtain the dependency parse of the sentences using the Stanford parser (De Marneffe et al., 2006)."},
{"id_case":"779","class":"","sentence1":"The learning algorithm used in our coreference engine is C4.5 (Quinlan, 1993).","sentence2":"The question classifier used in the experiments is the C4.5 decision tree classifier (Quinlan, 1993)."},
{"id_case":"780","class":"","sentence1":"We use Stanford parser (de Marneffe et al., 2006) to obtain parse trees and dependency relations.","sentence2":"We use the Stanford dependency parser (de Marneffe et al., 2006) for extracting dependency path and part-of-speech features."},
{"id_case":"781","class":"","sentence1":"Distributional semantics (see Cohen and Widdows (2009) for an overview is based on the observation that words that occur in similar contexts tend to be semantically related (Harris, 1954).","sentence2":"The former that is the most popular relies on the distributional hypothesis that puts forward the idea that words with similar meaning tend to occur in similar contexts (Harris, 1954)."},
{"id_case":"782","class":"","sentence1":"Their work is part of the state-of-the-art Arabic morphological tagger MADAMIRA (Pasha et al., 2014).","sentence2":"MADAMIRA (Pasha et al., 2014) is a morphological analysis and disambiguation tool of Arabic ."},
{"id_case":"783","class":"","sentence1":"We use Stanford parser (de Marneffe et al., 2006) to obtain parse trees and dependency relations.","sentence2":"We also obtain the dependency parse of the sentences using the Stanford parser (De Marneffe et al., 2006)."},
{"id_case":"785","class":"","sentence1":"Training data are based on a concatenation of 18 POS-tagged English corpora 2 from the CHILDES database (MacWhinney, 2000).","sentence2":"Both CDS corpora are available from the CHILDES database (MacWhinney, 2000)."},
{"id_case":"786","class":"","sentence1":"In-domain SMT: we used the parallel corpus (Section 3) to train an en-pt phrase-based SMT system using the Moses toolkit (Koehn et al., 2007).","sentence2":"For the phrase-based SMT system, we adopted the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"787","class":"","sentence1":"5-gram language models of Turkish and English were trained using KenLM (Heafield, 2011).","sentence2":"We built a 5-gram language model on the English side of QCA-train using KenLM (Heafield, 2011)."},
{"id_case":"788","class":"","sentence1":"We used the relation classification dataset of the SemEval 2010 task 8 (Hendrickx et al., 2010).","sentence2":"We evaluated our model on a semantic relation classification task: SemEval 2010 Task 8 (Hendrickx et al., 2010)."},
{"id_case":"789","class":"","sentence1":"We then run word alignment with GIZA++ (Och and Ney, 2003) in both directions, with the default parameters used in Moses.","sentence2":"We performed word alignment using GIZA++ (Och and Ney, 2003) with the default grow-diagfinal-and alignment symmetrization method."},
{"id_case":"790","class":"","sentence1":"In-domain SMT: we used the parallel corpus (Section 3) to train an en-pt phrase-based SMT system using the Moses toolkit (Koehn et al., 2007).","sentence2":"For all experiments, we used the Moses SMT system (Koehn et al., 2007)."},
{"id_case":"791","class":"","sentence1":"The significance tests were performed using the bootstrap resampling method (Koehn, 2004).","sentence2":"The statistical significance tests using 95% confidence interval are measured with paired bootstrap resampling (Koehn, 2004)."},
{"id_case":"792","class":"","sentence1":"All experiments were carried out using the open-source SMT toolkit Moses (Koehn et al., 2007), in its standard non-monotonic configuration .","sentence2":"The SMT systems were built using the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"793","class":"","sentence1":"These sentences have then be fed into an efficient HPSG parser (PET; (Callmeier, 2000)) with ERG loaded.","sentence2":"The sentences are fed into the PET HPSG parser (Callmeier, 2000) with the GG loaded."},
{"id_case":"794","class":"","sentence1":"The language model is a 5-gram KenLM (Heafield, 2011) model, trained using lmplz, with modified Kneser-Ney smoothing and no pruning.","sentence2":"The 5-gram target language model was trained using KenLM (Heafield, 2011)."},
{"id_case":"796","class":"","sentence1":"Distributional hypothesis theory (Harris, 1954) indicates that words that occur in the same context tend to have similar meanings.","sentence2":"Similar row vectors in T indicate similar context of two terms in the domain, and terms that occur in the same contexts tend to have similar meanings (Harris, 1954)"},
{"id_case":"797","class":"","sentence1":"Distributional hypothesis theory (Harris, 1954) indicates that words that occur in the same context tend to have similar meanings.","sentence2":"Similar row vectors in T indicate similar context of two terms in the domain, and terms that occur in the same contexts tend to have similar meanings (Harris, 1954)."},
{"id_case":"799","class":"","sentence1":"We use the Stanford dependency parser (Marneffe et al., 2006).","sentence2":"We first use a dependency parser (de Marneffe et al., 2006) to parse each sentence and extract the set of dependency relations associated with the sentence ."},
{"id_case":"802","class":"","sentence1":"We use SRILM for training the 5-gram language model with interpolated modified Kneser-Ney discounting (Stolcke, 2002 ; Chen and Goodman, 1996).","sentence2":"To construct language models and measure perplexity , we use SRILM (Stolcke, 2002) with interpolated modified Kneser-Ney discounting (Chen and Goodman, 1996) and with a fixed vocabulary ."},
{"id_case":"804","class":"","sentence1":"We use SRILM for training the 5-gram language model with interpolated modified Kneser-Ney discounting (Stolcke, 2002 ; Chen and Goodman, 1996).","sentence2":"To construct language models and measure perplexity , we use SRILM (Stolcke, 2002) with interpolated modified Kneser-Ney discounting (Chen and Goodman, 1996) and with a fixed vocabulary ."},
{"id_case":"805","class":"","sentence1":"An unpruned, modified Kneser-Ney-smoothed 4-gram language model is estimated using the KenLM toolkit (Heafield et al., 2013).","sentence2":"The language models are estimated using the KenLM toolkit (Heafield et al., 2013) with modified Kneser-Ney smoothing."},
{"id_case":"806","class":"","sentence1":"We test our metrics in the setting of the WMT 2009 evaluation task (Callison-Burch et al., 2009).","sentence2":"Development and test sets are from the news translation task of WMT 2009 (Callison-Burch et al., 2009)."},
{"id_case":"807","class":"","sentence1":"The similarity function is here the Smoothed Partial Tree Kernel (SPTK) proposed in (Croce et al., 2011).","sentence2":"The previous year model adopted the Smoothed Partial Tree Kernel (SPTK) (Croce et al., 2011) in place of the PTK."},
{"id_case":"807","class":"","sentence1":"The similarity function is here the Smoothed Partial Tree Kernel (SPTK) proposed in (Croce et al., 2011).","sentence2":"The previous year model adopted the Smoothed Partial Tree Kernel (SPTK) (Croce et al., 2011) in place of the PTK."},
{"id_case":"808","class":"","sentence1":"Bold indicates a significant improvement according to bootstrap resampling at p < 0.05 (Koehn, 2004).","sentence2":"We measure significance of results using bootstrap resampling at p < 0.05 (Koehn, 2004)."},
{"id_case":"809","class":"","sentence1":"For statistical significance testing, we use sign-test with bootstrap re-sampling (Koehn, 2004)  with 1,000 sam- ples.","sentence2":"For statistical significance testing, we use a paired bootstrap resampling method proposed in (Koehn, 2004)."},
{"id_case":"810","class":"","sentence1":"We measure statistical significance using 95% confidence intervals computed with paired bootstrap resampling (Koehn, 2004).","sentence2":"We measure significance of results using bootstrap resampling at p < 0.05 (Koehn, 2004)."},
{"id_case":"811","class":"","sentence1":"All LMs, OSMs and count-based JTR models are estimated with the KenLM toolkit (Heafield et al., 2013).","sentence2":"The language models are estimated using the KenLM toolkit (Heafield et al., 2013) with modified Kneser-Ney smoothing."},
{"id_case":"812","class":"","sentence1":"As mentioned before, we use the Stanford parser (De Marneffe et al., 2006) to obtain a typed dependency representation of the input sentence.","sentence2":"As for EJ translation, we use the Stanford parser (de Marneffe et al., 2006) to obtain English abstraction trees."},
{"id_case":"814","class":"","sentence1":"Specifically, GIZA++ toolkit (Och and Ney, 2000) with default setting is used for word alignment on the JRC-Acquis parallel corpora (Steinberger et al., 2006).","sentence2":"Word alignment is performed by GIZA++ (Och and Ney, 2000) in both directions with the default setting ."},
{"id_case":"815","class":"","sentence1":"As mentioned before, we use the Stanford parser (De Marneffe et al., 2006) to obtain a typed dependency representation of the input sentence.","sentence2":"As for EJ translation, we use the Stanford parser (de Marneffe et al., 2006) to obtain English abstraction trees."},
{"id_case":"816","class":"","sentence1":"The evaluation method that is based on overlapping SCUs in human and automatic summaries is described in the Pyramid method (Nenkova et al., 2007).","sentence2":"The Pyramid method is a summarization evaluation scheme built upon the observation that human summaries can be equally informative despite being divergent in content (Nenkova et al., 2007)."},
{"id_case":"818","class":"","sentence1":"Thus, Equation 3 jointly performs feature selection and parameter estimation; it induces sparsity by setting many coefficients of ? to zero (Tibshirani, 1996).","sentence2":"An 1 penalty induces sparsity by setting many parameters of the model ? to exactly zero (Tibshirani, 1996)."},
{"id_case":"819","class":"","sentence1":"We evaluated our models using data from the bilingual word alignment workshop held at HLT-NAACL 2003 (Mihalcea and Pedersen, 2003).","sentence2":"The experiments reported here were carried out using data from the workshop on building and using parallel texts held at HLT-NAACL 2003 (Mihalcea and Pedersen, 2003)."},
{"id_case":"820","class":"","sentence1":"Titles were tokenized using CoreNLP (Manning et al., 2014).","sentence2":"The webpages were parsed using the Stanford CoreNLP software (Manning et al., 2014)."},
{"id_case":"821","class":"","sentence1":"Titles were tokenized using CoreNLP (Manning et al., 2014).","sentence2":"The webpages were parsed using the Stanford CoreNLP software (Manning et al., 2014)."},
{"id_case":"822","class":"","sentence1":"Decoding is performed using Moses (Koehn et al., 2007).","sentence2":"Statistical machine translation is typically performed using phrase-based systems (Koehn et al., 2007)."},
{"id_case":"823","class":"","sentence1":"For this, the TreeTagger (Schmid, 1994) is used.","sentence2":"For this, we used the TreeTagger (Schmid, 1994) with its Chinese model."},
{"id_case":"824","class":"","sentence1":"Statistical machine translation is typically performed using phrase-based systems (Koehn et al., 2007).","sentence2":"Decoding is performed using Moses (Koehn et al., 2007)."},
{"id_case":"825","class":"","sentence1":"We built a modified Kneser- Ney smoothed 5-gram language model using the English side of the training data and performed querying with KenLM (Heafield, 2011) 7 .","sentence2":"We trained an English 5-gram language model using KenLM (Heafield, 2011)."},
{"id_case":"826","class":"","sentence1":"We use binary cross-entropy as the objective function and the Adam optimization algorithm (Kingma and Ba, 2014) for training the network.","sentence2":"We employ Adam (Kingma and Ba, 2014) as the optimization method and mean squared error as loss function for our model."},
{"id_case":"828","class":"","sentence1":"We used the PLTM implementation in Mallet (McCallum, 2002).","sentence2":"The topic analysis implementation used in this pilot study borrows from the UMass Mallet topic analysis (McCallum, 2002)."},
{"id_case":"829","class":"","sentence1":"We have used the SVM implementation of the WEKA tool with default parameters for our experiments (Hall et al., 2009).","sentence2":"Because of our experience with the Weka package (Hall et al., 2009) we chose this tool for implementation ."},
{"id_case":"830","class":"","sentence1":"For analysis, also the baseline method and the method of Bjrne et al. (2010) were evaluated on the test sets.","sentence2":"These changes were also applied to the method of Bjrne et al. (2010) when analysing BioInfer."},
{"id_case":"831","class":"","sentence1":"Very weak stemming is performed in this step using only the first step of the Porter stemmer (Porter, 1980).","sentence2":"The evaluation is based on stemmed keyphrases, where stemming is performed using the Porter stemmer (Porter, 1980)."},
{"id_case":"832","class":"","sentence1":"This is often called distributional semantics where a word (or a name in our case) is known by the company it keeps (Firth, 1957).","sentence2":"Word co-occurence statistics \" You shall know a word by the company it keeps \" (Firth, 1957)."},
{"id_case":"833","class":"","sentence1":"The system configurations are as follows: GIZA++ (Och and Ney, 2003) is used to obtain the bidirectional word alignments.","sentence2":"We used the GIZA++ software (Och and Ney, 2003) to do the word alignments."},
{"id_case":"834","class":"","sentence1":"The LM is a 4-gram back-off model estimated with the modified Kneser-Ney smoothing method (Chen and Goodman, 1998).","sentence2":"Both are 4-gram LMs estimated with Kneser-Ney smoothing (Chen and Goodman, 1998)."},
{"id_case":"835","class":"","sentence1":"We perform word alignment by runing GIZA++ (Och and Ney, 2000), and then use the alignment results for constituency projection.","sentence2":"We then made use of the GIZA++ software (Och and Ney, 2000) to perform word alignment on the parallel corpora."},
{"id_case":"836","class":"","sentence1":"This is often called distributional semantics where a word (or a name in our case) is known by the company it keeps (Firth, 1957).","sentence2":"Distributional Semantic Models are based on the intuition that \" a word is characterized by the company it keeps \" (Firth, 1957)."},
{"id_case":"837","class":"","sentence1":"We conduct an empirical evaluation using encoder-decoder NMT with attention and gated recurrent units as implemented in Nematus (Sennrich et al., 2017).","sentence2":"We utilize the Nematus implementation (Sennrich et al., 2017) to build encoder-decoder NMT systems with attention and gated recurrent units."},
{"id_case":"838","class":"","sentence1":"The shortest path between two entities in an SD tree supports the extraction of local and long-distance relationships (Bunescu and Mooney, 2005).","sentence2":"The dependency path is the shortest path between the two entities in a dependency parse graph and has been shown to be important for relation extraction (Bunescu and Mooney, 2005)."},
{"id_case":"839","class":"","sentence1":"It therefore follows the distributional hypothesis (Harris, 1954) which states that words that occur in the same contexts tend to have similar meanings.","sentence2":"The former that is the most popular relies on the distributional hypothesis that puts forward the idea that words with similar meaning tend to occur in similar contexts (Harris, 1954)."},
{"id_case":"840","class":"","sentence1":"BLE is based on the distributional hypothesis (Harris, 1954), stating that words with similar meaning have similar distributions across languages .","sentence2":"The former that is the most popular relies on the distributional hypothesis that puts forward the idea that words with similar meaning tend to occur in similar contexts (Harris, 1954)."},
{"id_case":"843","class":"","sentence1":"We used Mallet toolkit (McCallum, 2002) for CRF implementation .","sentence2":"We used the Mallet toolkit (McCallum, 2002) for training the CRF but implemented our own feature extraction and runtime system."},
{"id_case":"844","class":"","sentence1":"5-gram language models are trained over the target-side of the training data, using SRILM (Stolcke, 2002) with modified Kneser-Ney discounting (Chen and Goodman, 1996).","sentence2":"Our 5-gram language model was trained on the Xinhua section of the English Gigaword corpus (306 million words) using the SRILM toolkit (Stolcke, 2002)  with modified Kneser-Ney smoothing."},
{"id_case":"847","class":"","sentence1":"Given two mentions in a document, m 1 and m 2 , we generate the following features and feed them to a logistic regression classifier: ? binary indicators for all tokens contained in 12 These features ","sentence2":"These features were obtained using the Stanford parser 2 (Marneffe et al., 2006)."},
{"id_case":"848","class":"","sentence1":"Training is done with the Adam optimisation algorithm (Kingma and Ba, 2014) with learning rate of 104 .","sentence2":"The network was trained in batches (size 40) for 30 epochs with the Adam optimizer (Kingma and Ba, 2014), starting with a learning rate of 104 ."},
{"id_case":"850","class":"","sentence1":"amples provided, by using transformation-based learn- ing [Brill, 1995, Satta and Henderson, 1997] .","sentence2":"This list is then processed by a transformation-based learning paradigm [Brill, 1995, Satta and Henderson, 1997] as illustrated in Figure 4 ."},
{"id_case":"851","class":"","sentence1":"A distributional similarity model is constructed based on the distributional hypothesis (Harris, 1954): words that occur in the same contexts tend to share similar meanings.","sentence2":"Distributional similarity relies on the distributional hypothesis that similar terms appear in similar contexts (Harris, 1954)."},
{"id_case":"852","class":"","sentence1":"These methods are based on the distributional hypothesis which states that words appearing in the same contexts tend to have similar meaning (Harris, 1954).","sentence2":"Distributional similarity relies on the distributional hypothesis that similar terms appear in similar contexts (Harris, 1954)."},
{"id_case":"854","class":"","sentence1":"In this paper, we specifically use the MADAMIRA tool (Pasha et al., 2014) for morphological analysis and disambiguation of MSA and EGY.","sentence2":"MADAMIRA (Pasha et al., 2014) is a morphological analysis and disambiguation tool of Arabic ."},
{"id_case":"855","class":"","sentence1":"We conducted our experiments using a state-ofthe-art SMT system Moses (Koehn et al., 2007).","sentence2":"To test our method, we conducted two lowresource translation experiments using the phrase-based MT system Moses (Koehn et al., 2007)."},
{"id_case":"856","class":"","sentence1":"All experiments are conducted using the Moses phrase-based SMT system (Koehn et al., 2007) with a maximum phrase length of 8.","sentence2":"We conducted our experiments using a state-of-the-art SMT system Moses (Koehn et al., 2007)."},
{"id_case":"857","class":"","sentence1":"We evaluated the reordering approach within the MOSES phrase-based SMT system (Koehn et al., 2007).","sentence2":"We report results for the method on the NIST 2006 evaluation data, using the MOSES phrase-based SMT system (Koehn et al., 2007)."},
{"id_case":"858","class":"","sentence1":"Our 5-gram language model was trained on the Xinhua section of the English Gigaword corpus (306 million words) using the SRILM toolkit (Stolcke, 2002)  with modified Kneser-Ney smoothing.","sentence2":"5-gram language models were trained using SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998) and then interpolated using weights tuned on the newstest2011 deve"},
{"id_case":"859","class":"","sentence1":"Both were trained on Europarl (Koehn, 2005).","sentence2":"The systems for the English Spanish translation tasks were trained on the sentence-aligned Europarl corpus (Koehn, 2005)."},
{"id_case":"861","class":"","sentence1":"The judges achieved a reliability of = 0.708 (Krippendorff, 1980) this value shows that agreement is well above chance, and allows for tentative conclusions.","sentence2":"The judges achieved a reliability of = 0.708 (Krippendorff, 1980) Figure 4 shows the observed rating frequencies of sentence retriever (mean 3.0) and our approach (mean 3.6) on the test examples."},
{"id_case":"863","class":"","sentence1":"We use the Stanford dependency parser (de Marneffe et al., 2006) for extracting dependency path and part-of-speech features.","sentence2":"We use the Stanford parser with Stanford dependencies (de Marneffe et al., 2006)."},
{"id_case":"864","class":"","sentence1":"We calculated significance using paired bootstrap resampling (Koehn, 2004).","sentence2":"A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004)."},
{"id_case":"865","class":"","sentence1":"We use the Stanford parser with Stanford dependencies (de Marneffe et al., 2006).","sentence2":"We also obtain the dependency parse of the sentences using the Stanford parser (De Marneffe et al., 2006)."},
{"id_case":"866","class":"","sentence1":"We use the Stanford parser with Stanford dependencies (de Marneffe et al., 2006).","sentence2":"We use the Stanford dependency parser (de Marneffe et al., 2006) for extracting dependency path and part-of-speech features."},
{"id_case":"867","class":"","sentence1":"We use the phrase-structure treebank released for the SPMRL 2013 shared task (Seddah et al., 2013).","sentence2":"We use the standard SPMRL data set (Seddah et al., 2013)."},
{"id_case":"868","class":"","sentence1":"We use the Stanford dependency parser (de Marneffe et al., 2006) for extracting dependency path and part-of-speech features.","sentence2":"We use the Stanford parser with Stanford dependencies (de Marneffe et al., 2006)."},
{"id_case":"869","class":"","sentence1":"We use the Stanford parser with Stanford dependencies (de Marneffe et al., 2006).","sentence2":"We use the Stanford dependency parser (de Marneffe et al., 2006) for extracting dependency path and part-of-speech features."},
{"id_case":"870","class":"","sentence1":"We also obtain the dependency parse of the sentences using the Stanford parser (De Marneffe et al., 2006).","sentence2":"We use the Stanford parser with Stanford dependencies (de Marneffe et al., 2006)."},
{"id_case":"871","class":"","sentence1":"We use the Stanford parser with Stanford dependencies (de Marneffe et al., 2006).","sentence2":"We also obtain the dependency parse of the sentences using the Stanford parser (De Marneffe et al., 2006)."},
{"id_case":"872","class":"","sentence1":"A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004).","sentence2":"Significance was estimated using paired bootstrap resampling (Koehn, 2004)."},
{"id_case":"873","class":"","sentence1":"We also obtain the dependency parse of the sentences using the Stanford parser (De Marneffe et al., 2006).","sentence2":"We use the Stanford parser with Stanford dependencies (de Marneffe et al., 2006)."},
{"id_case":"874","class":"","sentence1":"An HPSG grammar can use Minimal Recursion Semantics (MRS) as meaning representation (Copestake et al., 2005).","sentence2":"The semantic representation is Minimal Recursion Semantics (Copestake et al., 2005)."},
{"id_case":"875","class":"","sentence1":"We calculated significance using paired bootstrap resampling (Koehn, 2004).","sentence2":"The significance tests were performed using the bootstrap resampling method (Koehn, 2004)."},
{"id_case":"877","class":"","sentence1":"Our second method is based on the recurrent neural network language model (RNNLM) approach to learning word embeddings of Mikolov et al. (2013a) and Mikolov et al. (2013b) , using the WORD2VEC package.","sentence2":"Our model is an extension of the contextual bag of words (CBOW) model of Mikolov et al. (2013b) , a method for learning vector representations of words based on their distributional contexts."},
{"id_case":"879","class":"","sentence1":"We built a modified Kneser- Ney smoothed 5-gram language model using the English side of the training data and performed querying with KenLM (Heafield, 2011) .","sentence2":"We built a trigram language model with Kneser-Ney smoothing using KenLM toolkit (Heafield, 2011)."},
{"id_case":"880","class":"","sentence1":"The language model is a 5-gram with interpolation and Kneser-Ney smoothing (Kneser and Ney, 1995).","sentence2":"The perplexity achieved by the 6-gram NN LM in the Spanish News2009 set was 281, versus 145 obtained with the standard 6-gram language model with interpolation and Kneser-Ney smoothing (Kneser and Ney, 1995)."},
{"id_case":"881","class":"","sentence1":"We use the cross-entropy loss function and minibatch AdaGrad (Duchi et al., 2011) to optimize parameters .","sentence2":"We choose cross-entropy loss as our training objective and use AdaGrad (Duchi et al., 2011) with a learning rate of 0.01 and a minibatch size of 50 to train the model."},
{"id_case":"883","class":"","sentence1":"We normalize the punctuation, tokenize and truecase all our data using the scripts that are available in Moses 6 (Koehn et al., 2007).","sentence2":"We tokenize and frequent-case the data with the standard scripts from the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"884","class":"","sentence1":"Our program takes as input a text that is lemmatized, tagged and parsed using the Stanford CoreNLP tools (Manning et al., 2014).","sentence2":"In addition, the data was tokenized, lemmatized, and parsed using Stanford CoreNLP (Manning et al., 2014)."},
{"id_case":"886","class":"","sentence1":"Our annotated data mainly comes from CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006).","sentence2":"The data in all these languages is obtained from the CoNLL 2006 shared task on multilingual dependency parsing (Buchholz and Marsi, 2006)."},
{"id_case":"887","class":"","sentence1":"The data in all these languages is obtained from the CoNLL 2006 shared task on multilingual dependency parsing (Buchholz and Marsi, 2006).","sentence2":"Our annotated data mainly comes from CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006)."},
{"id_case":"888","class":"","sentence1":"use the Stanford Parser (de Marneffe et al., 2006) to extract a set of dependencies from each comment.","sentence2":"We first use a dependency parser (de Marneffe et al., 2006) to parse each sentence and extract the set of dependency relations associated with the sentence ."},
{"id_case":"889","class":"","sentence1":"Our program takes as input a text that is lemmatized, tagged and parsed using the Stanford CoreNLP tools (Manning et al., 2014).","sentence2":"In addition, the data was tokenized, lemmatized, and parsed using Stanford CoreNLP (Manning et al., 2014)."},
{"id_case":"890","class":"","sentence1":"use the Stanford Parser (de Marneffe et al., 2006) to extract a set of dependencies from each comment.","sentence2":"We first use a dependency parser (de Marneffe et al., 2006) to parse each sentence and extract the set of dependency relations associated with the sentence ."},
{"id_case":"891","class":"","sentence1":"For decoding, we used the state of the art phrase based SMT toolkit Moses (Koehn et al., 2007) with default options, except for the distortion limit (6?20).","sentence2":"For training the translation model and for decoding we used the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"892","class":"","sentence1":"We used the SVM light library (Joachims, 1999) with two different kernel functions and feature sets for learning the classification models.","sentence2":"We used SVM light (Joachims, 1999) with a linear kernel function."},
{"id_case":"893","class":"","sentence1":"For decoding, we used the state of the art phrase based SMT toolkit Moses (Koehn et al., 2007) with default options, except for the distortion limit (620).","sentence2":"For the phrase-based SMT system, we adopted the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"894","class":"","sentence1":"We first use a dependency parser (de Marneffe et al., 2006) to parse each sentence and extract the set of dependency relations associated with the sentence .","sentence2":"We use the standard Stanford-style set of dependency labels (de Marneffe et al., 2006)."},
{"id_case":"895","class":"","sentence1":"For decoding, we used the state of the art phrase based SMT toolkit Moses (Koehn et al., 2007) with default options, except for the distortion limit (620).","sentence2":"For all experiments, we used the Moses SMT system (Koehn et al., 2007)."},
{"id_case":"896","class":"","sentence1":"For decoding, we used the state of the art phrase based SMT toolkit Moses (Koehn et al., 2007) with default options, except for the distortion limit (620).","sentence2":"For our SMT experiments, we use the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"897","class":"","sentence1":"We first use a dependency parser (de Marneffe et al., 2006) to parse each sentence and extract the set of dependency relations associated with the sentence .","sentence2":"We use the standard Stanford-style set of dependency labels (de Marneffe et al., 2006)."},
{"id_case":"898","class":"","sentence1":"This is done using IBM Model 1 (Brown et al., 1993) and GIZA++ (Och and Ney, 2003).","sentence2":"For comparison, Model 4 is trained for 5 iterations using 5 iterations each of Model 1 and Model 3 as initialization, using GIZA++ (Och and Ney, 2003)."},
{"id_case":"899","class":"","sentence1":"Training is done with the Adam optimisation algorithm (Kingma and Ba, 2014) with learning rate of 104 .","sentence2":"We used Adam (Kingma and Ba, 2014) with a learning rate of 0.0002 ()."},
{"id_case":"900","class":"","sentence1":"Word stems have been generated by the Porter stemmer (Porter, 1980).","sentence2":"We have stemmed the training and testing documents by using the Porter stemmer (Porter, 1980) ."},
{"id_case":"901","class":"","sentence1":"Our machine translation systems are trained using Moses (Koehn et al., 2007).","sentence2":"The first two baselines are standard systems using PBMT or Hiero trained using Moses (Koehn et al., 2007)."},
{"id_case":"902","class":"","sentence1":"We measure statistical significance confidence intervals computed with paired bootstrap resampling (Koehn, 2004).","sentence2":"The reported confidence intervals were estimated using bootstrap resampling (Koehn, 2004)."},
{"id_case":"903","class":"","sentence1":"All classification tasks were performed using the WEKA toolkit (Hall et al., 2009).","sentence2":"Model development were performed using the WEKA data mining software package (Hall et al., 2009)."},
{"id_case":"904","class":"","sentence1":"Statistical significance in BLEU score difference was measured by using paired bootstrap re-sampling (Koehn, 2004).","sentence2":"The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004)."},
{"id_case":"905","class":"","sentence1":"We start by extracting noun phrases using the Illinois Chunker (Punyakanok and Roth, 2001).","sentence2":"We then identify all noun phrases in these sentences using the Illinois Chunker (Punyakanok and Roth, 2001)."},
{"id_case":"906","class":"","sentence1":"Then we ran TreeTagger (Schmid, 1994) to obtain the tags and the lemmas.","sentence2":"For both English and German we used the part-ofspeech tagger TreeTagger (Schmid, 1994) to obtain POS-tags."},
{"id_case":"907","class":"","sentence1":"Kneser-Ney n-grams were trained with KenLM (Heafield, 2011).","sentence2":"Kneser-Ney n-grams trained with the KenLM toolkit (Heafield, 2011)."},
{"id_case":"908","class":"","sentence1":"We used a 4-gram language model, trained with KenLM (Heafield et al., 2013).","sentence2":"For the language model, we used all monolingual datasets and the French parts of the parallel datasets and trained a 5-gram language model with modified Kneser-Ney smoothing using KenLM (Heafield et al., 2013)."},
{"id_case":"909","class":"","sentence1":"Using this spanning tree representation, we extend the work of McDonald (McDonald et al. 2005)  on online large-margin discriminative training methods to non projective dependencies.","sentence2":"In this section, we review the work of McDonald (McDonald et al. 2005)  for online large-margin dependency parsing ."},
{"id_case":"910","class":"","sentence1":"We train the models with Adadelta (Zeiler, 2012), reshuffling the training corpus between epochs.","sentence2":"We used Adadelta (Zeiler, 2012) with a minibatch size of 80, and reshuffled the training set between epochs."},
{"id_case":"911","class":"","sentence1":"The classification models described above were implemented using the Weka package (Hall et al., 2009).","sentence2":"The different steps described below were carried out using the Weka Machine Learning toolkit (Hall et al., 2009)."},
{"id_case":"913","class":"","sentence1":"Light (Light et al., 2004) developed a classifier to predict the level of speculations in sentences in biomedical abstracts.","sentence2":"Light (Light et al., 2004) analyse the use of speculative language in biomedical abstracts."},
{"id_case":"914","class":"","sentence1":"The corpus was then automatically tagged with part-of-speech information, using TreeTagger (Schmid, 1994).","sentence2":"The corpus is lemmatised and tagged by part-of-speech on both sides using the TreeTagger (Schmid, 1994)."},
{"id_case":"915","class":"","sentence1":"We used the Random Forests implementation of scikit-learn toolkit (Pedregosa et al., 2011) with 50 estimators.","sentence2":"We used the scikit-learn (Pedregosa et al., 2011) implementation of these classifiers with their default parameter settings for our experiments."},
{"id_case":"916","class":"","sentence1":"The significance tests were performed using the bootstrap resampling method (Koehn, 2004).","sentence2":"We test the statistical significance of differences systems using the bootstrap resampling method (Koehn, 2004)."},
{"id_case":"917","class":"","sentence1":"All classifiers were implemented using the open source, freely available Weka (Hall et al., 2009).","sentence2":"The classification models described above were implemented using the Weka package (Hall et al., 2009)."},
{"id_case":"918","class":"","sentence1":"We train the models with Adadelta (Zeiler, 2012) reshuffling the training corpus between epochs.","sentence2":"We used Adadelta (Zeiler, 2012) and reshuffled the training set between epochs."},
{"id_case":"919","class":"","sentence1":"Statistical significance tests using bootstrap resampling (Koehn, 2004).","sentence2":"The significance tests were performed using the bootstrap resampling method (Koehn, 2004)."},
{"id_case":"920","class":"","sentence1":"To predict the readability of a document according to reading time, we use Ranking SVM (Joachims, 2002).","sentence2":"To label the instances of the unseen data we use SVM light (Joachims, 2002)."},
{"id_case":"921","class":"","sentence1":"The experiments focus on translation from German to English, using the Europarl data (Koehn, 2005).","sentence2":"All the data come from Europarl (Koehn, 2005)."},
{"id_case":"922","class":"","sentence1":"They are constructed by various models whose unifying philosophy is that the meaning of a word is defined by the company it keeps (Firth, 1957).","sentence2":"Distributional Semantic Models are based on the intuition that a word is characterized by the company it keeps (Firth, 1957)."},
{"id_case":"923","class":"","sentence1":"We measure statistical significance  computed with paired bootstrap resampling (Koehn, 2004).","sentence2":"The statistical significance test is also carried out with paired bootstrap resampling method (Koehn, 2004)."},
{"id_case":"924","class":"","sentence1":"That paper used the Europarl (Koehn, 2005) multilingual collection of English and Spanish sessions.","sentence2":"For baseline we used the Spanish and English sides of the Europarl multilingual parallel corpus (Koehn, 2005), with the standard training, development, and test sets."},
{"id_case":"925","class":"","sentence1":"Tense, aspect, and modality are also annotated in the TimeBank (Pustejovsky et al., 2005) as attributes of events.","sentence2":"The TimeBank (Pustejovsky et al., 2005) also annotates tense, aspect, and modality, as attributes of events."},
{"id_case":"926","class":"","sentence1":"They are based on Distributional Hypothesis which works under the assumption that similar words occur in similar contexts (Harris, 1954).","sentence2":"Distributional similarity relies on the distributional hypothesis that similar terms appear in similar contexts (Harris, 1954)."},
{"id_case":"927","class":"","sentence1":"Secondly, the reflexive pronoun is not necessarily the head of a constituent (Gazdar et al., 1985).","sentence2":"The HFC in GPSG (Gazdar et al., 1985) cannot instantiate the antecedent information of a reflexive pronoun on a mothernode in cases where the reflexive is not the head of a constituent."},
{"id_case":"928","class":"","sentence1":"The Levenshtein distance (Levenshtein, 1966) between two strings is defined as the minimum number of editing operations (substitutions, deletions, and insertions) for transforming one string into the other.","sentence2":"We start from the well established Levenshtein distance (Levenshtein, 1966), which is defined as the minimum number of insertions, deletions, and substitutions needed to transform one string into the other."},
{"id_case":"929","class":"","sentence1":"This is a corpus based metric relying on the distributional hypothesis of meaning suggesting that similarity of context implies similarity of meaning (Harris, 1954).","sentence2":"The fundamental assumption behind context based metrics is that similarity of context implies similarity of meaning (Harris, 1954)."},
{"id_case":"930","class":"","sentence1":"The fundamental assumption behind context-based metrics is that similarity of context implies similarity of meaning (Harris, 1954).","sentence2":"This is a corpus-based metric relying on the distributional hypothesis of meaning suggesting that similarity of context implies similarity of meaning (Z. Harris, 1954)."},
{"id_case":"931","class":"","sentence1":"Feature selection was performed using chi-square in Weka (Hall et al., 2009).","sentence2":"The model was trained using Weka (Hall et al., 2009)."},
{"id_case":"932","class":"","sentence1":"We adopt online learning, updating parameters using AdaGrad (Duchi et al., 2011).","sentence2":"We trained all models using AdaGrad (Duchi et al., 2011)."},
{"id_case":"933","class":"","sentence1":"We use Ranking SVM (Joachims, 2002) for candidate ranking.","sentence2":"We use SVM rank (Joachims, 2002) for training the candidate ranking model."},
{"id_case":"934","class":"","sentence1":"We use the Stanford dependency parser (Marneffe et al., 2006).","sentence2":"We parse the sentences with the Stanford Parser  (Marneffe et al., 2006)."},
{"id_case":"935","class":"","sentence1":"We modified GIZA++ (Och and Ney, 2003) to incorporate word similarity.","sentence2":"We used the GIZA++ software (Och and Ney, 2003) to do the word alignments."},
{"id_case":"936","class":"","sentence1":"Word alignment was done with GIZA++ (Och and Ney, 2003) for both systems.","sentence2":"Word alignment is done using GIZA++ (Och and Ney, 2003)."},
{"id_case":"937","class":"","sentence1":"The predictors are trained using the WEKA tookit (Hall et al., 2009).","sentence2":"The model was trained using Weka (Hall et al., 2009)."},
{"id_case":"938","class":"","sentence1":"We use ROUGE (Lin, 2004) for evaluating the content of summaries.","sentence2":"We used ROUGE (Lin, 2004) for evaluation of summaries."},
{"id_case":"939","class":"","sentence1":"These implementations came from the scikit-learn Python library (Pedregosa et al., 2011).","sentence2":"Classification uses the scikit-learn Python package (Pedregosa et al., 2011)."},
{"id_case":"940","class":"","sentence1":"The weights initialization follows the normalized initialization proposed in (Glorot and Bengio, 2010).","sentence2":"Dense weights use normalized uniform initialization (Glorot and Bengio, 2010)."},
{"id_case":"941","class":"","sentence1":"Word alignment was done with GIZA++ (Och and Ney, 2003) for both systems.","sentence2":"Word alignment is done using GIZA++ (Och and Ney, 2003)."},
{"id_case":"942","class":"","sentence1":"We build a sentence-specific, zero-cutoff stupidbackoff (Brants et al., 2007) 5-gram LMs estimated over the data.","sentence2":"We build sentence-specific, zero-cutoff stupidbackoff (Brants et al., 2007) 5-gram LMs estimated over approximately 1.4 billion words"},
{"id_case":"943","class":"","sentence1":"We use the same conversion style that we used in Seeker and Kuhn (2012) to convert the German Tiger Treebank (Brants et al., 2002).","sentence2":"We used Tiger Treebank (Brants et al., 2002) in the improved dependency conversion by Seeker and Kuhn (2012)."},
{"id_case":"944","class":"","sentence1":"In addition, we show that the latent representation coheres well with the characterization of discourse connectives in the Penn Discourse Treebank (Prasad et al., 2008).","sentence2":"One of the most important resources for discourse connectives in English is the Penn Discourse Treebank (Prasad et al., 2008)."},
{"id_case":"945","class":"","sentence1":"As a learning algorithm we adopt a ranking SVM (Joachims, 2002), which is an instance of preference learning.","sentence2":"Given such a ranking (which, we note, may be partial), we apply SVM rank (Joachims, 2002), which is part of the SVM light set of SVM-based machine learning tools."},
{"id_case":"947","class":"","sentence1":"For arbitrary word-reordering, the decoding problem is NP-complete (Knight, 1999).","sentence2":"If arbitrary word-reorderings are allowed, the search problem is NP-hard (Knight, 1999)."},
{"id_case":"948","class":"","sentence1":"We used GIZA++ (Och and Ney, 2003) to align the words in the corpus.","sentence2":"We used the mkcls tool in GIZA (Och and Ney, 2003) to learn the word classes."},
{"id_case":"949","class":"","sentence1":"For this purpose we use the Europarl corpus (Koehn, 2005).","sentence2":"We use the Europarl corpus (Koehn, 2005) as testing data."},
{"id_case":"950","class":"","sentence1":"We used the mkcls tool in GIZA (Och and Ney, 2003) to learn the word classes.","sentence2":"We used the GIZA++ software (Och and Ney, 2003) to do the word alignments."},
{"id_case":"951","class":"","sentence1":"Corpus-based VSMs follow the standard distributional hypothesis, which states that words appearing in the same contexts tend to have similar meaning (Harris, 1954).","sentence2":"According to the distributional hypothesis of meaning (Harris, 1954), words that occur in similar contexts tend to be semantically similar."},
{"id_case":"952","class":"","sentence1":"Here we review the parameters of the standard phrase-based translation model (Koehn et al., 2007).","sentence2":"It is a standard phrase-based machine translation model (Koehn et al., 2007)."},
{"id_case":"953","class":"","sentence1":"For tuning, we used the batch MIRA (Cherry and Foster, 2012).","sentence2":"For tuning the feature weights, we applied batch MIRA  (Cherry and Foster, 2012)."},
{"id_case":"954","class":"","sentence1":"5-gram language models are trained over the target-side of the training data, using SRILM  with modified Kneser-Ney discounting (Chen and Goodman, 1996).","sentence2":"5-gram language models were trained using SRILM toolkit with modified Kneser-Ney smoothing (Chen and Goodman, 1998)"},
{"id_case":"955","class":"","sentence1":"They are based on Distributional Hypothesis which works under the assumption that similar words occur in similar contexts (Harris, 1954).","sentence2":"Corpus-based meaning representations rely on the distributional hypothesis, which assumes that words occurring in a similar set of contexts are also similar in meaning (Harris, 1954)."},
{"id_case":"956","class":"","sentence1":"They used the Web-based annotation tool brat (Stenetorp et al., 2012) for the annotation .","sentence2":"All annotations were done using the brat rapid annotation tool (Stenetorp et al., 2012)."},
{"id_case":"958","class":"","sentence1":"Here, we use the logistic regression classifier with ridge regularization from WEKA (Witten and Frank, 2005).","sentence2":"Concretely, we employ the logistic regression classifier from the Weka Toolkit (Witten and Frank, 2005)."},
{"id_case":"959","class":"","sentence1":"A good data source for this experiments is the Europarl Corpus (Koehn, 2005).","sentence2":"The source of bilingual data used in the experiments is the Europarl collection (Koehn, 2005)."},
{"id_case":"960","class":"","sentence1":"Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013).","sentence2":"RED is being revamped to supplement the Abstract Meaning Representation (AMR) annotation schema (Banarescu et al., 2013)."},
{"id_case":"961","class":"","sentence1":"We evaluated the reordering approach within the MOSES phrase-based SMT system (Koehn et al., 2007).","sentence2":"We build a state of the art phrase-based SMT system using Moses (Koehn et al., 2007)."},
{"id_case":"963","class":"","sentence1":"The baseline systems are built with the opensource phrase-based SMT toolkit Moses (Koehn et al., 2007).","sentence2":"The questions are translated using a phrase-based system built using Moses (Koehn et al., 2007) (the Mo set)."},
{"id_case":"964","class":"","sentence1":"Parameters are updated using AdaGrad (Duchi et al., 2011) with a learning rate of 0.1.","sentence2":"Learning Online learning was performed using AdaGrad (Duchi et al., 2011) with initial learning rate of 0.1."},
{"id_case":"965","class":"","sentence1":"The questions are translated using a phrase-based system built using Moses (Koehn et al., 2007) (the Mo set).","sentence2":"We build a state of the art phrase-based SMT system using Moses (Koehn et al., 2007)."},
{"id_case":"966","class":"","sentence1":"The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) is the largest available resource of discourse relation annotations.","sentence2":"PDTB (Prasad et al., 2008) is the largest handannotated corpus of discourse relation so far."},
{"id_case":"967","class":"","sentence1":"We use the Universal POS Tagset (UPOS) of Petrov (Petrov et al. (2012) .","sentence2":"We use the Universal Tagset by Petrov (Petrov et al. (2012) for this purpose."},
{"id_case":"968","class":"","sentence1":"The first is the one used in the chunking competition in CoNLL-2000 (Tjong Kim Sang and Buchholz, 2000).","sentence2":"The conversion program is the same as used for the CoNLL-2000 shared task (Tjong Kim Sang and Buchholz, 2000)."},
{"id_case":"969","class":"","sentence1":"As shown in (Knight, 1999), if arbitrary reordering is allowed, the search problem is NP-hard.","sentence2":"If arbitrary word-reorderings are allowed, the search problem is NP-hard (Knight, 1999)."},
{"id_case":"970","class":"","sentence1":"In our experiments , we used the Stanford parser (De Marneffe et al., 2006) to create dependency parses.","sentence2":"The Stanford parser (Marneffe et al., 2006) was used to produce all dependency parses."},
{"id_case":"971","class":"","sentence1":"We conducted our experiments using a state of the art SMT system Moses (Koehn et al., 2007).","sentence2":"We build a baseline error correction system, using the MOSES SMT system (Koehn et al., 2007)."},
{"id_case":"972","class":"","sentence1":"All of the text data from Reddit was tokenized using the NLTK tokenizer (Bird et al., 2009).","sentence2":"For Reuters we segmented and tokenized the data using NLTK (Bird et al., 2009)."},
{"id_case":"973","class":"","sentence1":"RED is being revamped to supplement the Abstract Meaning Representation (AMR) annotation schema (Banarescu et al., 2013).","sentence2":"Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013)."},
{"id_case":"974","class":"","sentence1":"The word alignment was trained using GIZA++ (Och and Ney, 2003) with the configuration grow diag final-and.","sentence2":"Then we did word alignment using GIZA++ (Och and Ney, 2003) with the default grow diag final-and alignment symmetrization method."},
{"id_case":"975","class":"","sentence1":"We build a baseline error correction system, using the MOSES SMT system (Koehn et al., 2007).","sentence2":"We evaluated the reordering approach within the MOSES phrase-based SMT system (Koehn et al., 2007)."},
{"id_case":"976","class":"","sentence1":"We conducted baseline experiments for phrasebased machine translation using the Moses toolkit (Koehn et al., 2007).","sentence2":"We conducted our experiments using a state of the art SMT system Moses (Koehn et al., 2007)."},
{"id_case":"977","class":"","sentence1":"Both of our systems were based on the Moses decoder (Koehn et al., 2007).","sentence2":"Both systems are based on the Moses SMT toolkit (Koehn et al., 2007)."},
{"id_case":"978","class":"","sentence1":"This dataset was collected for the 2012 SemEval competition and consists of 1,500 pairs of short video descriptions which were then annotated (Agirre et al., 2012).","sentence2":"Microsoft Video Paraphrase Corpus (MSRVID) is from Task 6 of the 2012 SemEval competition (Agirre et al., 2012) and consists of 1,500 annotated pairs of video descriptions, with half for training."},
{"id_case":"979","class":"","sentence1":"In PB-SMT, the posterior probability P(e I 1 |f J 1) is directly modeled as a (log-linear) combination of features (Och and Ney, 2002).","sentence2":"The posterior probability P(e I 1 |f J 1) is modeled directly using a log-linear combination of several models (Och and Ney, 2002)."},
{"id_case":"980","class":"","sentence1":"Training is done with the Adam optimization algorithm (Kingma and Ba, 2014).","sentence2":"We use the Adam (Kingma and Ba, 2014) algorithm to minimize the sum of the loss."},
{"id_case":"981","class":"","sentence1":"The MT experiments were carried out using the standard log-linear phrase-based SMT toolkit MOSES (Koehn et al., 2007).","sentence2":"Our baseline is a phrase-based SMT system trained using the MOSES toolkit (Koehn et al., 2007)."},
{"id_case":"982","class":"","sentence1":"We use the cross-entropy loss function and minibatch AdaGrad (Duchi et al., 2011) to optimize parameters .","sentence2":"We use online learning to train model parameters , updating the parameters using the AdaGrad algorithm (Duchi et al., 2011)."},
{"id_case":"983","class":"","sentence1":"We train and evaluate the classifiers in a 10-fold crossvalidation using Weka (Hall et al., 2009).","sentence2":"We train Random Forest classifiers (Breiman, 2001) using Weka (Hall et al., 2009) for each step and also for the JOINT model."},
{"id_case":"984","class":"","sentence1":"We tokenize and frequent-case the data with the standard scripts from the Moses toolkit (Koehn et al., 2007).","sentence2":"We tokenize and frequent-case all of the corpora using code released with Moses (Koehn et al., 2007)."},
{"id_case":"985","class":"","sentence1":"The second type of evaluation follows the supervised evaluation of the SemEval-2007 WSI task (Agirre and Soroa, 2007).","sentence2":"The collocational WSI approach was evaluated under the framework and corpus of SemEval-2007 WSI task (Agirre and Soroa, 2007a)."},
{"id_case":"986","class":"","sentence1":"We built a   phrase-based SMT model from the bilingual domain corpus using the Moses toolkit (Koehn et al., 2007).","sentence2":"We built phrase-based machine translation systems using the open software toolkit Moses (Koehn et al., 2007)."},
{"id_case":"987","class":"","sentence1":"We found that using AdaGrad (Duchi et al., 2011) to update the parameters is very effective .","sentence2":"We use online learning to train model parameters , updating the parameters using the AdaGrad algorithm (Duchi et al., 2011)."},
{"id_case":"988","class":"","sentence1":"The tectogrammatical annotation layer is based on the Functional Generative Description theory (Sgall et al., 1986).","sentence2":"The theoretical basis of the tectogrammatical representation lies in the Functional Generative Description of language systems (Sgall et al., 1986)."},
{"id_case":"990","class":"","sentence1":"The first NLI Shared Task was part of the 2013 Building Educational Applications (BEA) workshop (Tetreault et al., 2013).","sentence2":"The first NLI Shared Task aims to identify the L1 of the text data of an essay response (Tetreault et al., 2013)."},
{"id_case":"991","class":"","sentence1":"Particularly, our results (ROUGE-2) are statistically significantly (p < 0.05) higher than TAC08 Best system, but are not statistically significant compared with (Li et al., 2013a) (p > 0.05).","sentence2":"Our result (R-2) is statistically significantly (p < 0.05) better than TAC11 Best system, but not statistically (p > 0.05) significantly different from (Li et al., 2013a)."},
{"id_case":"992","class":"","sentence1":"To obtain these we use the Stanford dependency parser (de Marneffe et al., 2006) and the forced alignment from Section 3.9.","sentence2":"To obtain the dependency trees and word lemmas and POS tags, we use the Stanford NLP tools (De Marneffe et al., 2006)."},
{"id_case":"993","class":"","sentence1":"To obtain these we use the Stanford dependency parser (de Marneffe et al., 2006) and the forced alignment from Section 3.9.","sentence2":"To obtain the dependency trees and word lemmas and POS tags, we use the Stanford NLP tools 2 (De Marneffe et al., 2006)."},
{"id_case":"994","class":"","sentence1":"We trained a model using Moses toolkit (Koehn et al., 2007) on the training data as our baseline system.","sentence2":"We refer to that model as Moses en-es-100k , because it was trained using the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"995","class":"","sentence1":"In the scoring step, we additionally apply the turboparser (Martins et al., 2010), which is based on linear programming relaxations.","sentence2":"As a second parser, we use turboparser (Martins et al., 2010), a parser based on linear programming relaxations."},
{"id_case":"996","class":"","sentence1":"Moreover, it also outperforms the integer linear programming (ILP) method for timeline construction proposed in (Do et al., 2012).","sentence2":"We also compare the proposed methods with an Integer Linear Programming (ILP) based method for timeline construction (Do et al., 2012)."},
{"id_case":"997","class":"","sentence1":"Once we have converted our sentences into parse trees, we train a discriminative constituency parser similar to that of (Finkel et al., 2008).","sentence2":"Once each sentence has been converted into a tree, we train a discriminative constituency parser, based on (Finkel et al., 2008)."},
{"id_case":"998","class":"","sentence1":"This concatenated corpus, tokenized, POS-tagged and lemmatized with the TreeTagger (Schmid, 1995), contains about 2.83 billion tokens.","sentence2":"The corpus has been tokenized , POS-tagged and lemmatized with the TreeTagger (Schmid, 1995), and it contains about 2.8 billion tokens."},
{"id_case":"999","class":"","sentence1":"We take the CRF model of (Mitchell et al., 2013) as the baseline, and explore two research questions .","sentence2":"We take the models of (Mitchell et al., 2013) as our baseline, which are standard CRF, with discrete manual features."},
{"id_case":"1000","class":"","sentence1":"Our system is a linear model estimated using ridge regression, as implemented in the scikit-learn toolkit (Pedregosa et al., 2011).","sentence2":"We trained a linear log-loss model using stochastic gradient descent learning as implemented in the Scikit learn library (Pedregosa et al., 2011)."},
{"id_case":"1501","class":"","sentence1":"We use fast-align (Dyer et al., 2013) to align the full training corpus (source and reference) along with Machine Translation output.","sentence2":"We use incremental fast-align (Dyer et al., 2013) to align the input and output of the neural Machine Translation system."},
{"id_case":"1502","class":"","sentence1":"We then describe in more detail a modern Chinese corpus, the Penn Chinese Treebank (Xue et al., 2005).","sentence2":"We used the Penn Chinese Treebank guidelines (Xue et al., 2005) in searching for a suitable set of reordering rules."},
{"id_case":"1503","class":"","sentence1":"We use a minibatch size of 100, and use AdaDelta (Zeiler, 2012).","sentence2":"We used Adadelta (Zeiler, 2012) with a minibatch size of 80."},
{"id_case":"1504","class":"","sentence1":"We experimented with a number of classification algorithms from the scikit-learn package (Pedregosa et al., 2011): Support Vector Machine with a linear kernel (SVM), Logistic Regression (LR).","sentence2":"We used several regression algorithms including Support Vector Regression (SVR) with 3 different kernels (i.e., linear, polynomial and rbf), Random Forest, Stochastic Gradient Descent (SGD)."},
{"id_case":"1504","class":"","sentence1":"We experimented with a number of classification algorithms from the scikit-learn package (Pedregosa et al., 2011): Support Vector Machine with a linear kernel (SVM), Logistic Regression (LR).","sentence2":"We used several regression algorithms including Support Vector Regression (SVR) with 3 different kernels (i.e., linear, polynomial and rbf), Random Forest, Stochastic Gradient Descent (SGD)."},
{"id_case":"1505","class":"","sentence1":"The Spanish-English (S2E) training corpus was drawn from the Europarl collection (Koehn, 2005).","sentence2":"The baseline phrase-based system was trained on the German-to-English corpus included in Europarl v7 (Koehn, 2005)."},
{"id_case":"1506","class":"","sentence1":"The word alignment was trained using GIZA++ (Och and Ney, 2003) with the configuration grow-diag-final-and.","sentence2":"In all experiments, word alignment was obtained using the grow-diag-final-and heuristic for symmetrizing GIZA++ (Och and Ney, 2003) alignments."},
{"id_case":"1507","class":"","sentence1":"Inter-annotator agreement on the training corpus was above 85% F-measure, which is considered high (Artstein and Poesio, 2008).","sentence2":"Nonetheless, agreement was only 0.33, which is considered very low (Artstein and Poesio, 2008)."},
{"id_case":"1508","class":"","sentence1":"The coarse categories are the universal POS tag set described by Petrov et al. (2012) .","sentence2":"We use the Universal POS Tagset (UPOS) of Petrov et al. (2012) ."},
{"id_case":"1509","class":"","sentence1":"In this work, the translation performance is measured with case-insensitive BLEU score and TER score (Snover et al., 2006).","sentence2":"Translation quality is measured in case-insensitive with BLEU and TER (Snover et al., 2006)."},
{"id_case":"1510","class":"","sentence1":"To obtain these we use the Stanford dependency parser (de Marneffe et al., 2006) and the forced alignment from Section.","sentence2":"We use Stanford parser (de Marneffe et al., 2006) to obtain parse trees and dependency relations."},
{"id_case":"1511","class":"","sentence1":"The numbers in boldface are significantly better than the MERT baseline measured by the bootstrap resampling (Koehn, 2004).","sentence2":"Scores with a dagger are significantly better than the MERT BASELINE paired bootstrap resampling tes (Koehn, 2004))."},
{"id_case":"1512","class":"","sentence1":"To obtain these we use the Stanford dependency parser (de Marneffe et al., 2006) and the forced alignment from Section 3.9.","sentence2":"We use Stanford parser (de Marneffe et al., 2006) to obtain parse trees and dependency relations."},
{"id_case":"1513","class":"","sentence1":" The Switchboard corpus (Godfrey et al., 1992) is a corpus of telephone conversations on selected topics.","sentence2":"The Switchboard Dialog Act Corpus (SWDA) is annotated on the Switchboard Corpus of human conversational telephone speech (Godfrey et al., 1992)."},
{"id_case":"1514","class":"","sentence1":"For calculating the required frequencies, we use the Web1T corpus (Brants and Franz, 2006).","sentence2":"For word frequency we use the unigrams data from the Google Web1T collection (Brants and Franz, 2006)."},
{"id_case":"1515","class":"","sentence1":"We consider the following types of implicit referents: The brat tool (Stenetorp et al., 2012) was used for annotation.","sentence2":"Afterwards, we used the brat rapid annotation tool (Stenetorp et al., 2012) for the annotation of the dataset."},
{"id_case":"1516","class":"","sentence1":"The data was tagged using TnT (Brants, 2000).","sentence2":"HCRC is tagged with TnT (Brants, 2000)."},
{"id_case":"1517","class":"","sentence1":"The Web 1T 5-gram corpus (Brants and Franz, 2006) is used for language modeling and collecting web Ngram counts.","sentence2":"The WEB 1T 5-GRAM CORPUS (Brants and Franz, 2006) is used for language modeling."},
{"id_case":"1518","class":"","sentence1":"The Spanish-English (S2E) training corpus was drawn from the Europarl collection (Koehn, 2005).","sentence2":"The parallel data come from the Europarl corpus version 7 (Koehn, 2005) and Kaist Corpus 4 ."},
{"id_case":"1519","class":"","sentence1":"ROUGE tool (Lin, 2004) was employed for automatic content evaluation, which allows the comparison between automatic and model summaries based on different types of n-grams.","sentence2":"There are also automatic methods for summary evaluation, such as ROUGE (Lin, 2004), which gives a score based on the similarity in the sequences of words between a human-written model summary and system summary."},
{"id_case":"1520","class":"","sentence1":"Note that cdb is a subset of the training corpus used in the CoNLL 2006 shared task (Buchholz and Marsi, 2006).","sentence2":"As the Dutch tagger used in the CoNLL 2006 shared task did not have the concept of multiwords , the organizers chose to treat them as a single token (Buchholz and Marsi, 2006)."},
{"id_case":"1521","class":"","sentence1":"We use the class of recursive matrix-vector models as the basis for our investigation; for a detailed introduction see the MV-RNN model described in Socher et al. (2012) .","sentence2":"We adopt the setting of Socher et al. (2012) ."},
{"id_case":"1522","class":"","sentence1":"We used the English side of the Europarl corpus (Koehn, 2005).","sentence2":"We used a subset of the data provided for the Second Workshop on Statistical Machine Translation 2 , which consists mainly of texts from the Europarl corpus (Koehn, 2005)."},
{"id_case":"1523","class":"","sentence1":"The parallel data come from the Europarl corpus version 7 (Koehn, 2005).","sentence2":"The statistical dictionary for this task was extracted from the Europarl 7 corpus (Koehn, 2005)."},
{"id_case":"1524","class":"","sentence1":"We built a source-to-target PB-SMT model from the bilingual domain corpus using the Moses toolkit (Koehn et al., 2007).","sentence2":"We trained a model using Moses toolkit (Koehn et al., 2007) on the training data as our baseline system."},
{"id_case":"1525","class":"","sentence1":"Inference can be done with Gibbs sampling, which is commonly used in LDA models (Griffiths and Steyvers, 2004).","sentence2":"We propose to use collapsed Gibbs sampling, which has been successfully used in LDA (Griffiths and Steyvers, 2004)."},
{"id_case":"1527","class":"","sentence1":"We compare the model against the Moses phrase-based translation system (Koehn et al., 2007).","sentence2":"We also compare with the standard phrase-based system of Moses (Koehn et al., 2007)."},
{"id_case":"1528","class":"","sentence1":"We use some ML methods included in the Weka toolkit (Hall et al., 2009) in order to combine results obtained in experiments using single knowledge sources (described in section 3.3).","sentence2":"We conducted experiments using Multinomial Naive Bayes classifier implemented in the Weka toolkit (Hall et al., 2009)."},
{"id_case":"1529","class":"","sentence1":"Koo et al. (2008) presented new features based on word clusters obtained from large-scale unlabeled data and achieved large improvement for English and Czech.","sentence2":"Koo et al. (2008)  have proposed to use word clusters as features to improve graph-based statistical dependency parsing for English and Czech."},
{"id_case":"1530","class":"","sentence1":"Recognizing Textual Entailment (RTE) is the task of determining whether a given textual statement (a hypothesis), H, can be inferred by a given text passage, T (Dagan et al., 2005).","sentence2":"Given an entailment text T and a hypothesis text H, T entails H if H can be inferred from the contents of T (Dagan et al., 2005)."},
{"id_case":"1531","class":"","sentence1":"Abstract Meaning Representation (AMR) is an annotation framework designed to capture the meaning of a sentence with a single rooted, directed graph (Banarescu et al., 2013).","sentence2":"Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph."},
{"id_case":"1532","class":"","sentence1":"For the classification algorithm, we employed a Support Vector Machine (SVM) with a linear kernel, using the LIBSVM package (Chang and Lin, 2011).","sentence2":"We train a Support Vector Machines classifier with RBF kernel for the binary classification task and a Support Vector regression model with RBF kernel for the regression task using the LibSVM package  (Chang and Lin, 2011)."},
{"id_case":"1536","class":"","sentence1":"Two clinicians (RB and SW) annotated the reports using BRAT (Stenetorp et al., 2012), a collaborative tool for text annotation that was configured to use our own schema.","sentence2":"Each dataset was annotated by a linguist (annotator A) using BRAT (Stenetorp et al., 2012), a webbased annotation tool, which was configured appropriately for the needs of the task."},
{"id_case":"1538","class":"","sentence1":"We built a 5-gram language model on the English side of QCA-train using KenLM (Heafield, 2011).","sentence2":"For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with Kneser-Ney smoothing."},
{"id_case":"1540","class":"","sentence1":"Statistical word alignments are learned in both directions with GIZA++ (Och and Ney, 2003), then combined with the grow-diag-final heuristic.","sentence2":"Once the MRL has been converted into such a structure (for an example see Figure 2), a word aligner, here GIZA++ (Och and Ney, 2003), can be used to generate word-to-word alignments in both translation."},
{"id_case":"1542","class":"","sentence1":"Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, represents texts by labeled hierarchical structures, called Discourse Trees (DTs).","sentence2":"Discourse structure in summarization Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) represents the discourse in a document in the form of a tree."},
{"id_case":"1543","class":"","sentence1":"Our similarity measure is based on a proposal in (Lin, 1997), where the similarity between two objects is defined to be the amount of information contained in the commonality between the objects.","sentence2":"The similarity measure simwN is based on the proposal in (Lin, 1997)."},
{"id_case":"1544","class":"","sentence1":"We user the training script of the Moses toolkit (Koehn et al., 2007).","sentence2":"For this purpose, we use the Moses toolkit to build a phrase-based statistical MT system (Koehn et al., 2007), with training data from the translation task of the WMT 2013 workshop."},
{"id_case":"1545","class":"","sentence1":"We used the relation classification dataset of the SemEval 2010 task 8 (Hendrickx et al., 2010).","sentence2":"In 2010, manually annotated data for relation classification was released in the context of a SemEval shared task (Hendrickx et al., 2010)."},
{"id_case":"1546","class":"","sentence1":"We first use a dependency parser (de Marneffe et al., 2006) to parse each sentence and extract the set of dependency relations.","sentence2":"We also obtain the dependency parse of the sentences using the Stanford parser (De Marneffe et al., 2006)."},
{"id_case":"1547","class":"","sentence1":"We first use a dependency parser (de Marneffe et al., 2006) to parse each sentence and extract the set of dependency relations associated with the sentence .","sentence2":"We also obtain the dependency parse of the sentences using the Stanford parser (De Marneffe et al., 2006)."},
{"id_case":"1548","class":"","sentence1":"We experiment with the original text-based semantic model used to predict fMRI patterns by (Mitchell et al., 2008).","sentence2":"We follow the paradigm proposed by (Mitchell et al., 2008) for fMRI data."},
{"id_case":"1549","class":"","sentence1":"The phrases were extracted and scored using the Moses training tools (Koehn et al., 2007).","sentence2":"All training data were pre-processed with standard tools from the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"1550","class":"","sentence1":"We used a phrase-based statistical machine translation model as implemented in the Moses toolkit (Koehn et al., 2007) for machine translation.","sentence2":"We use the state-of-the-art phrase-based machine translation system Moses (Koehn et al., 2007) top perform our machine translation experiments."},
{"id_case":"1551","class":"","sentence1":"This is a simple form of back off smoothing by linear interpolation (Jelinek and Mercer, 1980).","sentence2":"As in Jelinek-Mercer smoothing (Jelinek and Mercer, 1980), it is a linear interpolation of N-gram language models for N = 1 ."},
{"id_case":"1552","class":"","sentence1":"In each case, the improvement of EBMT TM + SMT over the baseline SMT is statistically significant (reliability of 98%) using bootstrap resampling (Koehn, 2004).","sentence2":"The improvement is statistically significant according to paired bootstrap resampling test (Koehn, 2004)."},
{"id_case":"1553","class":"","sentence1":"Our model has been applied to English, using the UKWaC corpus (Baroni et al., 2009).","sentence2":"The model for English has been trained on part of the UKWaC corpus (Baroni et al., 2009)."},
{"id_case":"1554","class":"","sentence1":"The difference between PDTB and the RST Discourse Treebank is the discourse organization framework, which in the case of the RST Discourse Treebank is the Rhetorical Structure Theory  (Mann and Thompson, 1988).","sentence2":"The closest area to our work consists of investigations of discourse relations in the context of Rhetorical Structure Theory (Mann and Thompson, 1988)."},
{"id_case":"1557","class":"","sentence1":"We employ an ranking-based objective function by (dos Santos et al., 2015).","sentence2":"We employ the ranking-based objective function following (dos Santos et al., 2015)."},
{"id_case":"1558","class":"","sentence1":"We train, evaluate and analyze the models on the English data of the CoNLL-2012 shared task on multilingual coreference resolution (Pradhan et al., 2012).","sentence2":"We evaluate our method on the following data sets: OntoNotes-Dev: Development set of the OntoNotes data provided by the CoNLL2012 shared task (Pradhan et al., 2012)."},
{"id_case":"1559","class":"","sentence1":"An unpruned, modified Kneser-Ney-smoothed 4-gram language model is estimated using the KenLM toolkit (Heafield et al., 2013).","sentence2":"Integration with the paraphrase language model is performed using incremental search (Heafield et al., 2013)."},
{"id_case":"1560","class":"","sentence1":"Feature weights, based on BLEU, are then tuned using Z- MERT (Zaidan, 2009).","sentence2":"The weights of these features are tuned using Z- MERT (Zaidan, 2009) on a development set."},
{"id_case":"1561","class":"","sentence1":"We apply the stochastic gradient descent algorithm with mini-batches and the AdaDelta update rule (Zeiler, 2012).","sentence2":"All parameters are updated with mini-batches by AdaDelta (Zeiler, 2012) gradient descent method."},
{"id_case":"1562","class":"","sentence1":"A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004).","sentence2":"For statistical significance testing, we use a paired bootstrap resampling method proposed in (Koehn, 2004)."},
{"id_case":"1563","class":"","sentence1":"One of the phrase-based systems moreover utilizes a lexicalized reordering model (Galley and Manning, 2008).","sentence2":"The experiments were conducted on top of the standard hierarchical lexicalized reordering model (Galley and Manning, 2008)."},
{"id_case":"1565","class":"","sentence1":"We compare the model against the Moses phrase-based translation system (Koehn et al., 2007), applied to phoneme sequences.","sentence2":"We use the state-of-the-art phrase-based machine translation system Moses (Koehn et al., 2007) to perform our machine translation experiments."},
{"id_case":"1566","class":"","sentence1":"We measure statistical significance computed with paired bootstrap resampling (Koehn, 2004).","sentence2":"A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004)."},
{"id_case":"1568","class":"","sentence1":"Our implementation uses an opensource version of the LBFGS optimization technique (Liu and Nocedal, 1989).","sentence2":"Our implementation of the model adopts LBFGS algorithm for parameter optimization as usual (Liu and Nocedal, 1989)."},
{"id_case":"1569","class":"","sentence1":"Weka (Hall et al., 2009) which contains the implementation of all three algorithms was used in our study.","sentence2":"The SVM implementation of Weka (Hall et al., 2009) was used to build the CLTE model."},
{"id_case":"1570","class":"","sentence1":"We trained all models using AdaGrad (Duchi et al., 2011).","sentence2":"We use AdaGrad (Duchi et al., 2011) parameter updates."},
{"id_case":"1571","class":"","sentence1":"Experiment conducted using the Moses phrase-based decoder (Koehn et al., 2007).","sentence2":"Decoding is performed using Moses (Koehn et al., 2007)."},
{"id_case":"1572","class":"","sentence1":"Both were trained on Europarl (Koehn, 2005).","sentence2":"European-language bitexts were extracted from Europarl (Koehn, 2005)."},
{"id_case":"1573","class":"","sentence1":"Decoding is performed using Moses (Koehn et al., 2007).","sentence2":"Was conducted using the Moses phrase-based decoder (Koehn et al., 2007)."},
{"id_case":"1574","class":"","sentence1":"We built phrase-based machine translation systems using the open software toolkit Moses (Koehn et al., 2007).","sentence2":"The Edinburgh\/JHU phrase-based translation systems for our participation in the WMT 2015 shared translation task 1 are based on the open source Moses toolkit (Koehn et al., 2007)."},
{"id_case":"1575","class":"","sentence1":"The former variant (to use only MT text on the source side) was proposed by Simard et al. (2007).","sentence2":"One is the monolingual statistical translation approach proposed by Simard et al. (2007) ."},
{"id_case":"1577","class":"","sentence1":"The baseline systems are built with the opensource phrase-based SMT toolkit Moses (Koehn et al., 2007).","sentence2":"The Edinburgh\/JHU phrase-based translation systems for our participation in the WMT 2015 shared translation task 1 are based on the open source Moses toolkit (Koehn et al., 2007)."},
{"id_case":"1578","class":"","sentence1":"The experiments reported in Stevenson (Stevenson et al., 2000), which show a main effect of thematic focusing.","sentence2":"Stevenson (Stevenson et al., 2000) report a main effect of semantic focusing in consequence-so continuations."},
{"id_case":"1579","class":"","sentence1":"We use the scikit implementation of Random Forest (Pedregosa et al., 2011).","sentence2":"We use the Bernoulli Naive Bayes classifier in scikit with the default settings (Pedregosa et al., 2011)."},
{"id_case":"1581","class":"","sentence1":"The source domain is from the CoNLL03 shared task (Tjong Kim Sang and De Meulder, 2003) and the target domain is from the MUC7 dataset.","sentence2":"AIDA\/YAGO is derived from the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003)."},
{"id_case":"1582","class":"","sentence1":"We found that using AdaGrad (Duchi et al., 2011) to update the parameters is very effective .","sentence2":"We adopt online learning, updating parameters using AdaGrad (Duchi et al., 2011)."},
{"id_case":"1583","class":"","sentence1":"We used GIZA++ (Och and Ney, 2003) to align the words in the corpus.","sentence2":"We used GIZA++ (Och and Ney, 2003) along with the growing heuristics to word-align the training corpus."},
{"id_case":"1584","class":"","sentence1":"We used the Moses toolkit (Koehn et al., 2007) with its default settings.","sentence2":"We tokenize and frequent-case the data with the standard scripts from the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"1585","class":"","sentence1":"We calculate our features using the KenLM toolkit (Heafield, 2011).","sentence2":"We built a trigram language model with Kneser-Ney smoothing using KenLM toolkit (Heafield, 2011)."},
{"id_case":"1586","class":"","sentence1":"The source domain is from the CoNLL03 shared task (Tjong Kim Sang and De Meulder, 2003) and the target domain is from the MUC7 dataset.","sentence2":"AIDA\/YAGO is derived from the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003)."},
{"id_case":"1587","class":"","sentence1":"We used standard classifiers available in scikit-learn package (Pedregosa et al., 2011).","sentence2":"Specifically, we used the standard Gradient Boosting Regressor in the scikit-learn toolkit 4 (Pedregosa et al., 2011)."},
{"id_case":"1588","class":"","sentence1":"For training the translation model and for decoding we used the Moses toolkit (Koehn et al., 2007).","sentence2":"Finally, we used Moses toolkit as phrase-based reference (Koehn et al., 2007)."},
{"id_case":"1589","class":"","sentence1":"For training SVM classifiers we used LIBSVM package (Chang and Lin, 2001).","sentence2":"For the hypernode identification, we trained a binary polynomial SVM from LIBSVM (Chang and Lin, 2001)."},
{"id_case":"1590","class":"","sentence1":"We used GIZA++ (Och and Ney, 2003) along with the growing heuristics to word alignments the training corpus.","sentence2":"We used the GIZA++ software (Och and Ney, 2003) to do the word alignments."},
{"id_case":"1591","class":"","sentence1":"We used the English side of the Europarl corpus (Koehn, 2005).","sentence2":"The source of bilingual data used in the experiments is the Europarl collection (Koehn, 2005)."},
{"id_case":"1592","class":"","sentence1":"The weights initialization follows the normalized initialization proposed in (Glorot and Bengio, 2010).","sentence2":"For training, we randomly initialized parameters in accordance with the normalized initialization (Glorot and Bengio, 2010)."},
{"id_case":"1593","class":"","sentence1":"We measure significance of results using bootstrap resampling (Koehn, 2004).","sentence2":"We used bootstrap resampling for testing statistical significance (Koehn, 2004)."},
{"id_case":"1594","class":"","sentence1":"We tokenize English data and segment Chinese data using the Stanford CoreNLP toolkit (Manning et al., 2014).","sentence2":"We also lemmatized all words using Stanford CoreNLP (Manning et al., 2014)."},
{"id_case":"1595","class":"","sentence1":"We measure significance of results using bootstrap resampling at p < 0.05 (Koehn, 2004).","sentence2":"Statistical significance tests are performed using bootstrap resampling (Koehn, 2004)."},
{"id_case":"1596","class":"","sentence1":"We tokenize English data and segment Chinese data using the Stanford CoreNLP toolkit (Manning et al., 2014).","sentence2":"We also lemmatized all words using Stanford CoreNLP (Manning et al., 2014)."},
{"id_case":"1598","class":"","sentence1":"We have used the semantic frames of FrameNet-1.5 (Baker et al., 1998) to obtain these semantic categories.","sentence2":"We consider the continuous embedding of semantic frames (Baker et al., 1998)."},
{"id_case":"1599","class":"","sentence1":"All novels were lemmatized and POS-tagged using TreeTagger (Schmid, 1994).","sentence2":"All English data are POS tagged and lemmatized using the TreeTagger (Schmid, 1994)."},
{"id_case":"1600","class":"","sentence1":"The word alignment was obtained by running Giza++ (Och and Ney, 2003).","sentence2":"The phrase table was generated employing symmetrised word alignments obtained with GIZA++ (Och and Ney, 2003)."},
{"id_case":"2101","class":"","sentence1":"Statistics are shown in  We built unpruned modified Kneser-Ney language models using lmplz (Heafield et al., 2013).","sentence2":"The language models are estimated using the KenLM toolkit (Heafield et al., 2013) with modified Kneser-Ney smoothing."},
{"id_case":"2102","class":"","sentence1":"We conducted experiments using Multinomial Naive Bayes classifier implemented in the Weka toolkit (Hall et al., 2009).","sentence2":"We train and evaluate the classifiers in a 10-fold crossvalidation using Weka (Hall et al., 2009)."},
{"id_case":"2103","class":"","sentence1":"We use the Stanford parser to generate a DG for each sentence (de Marneffe et al., 2006).","sentence2":"As for EJ translation, we use the Stanford parser (de Marneffe et al., 2006) to obtain English abstraction trees."},
{"id_case":"2104","class":"","sentence1":"Specifically, we used the python scikit-learn module (Pedregosa et al., 2011), which interfaces with the widely-used libsvm .","sentence2":"Specifically, we used the standard Gradient Boosting Regressor in the scikit-learn toolkit 4 (Pedregosa et al., 2011)."},
{"id_case":"2105","class":"","sentence1":"We use the Stanford parser to generate a DG for each sentence (de Marneffe et al., 2006).","sentence2":"As for EJ translation, we use the Stanford parser (de Marneffe et al., 2006) to obtain English abstraction trees."},
{"id_case":"2106","class":"","sentence1":"Then we try entity-aspect model proposed by Li et al. (2010) to do sentence clustering.","sentence2":"The most related extension is entity-aspect model proposed by Li et al. (2010) ."},
{"id_case":"2108","class":"","sentence1":"The system will use handwritten grammars in the LFG formalism for English, there is an already-existing wide-coverage one developed for XLE as part of the ParGram project (Butt et al., 2002)","sentence2":"The experiments reported in this paper use the German LFG grammar constructed as part of the ParGram project (Butt et al., 2002)."},
{"id_case":"2109","class":"","sentence1":"Marton et al. (2009)  use a monolingual text on the source side to find paraphrases to oov words for which the translations are available .","sentence2":"Marton et al. (2009) was the first to successfully integrate a collocational approach to finding translations for oov words into an end-to-end SMT system ."},
{"id_case":"2110","class":"","sentence1":"Corpus-based VSMs follow the standard \" distributional hypothesis, \" which states that words appearing in the same contexts tend to have similar meaning (Harris, 1954).","sentence2":"Similar row vectors in T indicate similar context of two terms in the domain, and terms that occur in the same contexts tend to have similar meanings (Harris, 1954) ."},
{"id_case":"2111","class":"","sentence1":"In (Magnini et al., 2002) it has been claimed that domain information provides generalized features at the paradigmatic level that are useful to discriminate among word senses.","sentence2":"In fact, in (Magnini et al., 2002), it has been claimed that knowing the domain of the text in which the word is located is a crucial information for WSD."},
{"id_case":"2112","class":"","sentence1":"These methods are based on the distributional hypothesis which states that words appearing in the same contexts tend to have similar meaning (Harris, 1954).","sentence2":"Similar row vectors in T indicate similar context of two terms in the domain, and terms that occur in the same contexts tend to have similar meanings (Harris, 1954) "},
{"id_case":"2113","class":"","sentence1":"The resulting clusters thus contain words occurring in similar lexical contexts and are supposed to be semantically related according to the distributional hypothesis (Harris, 1954).","sentence2":"These methods are based on the distributional hypothesis which states that words appearing in the same contexts tend to have similar meaning (Harris, 1954)."},
{"id_case":"2114","class":"","sentence1":"The resulting clusters thus contain words occurring in similar lexical contexts and are supposed to be semantically related according to the distributional hypothesis (Harris, 1954).","sentence2":"Distributional semantics (see Cohen and Widdows (2009) for an overview) is based on the observation that words that occur in similar contexts tend to be semantically related (Harris, 1954)."},
{"id_case":"2115","class":"","sentence1":"We run our experiments on Europarl (Koehn, 2005), a multilingual parallel corpus extracted from the proceedings of the European Parliament.","sentence2":"In a large body of work on multilingual word representations , Europarl (Koehn, 2005) is the preferred source of parallel data."},
{"id_case":"2117","class":"","sentence1":"The SemEval-2017 Task 4 (Rosenthal et al., 2017) focuses on the classification of tweets into positive , neutral and negative sentiment classes.","sentence2":"The task requires classifying the sentiment of single Arabic tweets into one of the classes: positive, negative or neutral (Rosenthal et al., 2017)."},
{"id_case":"2119","class":"","sentence1":"For French-English experiments, we used the EMEA parallel corpus (Tiedemann, 2009), which are medical documents from the European Medecines Agency.","sentence2":"EMEA Corpus (Tiedemann, 2009): EMEA is a parallel corpus made out of PDF documents from the European Medicines Agency."},
{"id_case":"2120","class":"","sentence1":"The translation model was trained by GIZA++ (Och and Ney, 2003), and the trigram was trained by the CMU-Cambridge Statistical Language Modeling Toolkit v2 (Clarkson and Rosenfeld, 1997).","sentence2":"The aligned model was trained with GIZA++ (Och and Ney, 2003), which implements the most typical IBM and HMM alignment models."},
{"id_case":"2121","class":"","sentence1":"In this section, we would empirically compare the two implementations: WBD with metaprobability classification (Huang et al., 2007) and WBD with character tagging with CRF.","sentence2":"In this work, we analyze the relationship between WBD (Huang et al., 2007) and 4-tag character tagging approach for Chinese word segmentation."},
{"id_case":"2122","class":"","sentence1":"We used bootstrap resampling for testing statistical significance (Koehn, 2004).","sentence2":"Statistical significance tests are performed using bootstrap resampling (Koehn, 2004)."},
{"id_case":"2123","class":"","sentence1":"Our data were obtained from the RST-DT (Carlson et al., 2002), which consists of 385 Wall Street Journal articles.","sentence2":"Our first corpus is the standard RST-DT (Carlson et al., 2002), which consists of 385 Wall Street Journal articles."},
{"id_case":"2124","class":"","sentence1":"For tuning , we used the batch MIRA (Cherry and Foster, 2012).","sentence2":"We tune the systems using kbest batch MIRA (Cherry and Foster, 2012)."},
{"id_case":"2125","class":"","sentence1":"The specific classification software we utilize is LibSVM (Chang and Lin, 2001).","sentence2":"The learning package we use is LIBLINEAR (Chang and Lin, 2001) 3 ."},
{"id_case":"2126","class":"","sentence1":"Training and test data is pre-processed using RASP (Briscoe et al., 2006).","sentence2":"We lemmatised all prompts and essays using RASP (Briscoe et al., 2006)."},
{"id_case":"2129","class":"","sentence1":"For this purpose we use the Europarl corpus (Koehn, 2005).","sentence2":"We used the English side of the Europarl corpus (Koehn, 2005)."},
{"id_case":"2130","class":"","sentence1":"The significance test is based on computationally intensive randomizations as described in (Yeh and Church, 2000).","sentence2":"We used the significance test described in (Yeh and Church, 2000), it is based on computationally intensive randomizations."},
{"id_case":"2132","class":"","sentence1":"We used the SVM light package (Joachims, 2002) in our experiment.","sentence2":"For experiments, we utilized the SVM light package (T. Joachims, 2002)."},
{"id_case":"2134","class":"","sentence1":"This is referred to as an IOB representation (Ramshaw and Marcus, 1995).","sentence2":"This is similar to results in the literature (Ramshaw and Marcus, 1995)."},
{"id_case":"2135","class":"","sentence1":"We used the implementation of the scikit-learn module (Pedregosa et al., 2011).","sentence2":"We used standard classifiers available in scikit-learn package (Pedregosa et al., 2011)."},
{"id_case":"2136","class":"","sentence1":"The corpus was then automatically tagged with part-of-speech information, using TreeTagger (Schmid, 1994).","sentence2":"The February 2013 dumps of Wikipedia articles were downloaded and cleaned for both languages after which the English corpus was tokenized, part-of-speech (PoS) tagged and lemmatized with the TreeTagge (Schmid, 1994)."},
{"id_case":"2138","class":"","sentence1":"The Google N-gram corpus is a collection of English N-grams, ranging from one to five N-grams, and their respective frequency counts observed on the Web (Brants and Franz, 2006).","sentence2":"This technique is applied to the 5-grams from the Google corpus (Brants and Franz, 2006) and the BNC (creating the lists 5-grams and BNCcaps)."},
{"id_case":"2139","class":"","sentence1":"We use the Moses software package 5 to train a PBMT model (Koehn et al., 2007).","sentence2":"Translation setup We use the Moses decoder (Koehn et al., 2007) to train a phrase-based MT system on the English German Common crawl parallel corpus and WMT news test 2010 (tuning)."},
{"id_case":"2141","class":"","sentence1":"It has been shown in previous work on relation extraction that the shortest dependency path between any two entities captures the information required to assert a relationship between them (Bunescu and Mooney, 2005).","sentence2":"This can be attributed to the intuitive advantage of dependency trees where the shortest path between entities captures most of the information about the relation (Bunescu and Mooney, 2005)."},
{"id_case":"2143","class":"","sentence1":"Second, to avoid the error propagation problem inherent in the pipeline approach, we perform joint inference over the outputs of the ACI and RI classifiers in an Integer Linear Programming (Roth and Yih, 2004).","sentence2":"To enforce global consistency, we employ global constraints implemented in the Integer Linear Programming (ILP) framework (Roth and Yih, 2004)."},
{"id_case":"2149","class":"","sentence1":"Similar row vectors in T indicate similar context of two terms in the domain, and terms that occur in the same contexts tend to have similar meanings (Harris, 1954)","sentence2":"According to the distributional hypothesis of meaning (Harris, 1954), words that occur in similar contexts tend to be semantically similar."},
{"id_case":"2150","class":"","sentence1":"We cluster the typed predicate vectors using the Chinese Whispers algorithm (Biemann, 2006).","sentence2":"We use the Chinese Whispers clustering algorithm (Biemann, 2006)."},
{"id_case":"2152","class":"","sentence1":"The training set is used to train the phrase-based translation model and language model for Moses (Koehn et al., 2007).","sentence2":"The Moses toolkit (Koehn et al., 2007) is used for phrase extraction."},
{"id_case":"2153","class":"","sentence1":"We use the CRF-suite package (Okazaki, 2007) for our baseline.","sentence2":"We use the CRF-Suite toolkit (Okazaki, 2007) for our experiments."},
{"id_case":"2154","class":"","sentence1":"We use Ridge Regression (RR) with l2-norm regularization and Support Vector Regression (SVR) with an RBF kernel from scikit-learn (Pedregosa et al., 2011).","sentence2":"We experimented with a number of classification algorithms from the scikit-learn package (Pedregosa et al., 2011)."},
{"id_case":"2157","class":"","sentence1":"We used Mallet toolkit (McCallum, 2002) for CRF implementation .","sentence2":"We used the Mallet toolkit (McCallum, 2002) for learning maximum entropy models for all our experiments."},
{"id_case":"2158","class":"","sentence1":"We extract instances from the NUCLE corpus (Dahlmeier and Ng, 2011), an error annotated corpus mainly written by Singaporean students, to conduct this study.","sentence2":"In the NUCLE corpus (Dahlmeier and Ng, 2011), an annotated learner corpus comprised of essays written by primarily Singaporean students, 13.71% errors are tagged as \" local redundancy errors \"."},
{"id_case":"2159","class":"","sentence1":"Sentiment Analysis or \" Subjectivity Analysis \" in (Liu, 2010) is defined as the computational treatment of opinions, sentiments and emotions expressed in a text.","sentence2":"Automatic techniques such as Opinion Mining and Sentiment Analysis (Liu, 2010) allow us to determine the views expressed in a piece of textual data."},
{"id_case":"2161","class":"","sentence1":"Distributional representations encode an expression by its environment, assuming the contextdependent nature of meaning according to which one \" shall know a word by the company it keeps \" (Firth, 1957).","sentence2":"Distributional Semantic Models are based on the intuition that \" a word is characterized by the company it keeps \" (Firth, 1957)."},
{"id_case":"2162","class":"","sentence1":"Machine translation system settings: We used a phrase-based statistical machine translation model as implemented in the Moses toolkit (Koehn et al., 2007) for machine translation.","sentence2":"In current phrase-based statistical machine translation systems such as Moses (Koehn et al., 2007), the translation model is defined in terms of phrase pairs (biphrases) extracted from a bilingual corpus."},
{"id_case":"2165","class":"","sentence1":"For training, we use word2vec  (Levy and Goldberg, 2014) which allows training for arbitrary context using the skipgram model.","sentence2":"For all context types other than BOW we use the word2vec package of (Levy and Goldberg, 2014), 5 which augments the standard word2vec toolkit with code that allows arbitrary context definition."},
{"id_case":"2166","class":"","sentence1":"We experimented with a number of classification algorithms from the scikit-learn package (Pedregosa et al., 2011).","sentence2":"We used SVM implementations from scikit-learn (Pedregosa et al., 2011) and experimented with a number of classifiers."},
{"id_case":"2167","class":"","sentence1":"The statistical significance test is also carried out with paired bootstrap resampling method (Koehn, 2004).","sentence2":"For statistical significance testing, we use a paired bootstrap resampling method proposed in (Koehn, 2004)."},
{"id_case":"2169","class":"","sentence1":"We parse the sentences with the Stanford Parser (de Marneffe et al., 2006).","sentence2":"Learning We learn the model from data {(w i , d i , I i) | i = 1 . . . m} containing sentences w i , dependency trees d i , computed with the Stanford parser (de Marneffe et al., 2006)."},
{"id_case":"2170","class":"","sentence1":"We use the Moses toolkit (Koehn et al., 2007) to create a statistical phrase-based machine translation model.","sentence2":"We use the state-of-the-art phrase-based machine translation system Moses (Koehn et al., 2007)."},
{"id_case":"2171","class":"","sentence1":"We evaluate our proposed ILP approach on the test data from the Helping Our Own (HOO) 2011 shared task (Dale and Kilgarriff, 2011).","sentence2":"One of the classes of errors in the Helping Our Own (HOO) 2011 shared task (Dale and Kilgarriff, 2011) was punctuation."},
{"id_case":"2173","class":"","sentence1":"This data is part of the NUCLE corpus (Dahlmeier et al., 2013).","sentence2":"The training data provided for the task is a subset of the NUCLE v2.3 corpus (Dahlmeier et al., 2013)."},
{"id_case":"2175","class":"","sentence1":"We parse the sentences with the Stanford Parser (de Marneffe et al., 2006).","sentence2":"Learning We learn the model from data {(w i , d i , I i) | i = 1 . . . m} containing sentences w i , dependency trees d i , computed with the Stanford parser (de Marneffe et al., 2006)"},
{"id_case":"2177","class":"","sentence1":"The surface heuristic can define consistency according to any word alignment; but most often, the alignment is provided by GIZA++ (Och and Ney, 2003).","sentence2":"The word alignment is created by GIZA++ (Och and Ney, 2003); the intersection symmetrization is used."},
{"id_case":"2178","class":"","sentence1":"We train using mini-batch stochastic gradient descent, the Adadelta update rule (Zeiler, 2012).","sentence2":"We apply the stochastic gradient descent algorithm with AdaDelta update rule (Zeiler, 2012)."},
{"id_case":"2179","class":"","sentence1":"According to the distributional hypothesis of meaning (Harris, 1954), words that occur in similar contexts tend to be semantically similar.","sentence2":"Distributional similarity relies on the distributional hypothesis that similar terms appear in similar contexts (Harris, 1954)."},
{"id_case":"2180","class":"","sentence1":"We evaluate our approach on the English portion of the CoNLL-2012 dataset (Pradhan et al., 2012).","sentence2":"We evaluate our method on the following data sets provided by the CoNLL2012 shared task (Pradhan et al., 2012)."},
{"id_case":"2181","class":"","sentence1":"We used a 2009 snapshot of Wikipedia, which was PoS tagged and lemmatized using the TreeTagger (Schmid, 1994).","sentence2":"Moreover, each of the 4 languages has been automatically PoS tagged using the TreeTagger (Schmid, 1994)."},
{"id_case":"2182","class":"","sentence1":"We utilise the freely available Weka machine learning toolkit (Hall et al., 2009), and the algorithm used for classification in each step is Naive Bayes.","sentence2":"We used WEKA (Hall et al., 2009) for training the machine learning models and for feature selection."},
{"id_case":"2183","class":"","sentence1":"The statistical significance test is also carried out with paired bootstrap resampling method (Koehn, 2004).","sentence2":"Statistical significance of the difference between systems is computed with paired bootstrap resampling (Koehn, 2004)."},
{"id_case":"2184","class":"","sentence1":"We made few changes to the guidelines (Bharati et al., 2006) to address and accommodate the v2 phenomenon in Kashmiri.","sentence2":"We are using the ILMT guidelines for chunking (Bharati et al., 2006) with significant changes addressing the verb second v2 phenomenon in Kashmiri."},
{"id_case":"2185","class":"","sentence1":"We are using the ILMT guidelines for chunking (Bharati et al., 2006), however, with significant changes addressing the verbsecond (V2) phenomenon in Kashmiri.","sentence2":"We made few changes to the guidelines (Bharati et al., 2006) to address and accommodate the v2 phenomenon in Kashmiri."},
{"id_case":"2186","class":"","sentence1":"We made few changes to the guidelines (Bharati et al., 2006) to address and accommodate the v2 phenomenon in Kashmiri.","sentence2":"We are using the ILMT guidelines for chunking (Bharati et al., 2006), however, with significant changes addressing the verbsecond (V2) phenomenon in Kashmiri."},
{"id_case":"2187","class":"","sentence1":"We made few changes to the guidelines (Bharati et al., 2006) to address and accommodate the v2 phenomenon in Kashmiri.","sentence2":"We are using the ILMT guidelines for chunking (Bharati et al., 2006), however, with significant changes addressing the verbsecond (V2) phenomenon in Kashmiri."},
{"id_case":"2188","class":"","sentence1":"We use hyponym together with word co-occurrence information, in this case bigrams from the Web 1T corpus (Brants and Franz, 2006).","sentence2":"As a practical approximation, we use bigram counts from the Web 1T corpus (Brants and Franz, 2006)."},
{"id_case":"2189","class":"","sentence1":"This is a corpus-based metric relying on the distributional hypothesis of meaning suggesting that similarity of context implies similarity of meaning (Z. Harris, 1954).","sentence2":"Distributional similarity relies on the distributional hypothesis that similar terms appear in similar contexts (Harris, 1954)."},
{"id_case":"2190","class":"","sentence1":"The standard PB- SMT system uses a hierarchical lexicalized reordering model (Galley and Manning, 2008) in addition to the distance-based distortion limit.","sentence2":"The experiments were conducted on top of the standard hierarchical lexicalized reordering model (Galley and Manning, 2008)."},
{"id_case":"2193","class":"","sentence1":"Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) represents the coherence structure of a text by a labeled tree, called discourse tree (DT) as shown in Figure 1 .","sentence2":"The former rely on the tree representation of a document based on the Rhetorical Structure Theory (RST) (Mann and Thompson, 1988)."},
{"id_case":"2194","class":"","sentence1":"The standard output is a meaning representation in the form of a Discourse Representation Structure (Kamp and Reyle, 1993).","sentence2":"Tense The standard reference textbook for Discourse Representation Theory has an extensive analysis of various tenses found in the English language (Kamp and Reyle, 1993)."},
{"id_case":"2195","class":"","sentence1":"It considers a shallow syntactic similarity (n = 1, 2, 3, 4 was used in all experiments); Partial tree kernel (Moschitti, 2006) between the parse tree of the sentences.","sentence2":"We further use a partial tree kernel (Moschitti, 2006) to calculate the similarity between the question and the comment based on their corresponding shallow syntactic trees."},
{"id_case":"2197","class":"","sentence1":"The standard output is a meaning representation in the form of a Discourse Representation Structure (Kamp and Reyle, 1993).","sentence2":"Tense The standard reference textbook for Discourse Representation Theory has an extensive analysis of various tenses found in the English language (Kamp and Reyle, 1993)."},
{"id_case":"2198","class":"","sentence1":"The phrase tables were generated by means of symmetrised word alignments obtained with GIZA++ (Och and Ney, 2003).","sentence2":"Phrase pairs were extracted from symmetrized word alignments and distortions generated by GIZA++ (Och and Ney, 2003)."},
{"id_case":"2199","class":"","sentence1":"Lch similarity (Leacock et al., 1998) measure two words similarity by using the depth of concepts in the WordNet hierarchy tree.","sentence2":"The similarity measure of Leacock and Chodorow (LCH) is based on the shortest path length between two concepts in an is-a hierarchy (Leacock et al., 1998)."},
{"id_case":"2204","class":"","sentence1":"BLE is based on the distributional hypothesis (Harris, 1954), stating that words with similar meaning have similar distributions across languages .","sentence2":"A distributional similarity model is constructed based on the distributional hypothesis (Harris, 1954): words that occur in the same contexts tend to share similar meanings."},
{"id_case":"2205","class":"","sentence1":"For language modeling, we trained a separate 5-gram Kneser-Ney smoothed LM model on the target (i.e. English) side of the training bi-text using KenLM (Heafield, 2011).","sentence2":"We used KenLM (Heafield, 2011) to create 3-gram language models with Kneser-Ney smoothing on the target side of the bilingual training corpora."},
{"id_case":"2207","class":"","sentence1":"Machine translation system settings: We used a phrase-based statistical machine translation model as implemented in the Moses toolkit (Koehn et al., 2007) for machine translation.","sentence2":"We then use the phrase extraction utility in the Moses statistical machine translation system (Koehn et al., 2007) to extract a phrase table which operates over characters ."},
{"id_case":"2208","class":"","sentence1":"For single label multi-class classification, micro-average precision(mip) and macro-average precision(map) are the two most popular performance measures (Yang, 1999), and we use them as benchmark.","sentence2":"Due to this problem, we use macro precision and macro recall (Yang, 1999) as performance measures and are given by the below equations."},
{"id_case":"2209","class":"","sentence1":"It therefore follows the distributional hypothesis (Harris, 1954) which states that words that occur in the same contexts tend to have similar meanings.","sentence2":"Distributional semantics (see Cohen and Widdows (2009) for an overview) is based on the observation that words that occur in similar contexts tend to be semantically related (Harris, 1954)."},
{"id_case":"2210","class":"","sentence1":"We used a 5-gram language model built on the \" Associated Press Worldstream English Service \" from English Gigaword corpus and NU- CLE 3.2 (Dahlmeier et al., 2013).","sentence2":"We ran the grammar correction system on the NU- CLE (NUS Corpus of Learner English) corpus (Dahlmeier et al., 2013)."},
{"id_case":"2211","class":"","sentence1":"Both of our systems were based on the Moses decoder (Koehn et al., 2007).","sentence2":"The Edinburgh\/JHU phrase-based translation systems for our participation in the WMT 2015 shared translation task 1 are based on the open source Moses toolkit (Koehn et al., 2007)."},
{"id_case":"2212","class":"","sentence1":"Markov logic networks combine Markov networks with first-order logic in a probabilistic framework (Richardson and Domingos, 2006).","sentence2":"Markov Logic Networks (MLN) (Richardson and Domingos, 2006) are a framework for probabilistic logic that employ weighted formulas in firstorder logic to compactly encode complex undirected probability."},
{"id_case":"2215","class":"","sentence1":"We used the phrase-based model Moses (Koehn et al., 2007) for the experiments with all the standard settings, including a lexicalized reordering model, and a 5-gram language model.","sentence2":"We trained a standard Moses baseline (Koehn et al., 2007) on the same training data and used the same 4- gram language model to generate responses."},
{"id_case":"2217","class":"","sentence1":"Europarl 2 (Koehn, 2005): it is a corpus of parallel texts in 11 languages from the proceedings of the European Parliament .","sentence2":"Translations for English words in the lexical sample are extracted from a semi-automatic word alignment of sentences from the Europarl parallel corpus (Koehn, 2005)."},
{"id_case":"2221","class":"","sentence1":"In addition, we show that the latent representation coheres well with the characterization of discourse connectives in the Penn Discourse Treebank (Prasad et al., 2008).","sentence2":"In the annotation of connectives and their arguments, we followed the general practice of the Penn Discourse Treebank (Prasad et al., 2008), but with a few modifications."},
{"id_case":"2222","class":"","sentence1":"We use some ML methods included in the Weka toolkit (Hall et al., 2009) in order to combine results obtained in experiments using single knowledge sources (described in section 3.3).","sentence2":"We have tested a wide range of regression algorithms in order to predict the scores, using the Weka 12 toolkit (Hall et al., 2009)."},
{"id_case":"2226","class":"","sentence1":"Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph.","sentence2":"Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013)."},
{"id_case":"2227","class":"","sentence1":"Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph.","sentence2":"Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013)."},
{"id_case":"2228","class":"","sentence1":"We use Stanford parser (de Marneffe et al., 2006) to obtain parse trees and dependency relations.","sentence2":"To obtain the dependency trees and word lemmas and POS tags, we use the Stanford NLP tools 2 (De Marneffe et al., 2006)."},
{"id_case":"2229","class":"","sentence1":"The Spanish-English (S2E) training corpus was drawn from the Europarl collection (Koehn, 2005).","sentence2":"The statistics on the distributions of words, alignments and lexical probabilities were extracted from the Europarl corpus (Koehn, 2005)."},
{"id_case":"2232","class":"","sentence1":"As mentioned before, we use the Stanford parser (De Marneffe et al., 2006) to obtain a typed dependency representation of the input sentence.","sentence2":"We use Stanford parser (de Marneffe et al., 2006) to obtain parse trees and dependency relations."},
{"id_case":"2233","class":"","sentence1":"All the models were estimated as 6-gram models with Kneser- Ney smoothing using the IRSTLM language modelling toolkit (Federico et al., 2008).","sentence2":"Language models (LM) were created with an open source IRSTLM toolkit (Federico et al., 2008)."},
{"id_case":"2234","class":"","sentence1":"To test our method, we conducted two lowresource translation experiments using the phrase-based MT system Moses (Koehn et al., 2007).","sentence2":"We use the Moses phrase-based translation system (Koehn et al., 2007) to implement our models."},
{"id_case":"2236","class":"","sentence1":"The SMT systems are trained with the Moses toolkit (Koehn et al., 2007), according to the WMT 2011 guidelines 6 .","sentence2":"The baseline will be created by the Moses SMT toolkit (Koehn et al., 2007)."},
{"id_case":"2237","class":"","sentence1":"The word alignment was trained using GIZA++ (Och and Ney, 2003) with the configuration grow-diag-final-and.","sentence2":"GIZA++ (Och and Ney, 2003) implementation of IBM word alignment model 4 with the grow-diagonalfinal-and heuristic was used for performing word alignment ."},
{"id_case":"2238","class":"","sentence1":"All experiments are conducted using the Moses phrase-based SMT system (Koehn et al., 2007) with a maximum phrase length of 8.","sentence2":"The questions are translated using a phrase-based system built using Moses (Koehn et al., 2007) (the Mo set)."},
{"id_case":"2239","class":"","sentence1":"The statistics on the distributions of words, alignments and lexical probabilities were extracted from the Europarl corpus (Koehn, 2005).","sentence2":"The Europarl corpus (Koehn, 2005) is built from the proceedings of the European Parliament."},
{"id_case":"2240","class":"","sentence1":"For a detailed survey of the field of sentiment analysis see (Pang and Lee, 2008).","sentence2":"Early studies on sentiment analysis considers only textual content for classifying the sentiment of a given review (Pang and Lee, 2008)."},
{"id_case":"2242","class":"","sentence1":"As mentioned before, we use the Stanford parser (De Marneffe et al., 2006) to obtain a typed dependency representation of the input sentence.","sentence2":"We use Stanford parser (de Marneffe et al., 2006) to obtain parse trees and dependency relations."},
{"id_case":"2243","class":"","sentence1":"Additionally, we will compare two decision rules, the common maximum a-posteriori (MAP) decision rule and the minimum Bayes risk (MBR) decision rule (Kumar and Byrne, 2004).","sentence2":"In the following, we will call this decision rule the MBR rule (Kumar and Byrne, 2004)."},
{"id_case":"2244","class":"","sentence1":"We use Stanford parser (de Marneffe et al., 2006) to obtain parse trees and dependency relations.","sentence2":"To obtain the dependency trees and word lemmas and POS tags, we use the Stanford NLP tools 2 (De Marneffe et al., 2006)."},
{"id_case":"2245","class":"","sentence1":"As our learner, we use LIBSVM with a linear kernel (Chang and Lin, 2011).","sentence2":"For the classification algorithm, we employed a Support Vector Machine (SVM) with a linear kernel, using the LIBSVM package (Chang and Lin, 2011)."},
{"id_case":"2246","class":"","sentence1":"We tokenize English data and segment Chinese data using the Stanford CoreNLP toolkit (Manning et al., 2014).","sentence2":"The webpages were parsed using the Stanford CoreNLP software (Manning et al., 2014)."},
{"id_case":"2247","class":"","sentence1":"Translation Model Moses is a state-of-the-art toolkit for phrase-based SMT systems (Koehn et al., 2007).","sentence2":"For the phrase-based SMT system, we adopted the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"2248","class":"","sentence1":"The SMT systems were built using the Moses toolkit (Koehn et al., 2007).","sentence2":"We build a state of the art phrase-based SMT system using Moses (Koehn et al., 2007)."},
{"id_case":"2249","class":"","sentence1":"We used the scikit-learn (Pedregosa et al., 2011) implementation of SVRs and the SKLL toolkit.","sentence2":"We used SVM implementations from scikit-learn (Pedregosa et al., 2011) and experimented with a number of classifiers."},
{"id_case":"2250","class":"","sentence1":"In addition, the corpus was lemmatised using the TreeTagger lemmatiser (Schmid, 1994).","sentence2":"All English data are POS tagged and lemmatised using the TreeTagger (Schmid, 1994)."},
{"id_case":"2251","class":"","sentence1":"We conducted baseline experiments for phrasebased machine translation using the Moses toolkit (Koehn et al., 2007).","sentence2":"All the experiments are carried out in Moses toolkit (Koehn et al., 2007)."},
{"id_case":"2252","class":"","sentence1":"The English side was tokenized using the Moses toolkit (Koehn et al., 2007).","sentence2":"The baseline systems are built with the opensource phrase-based SMT toolkit Moses (Koehn et al., 2007)."},
{"id_case":"2253","class":"","sentence1":"The training data processed by the Stanford parser (de Marneffe et al., 2006) is provided.","sentence2":"The Stanford dependency parser (De Marneffe et al., 2006) is used for extracting features from the dependency parse trees."},
{"id_case":"2254","class":"","sentence1":"For the phrase-based SMT system, we adopted the Moses toolkit (Koehn et al., 2007).","sentence2":"We built phrase-based machine translation systems using the open software toolkit Moses (Koehn et al., 2007)."},
{"id_case":"2255","class":"","sentence1":"Besides long-distance reordering (Xie et al., 2011), another attraction of the Dep2Str model is its simplicity.","sentence2":"In the Dep2Str model (Xie et al., 2011), the HD fragment is the basic unit."},
{"id_case":"2256","class":"","sentence1":"The Polish data is taken from the EUROPARL corpus (Koehn, 2005).","sentence2":"The EuroParl data set consists of 707 sentences of the German part of the EuroParl corpus (Koehn, 2005)."},
{"id_case":"2257","class":"","sentence1":"The baseline systems are built with the opensource phrase-based SMT toolkit Moses (Koehn et al., 2007).","sentence2":"For our SMT experiments, we use the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"2258","class":"","sentence1":"The word-alignment is done using GIZA++ (Och and Ney, 2003) toolkit and then growing heuristics are applied.","sentence2":"This is done using IBM Model 1 (Brown et al., 1993) and GIZA++ (Och and Ney, 2003)."},
{"id_case":"2259","class":"","sentence1":"We also removed all stop-words using the stop list defined in NLTK (Bird et al., 2009).","sentence2":"We tokenise the text using the default tokeniser from NLTK (Bird et al., 2009)."},
{"id_case":"2260","class":"","sentence1":"We also removed all stop-words using the stop list defined in NLTK (Bird et al., 2009).","sentence2":"3 We lemmatize the obtained words using the NLTK toolkit (Bird et al., 2009)."},
{"id_case":"2261","class":"","sentence1":"We conducted baseline experiments for phrasebased machine translation using the Moses toolkit (Koehn et al., 2007).","sentence2":"The English side was tokenized using the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"2262","class":"","sentence1":"The SMT systems were built using the Moses toolkit (Koehn et al., 2007).","sentence2":"Both systems are based on the Moses SMT toolkit (Koehn et al., 2007) and constructed as follows."},
{"id_case":"2263","class":"","sentence1":"The English side was tokenized using the Moses toolkit (Koehn et al., 2007).","sentence2":"We built phrase-based machine translation systems using the open software toolkit Moses (Koehn et al., 2007)."},
{"id_case":"2264","class":"","sentence1":"POS tagging is performed using the IMS Tree Tagger (Schmid, 1994).","sentence2":"The rules were extracted using the POS tags generated by the Tree- Tagger (Schmid, 1994)."},
{"id_case":"2265","class":"","sentence1":"We conducted baseline experiments for phrasebased machine translation using the Moses toolkit (Koehn et al., 2007).","sentence2":"The SMT systems were built using the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"2266","class":"","sentence1":"We used the scikit-learn toolkit to train our classifiers (Pedregosa et al., 2011).","sentence2":"Specifically, we used the standard Gradient Boosting Regressor in the scikit-learn toolkit 4 (Pedregosa et al., 2011)."},
{"id_case":"2268","class":"","sentence1":"The 5-gram target language model was trained using KenLM (Heafield, 2011).","sentence2":"We built a trigram language model with Kneser-Ney smoothing using KenLM toolkit (Heafield, 2011)."},
{"id_case":"2269","class":"","sentence1":"All the experiments are carried out in Moses toolkit (Koehn et al., 2007).","sentence2":"The baseline systems are built with the opensource phrase-based SMT toolkit Moses (Koehn et al., 2007)."},
{"id_case":"2270","class":"","sentence1":"For our experiments, we used translated movie subtitles from the OPUS corpus (Tiedemann, 2009b).","sentence2":"For medical we use the biomedical data from EMEA (Tiedemann, 2009)."},
{"id_case":"2271","class":"","sentence1":"For all experiments, we used the Moses SMT system (Koehn et al., 2007).","sentence2":"We build a state of the art phrase-based SMT system using Moses (Koehn et al., 2007)."},
{"id_case":"2272","class":"","sentence1":"The LKB chart parser uses a Head-driven Phrase Structure Grammar (HPSG), based on the grammar in (Sag et al., 2003), to parse the input text string and build a feature structure representation.","sentence2":"The goal of our research is to develop a data-driven Chinese parser that is based on Head-driven Phrase Structure Grammar (HPSG) (Sag et al., 2003)."},
{"id_case":"2273","class":"","sentence1":"For the phrase-based SMT system, we adopted the Moses toolkit (Koehn et al., 2007).","sentence2":"We build a state of the art phrase-based SMT system using Moses (Koehn et al., 2007)."},
{"id_case":"2275","class":"","sentence1":"The Polish data is taken from the EUROPARL corpus (Koehn, 2005).","sentence2":"The source of bilingual data used in the experiments is the Europarl collection (Koehn, 2005)."},
{"id_case":"2276","class":"","sentence1":"We conducted baseline experiments for phrasebased machine translation using the Moses toolkit (Koehn et al., 2007).","sentence2":"Our machine translation systems are trained using Moses 3 (Koehn et al., 2007)."},
{"id_case":"2277","class":"","sentence1":"Libsvm (Chang and Lin, 2001), which supports multi-class classification, was used as the classifier for WSD.","sentence2":"The classifier used for the system was an SVM (libsvm) (Chang and Lin, 2001)."},
{"id_case":"2278","class":"","sentence1":"Translation Model Moses is a state-of-the-art toolkit for phrase-based SMT systems (Koehn et al., 2007).","sentence2":"The SMT systems were built using the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"2279","class":"","sentence1":"For our experiments, we used translated movie subtitles from the OPUS corpus (Tiedemann, 2009).","sentence2":"In addition, we also used the EMEA corpus 8 (Tiedemann, 2009)."},
{"id_case":"2280","class":"","sentence1":"We used SVM implementations from scikit-learn (Pedregosa et al., 2011) and experimented with a number of classifiers.","sentence2":"We used a GB implementation of the scikit-learn package (Pedregosa et al., 2011)."},
{"id_case":"2281","class":"","sentence1":"For the linear logistic regression implementation we used scikit-learn (Pedregosa et al., 2011).","sentence2":"Specifically, we used the standard Gradient Boosting Regressor in the scikit-learn toolkit (Pedregosa et al., 2011)."},
{"id_case":"2282","class":"","sentence1":"We conducted baseline experiments for phrasebased machine translation using the Moses toolkit (Koehn et al., 2007).","sentence2":"First, we used the Moses toolkit (Koehn et al., 2007) for statistical machine translation."},
{"id_case":"2283","class":"","sentence1":"We conducted baseline experiments for phrasebased machine translation using the Moses toolkit (Koehn et al., 2007).","sentence2":"We develop translation models using the phrase-based Moses (Koehn et al., 2007) SMT system."},
{"id_case":"2284","class":"","sentence1":"The corpora are tokenised and truecased using scripts from the Moses toolkit (Koehn et al., 2007).","sentence2":"All the experiments are carried out in Moses toolkit (Koehn et al., 2007)."},
{"id_case":"2285","class":"","sentence1":"The training data processed by the Stanford parser (de Marneffe et al., 2006) is provided.","sentence2":"The Stanford dependency parser (De Marneffe et al., 2006) is used for extracting features from the dependency parse trees."},
{"id_case":"2286","class":"","sentence1":"We tokenize English data and segment Chinese data using the Stanford CoreNLP toolkit (Manning et al., 2014).","sentence2":"The webpages were parsed using the Stanford CoreNLP software (Manning et al., 2014)."},
{"id_case":"2287","class":"","sentence1":"They are selected based on their distance to the CLWSD trial and test sentences (Moore and Lewis, 2010).","sentence2":"The sentence pairs are selected based on the source scores, target scores or the sum of source and target scores (Moore and Lewis, 2010)."},
{"id_case":"2289","class":"","sentence1":"We train a log linear classifier on the features described in using scikit-learn (Pedregosa et al., 2011).","sentence2":"Our system is a linear model estimated using ridge regression, as implemented in the scikit-learn toolkit (Pedregosa et al., 2011)."},
{"id_case":"2290","class":"","sentence1":"GIZA++ (Och and Ney, 2003) implementation of IBM word alignment model with the grow-diagonalfinal-and heuristic was used for performing word alignment .","sentence2":"In all experiments, word alignment was obtained using the grow-diag-final heuristic for symmetrizing GIZA++ (Och and Ney, 2003) alignments."},
{"id_case":"2291","class":"","sentence1":"The Web 1T 5-gram corpus (Brants and Franz, 2006) is used for language modeling and collecting web Ngram counts.","sentence2":"The Google Web 1T corpus (Brants and Franz, 2006) was used to train the classifiers for the ArtOrDet and Prep error categories."},
{"id_case":"2292","class":"","sentence1":"We built a source-to-target PB-SMT model from the bilingual domain corpus using the Moses toolkit (Koehn et al., 2007).","sentence2":"We estimated a hierarchical MT model for the train partition with the standard configuration of the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"2293","class":"","sentence1":"The statistics on the distributions of words, alignments and lexical probabilities were extracted from the Europarl corpus (Koehn, 2005).","sentence2":"The parallel data come from the Europarl corpus version 7 (Koehn, 2005) and Kaist Corpus 4 ."},
{"id_case":"2294","class":"","sentence1":"Results were evaluated in terms of alignment precision (P), recall (R), F-measure and alignment error rate (AER) (Och and Ney, 2000).","sentence2":"According to the definition of the alignment error rate (AER) in (Och and Ney, 2000), AER can be calculated with Equation (4)."},
{"id_case":"2295","class":"","sentence1":"The statistics on the distributions of words, alignments and lexical probabilities were extracted from the Europarl corpus (Koehn, 2005).","sentence2":"The statistical dictionary for this task was extracted from the English-French Europarl 7 corpus (Koehn, 2005)."},
{"id_case":"2296","class":"","sentence1":"As mentioned before, we use the Stanford parser (De Marneffe et al., 2006) to obtain a typed dependency representation of the input sentence.","sentence2":"To obtain these we use the Stanford dependency parser (de Marneffe et al., 2006) and the forced alignment from Section 3.9."},
{"id_case":"2297","class":"","sentence1":"We then run word alignment with GIZA++ (Och and Ney, 2003) in both directions, with the default parameters used in Moses.","sentence2":"Statistical word alignments are learned in both directions with GIZA++ (Och and Ney, 2003), then combined with the \" grow-diag-final \" heuristic."},
{"id_case":"2298","class":"","sentence1":"We assume some familiarity with Lexicalized Tree- Adjoining Grammar (LTAG); (Joshi and Schabes, 1997) is a good introduction to this formalism.","sentence2":"In this section, we give a brief introduction to the Lexicalized Tree Adjoining Grammar (more details can be found in (Joshi and Schabes, 1997))."},
{"id_case":"2299","class":"","sentence1":"We built a source-to-target PB-SMT model from the bilingual domain corpus using the Moses toolkit (Koehn et al., 2007).","sentence2":"We refer to that model as Moses en-es-100k , because it was trained using the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"2300","class":"","sentence1":"As mentioned before, we use the Stanford parser (De Marneffe et al., 2006) to obtain a typed dependency representation of the input sentence.","sentence2":"To obtain these we use the Stanford dependency parser (de Marneffe et al., 2006) and the forced alignment from Section 3.9."},
{"id_case":"2301","class":"","sentence1":"We experiment with the phrase-based statistical machine translation toolkit Moses (Koehn et al., 2007) in order to train a Japanese -English system and to show the influence of the expanded parallel","sentence2":"We use the Moses toolkit (Koehn et al., 2007) to create a statistical phrase-based machine translation model built on the best pre-processed data, as described above."},
{"id_case":"2302","class":"","sentence1":"The Google N-gram corpus is a collection of English N-grams, ranging from one to five N-grams, and their respective frequency counts observed on the Web (Brants and Franz, 2006).","sentence2":"Feature sources: The n-gram semantic features are extracted from the Google n-grams corpus (Brants and Franz, 2006), a large collection of English n-grams (for n = 1 to 5) and their frequencies."},
{"id_case":"2303","class":"","sentence1":"We propose to use Markov Logic Networks (ML- N) (Richardson and Domingos, 2006) to learn the joint model for subjective classification and explanatory relation extraction.","sentence2":"In this paper, we study how to use a different learning framework, Markov logic (Richardson and Domingos, 2006), to learn a joint model for both pairwise classification and mention clustering under."},
{"id_case":"2305","class":"","sentence1":"The learning algorithm for level-0 dependency is similar to the guided learning algorithm for labelling as described in (Shen et al., 2007).","sentence2":"Finally, we increased the beam size of guided learning from 1 to 3 as in (Shen et al., 2007)."},
{"id_case":"2306","class":"","sentence1":"For language modeling, we use the English Gigaword corpus with 5-gram LM implemented with the KenLM toolkit (Heafield, 2011).","sentence2":"We use a 5-gram LM trained on the Gigaword corpus and use KenLM (Heafield, 2011) for LM scoring during decoding."},
{"id_case":"2307","class":"","sentence1":"For language modeling, we use the English Gigaword corpus with 5-gram LM implemented with the KenLM toolkit (Heafield, 2011).","sentence2":"For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5."},
{"id_case":"2308","class":"","sentence1":"The usages from the ukWaC are tokenised and lemmatised using TreeTagger (Schmid, 1994), as provided by the corpus.","sentence2":"The corpus is lemmatised and tagged by part-of-speech on both sides using the TreeTagger (Schmid, 1994)."},
{"id_case":"2310","class":"","sentence1":"For this purpose we used the logistic regression classifier from scikit-learn with L2 regularisation (Pedregosa et al., 2011) .","sentence2":"We used logistic regression (though linear SVM showed almost the same results) for the final reranking, the implementation was taken from scikit-learn package (Pedregosa et al., 2011)."},
{"id_case":"2311","class":"","sentence1":"The system was trained on the English and Danish part of the Europarl corpus version 3 (Koehn, 2005).","sentence2":"The major part of data comes from current and upcoming full releases of the Europarl data set (Koehn, 2005)."},
{"id_case":"2316","class":"","sentence1":"In contrast, the recent SimLex999 dataset (Hill et al., 2014) contains word similarity scores for nouns (666 pairs), verbs (222 pairs) and adjectives (111 pairs).","sentence2":"While many datasets for these tasks are limited to pairs of nouns, the recent SimLex999 word similarity dataset (Hill et al., 2014) also consists of similarity scores for verb and adjective pairs."},
{"id_case":"2317","class":"","sentence1":"The training data for the task is from the NUCLE corpus (Dahlmeier et al., 2013), an error-tagged collection of essays written by non-native learners of English.","sentence2":"The first corpus is NUCLE (Dahlmeier et al., 2013), containing essays written by students at the National University of Singapore (NUS) which have been manually corrected by English instructors."},
{"id_case":"2318","class":"","sentence1":"We use the Weka (Hall et al., 2009) AdaBoost .","sentence2":"We use the Weka (Hall et al., 2009) toolkit for running standard training and prediction algorithms with the SVM."},
{"id_case":"2320","class":"","sentence1":"We use both the original depen-dency paths and their collapsed Stanford Dependencies forms (de Marneffe and Manning, 2008).","sentence2":"We use the Stanford-dependencies representation (de Marneffe and Manning, 2008)."},
{"id_case":"2322","class":"","sentence1":"The baselines use the LibSVM package (Chang and Lin, 2011) for SVM training and prediction.","sentence2":"We use LibSVM (Chang and Lin, 2011) as the SVM tool."},
{"id_case":"2323","class":"","sentence1":"Later work established aspect-oriented opinion mining (Hu and Liu, 2004), where the aim is to find features or aspects of products that are discussed in a review.","sentence2":"Our work is in the area of topic\/feature-based sentiment analysis or opinion mining (Hu and Liu, 2004)."},
{"id_case":"2326","class":"","sentence1":"For our language model, we use a tri-gram smoothed language model trained using the newswire text provided in the English Gigaword corpus (Graff and Cieri, 2003).","sentence2":"The trigram target language model is trained from the Xinhua portion of English Gigaword corpus (Graff and Cieri, 2003)."},
{"id_case":"2327","class":"","sentence1":"We used the phrase-based model Moses (Koehn et al., 2007) for the experiments with all the standard settings, including a lexicalized reordering model, and a 5-gram language model.","sentence2":"The training set is used to train the phrase-based translation model and language model for Moses (Koehn et al., 2007)."},
{"id_case":"2328","class":"","sentence1":"@BULLET SVM-Rel: We also use LibSVM (Chang and Lin, 2011) to classify the relations between clauses.","sentence2":"We use LibSVM (Chang and Lin, 2011) as the SVM tool."},
{"id_case":"2330","class":"","sentence1":"We implemented SVM using scikit (Pedregosa et al., 2011) library.","sentence2":"Linear SVM classifier with different cost parameter C was implemented using scikit learn (Pedregosa et al., 2011)."},
{"id_case":"2332","class":"","sentence1":"POS tagging was performed with TreeTagger (Schmid, 1994).","sentence2":"The corpus was then automatically tagged with part-of-speech information, using TreeTagger (Schmid, 1994)."},
{"id_case":"2333","class":"","sentence1":"We used the CRFsuite (Okazaki, 2007) implementation of CRFs.","sentence2":"We use a CRF implementation called CRFsuite (Okazaki, 2007) with the passiveaggressive learning algorithm."},
{"id_case":"2334","class":"","sentence1":"Word alignment is performed using GIZA++ (Och and Ney, 2003).","sentence2":"We performed word alignment using GIZA++ (Och and Ney, 2003) with the default grow-diagfinal-and alignment symmetrization method."},
{"id_case":"2335","class":"","sentence1":"In particular, we apply the popular grow-diag-final-and heuristics as implemented in the Moses toolbox (Koehn et al., 2007).","sentence2":"Next we apply the empirical compound splitter as described by Koehn and Knight (2003) and as implemented in the Perl script which is part of the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"2336","class":"","sentence1":"We use LibSVM (Chang and Lin, 2011) as the SVM tool.","sentence2":"We trained an SVM using the LibSVM package (Chang and Lin, 2011) and a linear kernel."},
{"id_case":"2337","class":"","sentence1":"The phrases were extracted and scored using the Moses training tools (Koehn et al., 2007).","sentence2":"conducted using the Moses phrase-based decoder (Koehn et al., 2007)."},
{"id_case":"2338","class":"","sentence1":"We use the Weka (Hall et al., 2009) AdaBoost .","sentence2":"We conducted experiments using Multinomial Naive Bayes classifier implemented in the Weka toolkit (Hall et al., 2009)."},
{"id_case":"2339","class":"","sentence1":"To automatically learn such patterns, we use the AutoSlog-TS weakly-supervised extraction pattern learner (Riloff, 1996).","sentence2":"To learn patterns from texts labeled as FACT or FEELING arguments, we use the AutoSlog-TS (Riloff, 1996) extraction pattern learner, which is freely available for research."},
{"id_case":"2340","class":"","sentence1":"We used bootstrap resampling (Koehn, 2004) to measure significance on the mixed test set and marked all statistically significant results compared to the respective baselines with asterisk.","sentence2":"In all cases, results are statistically significant (99%) following the \" pair bootstrap resampling \" (Koehn, 2004)."},
{"id_case":"2341","class":"","sentence1":"We used bootstrap resampling (Koehn, 2004) to measure significance on the mixed test set and marked all statistically significant results compared to the respective baselines with asterisk.","sentence2":"In all cases, results are statistically significant (99%) following the \" pair bootstrap resampling \" (Koehn, 2004)."},
{"id_case":"2342","class":"","sentence1":"The WORDNET system is a comprehensive manually developed lexical database from Princeton University (Miller et al., 1990).","sentence2":"The WordNet on-line lexical database (Miller et al., 1990)."},
{"id_case":"2343","class":"","sentence1":"We use both the original depen-dency paths and their collapsed Stanford Dependencies forms (de Marneffe and Manning, 2008).","sentence2":"We use the Stanford-dependencies representation (de Marneffe and Manning, 2008)."},
{"id_case":"2344","class":"","sentence1":"5-gram language models of Turkish and English were trained using KenLM (Heafield, 2011).","sentence2":"Kneser-Ney n-grams were trained with KenLM (Heafield, 2011)."},
{"id_case":"2345","class":"","sentence1":"The model was trained using Weka (Hall et al., 2009).","sentence2":"The classification models described above were implemented using the Weka package (Hall et al., 2009)."},
{"id_case":"2348","class":"","sentence1":"We applied the Naive Bayes probabilistic supervised learning algorithm from the Weka machine learning library (Hall et al., 2009).","sentence2":"We use the Weka (Hall et al., 2009) AdaBoost ."},
{"id_case":"2349","class":"","sentence1":"We use the well-known LibSVM implementation (Chang and Lin, 2011).","sentence2":"@BULLET SVM-Rel: We also use LibSVM (Chang and Lin, 2011) to classify the relations between clauses."},
{"id_case":"2350","class":"","sentence1":"We use Ridge Regression (RR) with l2-norm regularization and Support Vector Regression (SVR) with an RBF kernel from scikit-learn (Pedregosa et al., 2011).","sentence2":"@BULLET Support Vector Machine(SVM): We use an SVM with a linear kernel, C=1, a squared-hinge loss function, and L2 loss penalty (Pedregosa et al., 2011)."},
{"id_case":"2351","class":"","sentence1":"We used the English side of the Europarl corpus (Koehn, 2005).","sentence2":"For baseline we used the Spanish and English sides of the Europarl multilingual parallel corpus (Koehn, 2005), with the standard training, development, and test sets."},
{"id_case":"2352","class":"","sentence1":"This data is part of the NUCLE corpus (Dahlmeier et al., 2013).","sentence2":"The training data for the task is from the NUCLE corpus (Dahlmeier et al., 2013), an error-tagged collection of essays written by non-native learners of English."},
{"id_case":"2354","class":"","sentence1":"We used the English side of the Europarl corpus (Koehn, 2005).","sentence2":"Translations for English words in the lexical sample are extracted from a semi-automatic word alignment of sentences from the Europarl parallel corpus (Koehn, 2005)."},
{"id_case":"2355","class":"","sentence1":"Our text processing uses the Natural Language Toolkit (NLTK) (Bird et al., 2009).","sentence2":"Our plan is to use the Natural Language Toolkit (Bird et al., 2009) for stemming and stop words removal, and WordNet as external lexical base (Fellbaum, 1998)."},
{"id_case":"2359","class":"","sentence1":"Our program takes as input a text that is lemmatized, tagged and parsed using the Stanford CoreNLP tools (Manning et al., 2014).","sentence2":"The webpages were parsed using the Stanford CoreNLP software (Manning et al., 2014)."},
{"id_case":"2360","class":"","sentence1":"Then these two alignments are combined to form the final word alignment with the heuristics used in the Moses toolkit (Koehn et al., 2007).","sentence2":"All the experiments are carried out in Moses toolkit (Koehn et al., 2007)."},
{"id_case":"2361","class":"","sentence1":"The matrix was then normalized with pointwise mutual information (Church and Hanks, 1990).","sentence2":"The resulting collection of verbs is then ranked by computing their pointwise mutual information (Church and Hanks, 1990) with the subject role r."},
{"id_case":"2362","class":"","sentence1":"After tokenization , we lemmatize and stem tweets and remove stopwords from each tweet using the NLTK toolkit (Bird et al., 2009).","sentence2":"We lemmatize the obtained words using the NLTK toolkit (Bird et al., 2009)."},
{"id_case":"2363","class":"","sentence1":"The column \" pair-CI \" shows 95% confidence intervals relative to the primary system using the paired bootstrap re-sampling method (Koehn, 2004).","sentence2":"The reported confidence intervals were estimated using bootstrap resampling (Koehn, 2004)."},
{"id_case":"2365","class":"","sentence1":"In the implementation, we used freely available WordNet::Similarity package (Pedersen et al., 2004).","sentence2":"WordNet::Similarity (Pedersen et al., 2004) is a freely available software package for measuring the semantic similarity or relatedness between a pair of concepts (or word senses)."},
{"id_case":"2366","class":"","sentence1":"We tested the difference in performance for statistical significance using an approximate randomization procedure (Yeh, 2000) with 10000 iterations .","sentence2":"We tested the significance of differences using stratified shuffling (Yeh, 2000)."},
{"id_case":"2367","class":"","sentence1":"MRS is integrated with the HPSG English Resource Grammar (ERG) (Flickinger, 2000).","sentence2":"The experiments are carried out on a broad-coverage linguistically-precise HPSG grammar for English, the LinGO English Resource Grammar (ERG) (Copestake and Flickinger, 2000)."},
{"id_case":"2368","class":"","sentence1":"The resulting matrix is weighted using pointwise mutual information (Church and Hanks, 1990).","sentence2":"The resulting collection of verbs is then ranked by computing their pointwise mutual information (Church and Hanks, 1990) with the subject role r."},
{"id_case":"2369","class":"","sentence1":"Second, tokenization, POS tagging and lemmatization are performed with Treetagger (Schmid, 1994) using the standard French and English parameter files.","sentence2":"POS tagging is performed using the IMS Tree Tagger (Schmid, 1994)."},
{"id_case":"2371","class":"","sentence1":"Our program takes as input a text that is lemmatized, tagged and parsed using the Stanford CoreNLP tools (Manning et al., 2014).","sentence2":"The webpages were parsed using the Stanford CoreNLP software (Manning et al., 2014)."},
{"id_case":"2372","class":"","sentence1":"We use the standard alignment tool Giza++ (Och and Ney, 2003) to word align the parallel data.","sentence2":"We used the mkcls tool in GIZA (Och and Ney, 2003) to learn the word classes."},
{"id_case":"2373","class":"","sentence1":"The word alignment was trained using GIZA++ (Och and Ney, 2003) with the configuration grow-diag-final-and.","sentence2":"The word alignment is created by GIZA++ (Och and Ney, 2003); the intersection symmetrization is used."},
{"id_case":"2374","class":"","sentence1":"Corpus-based meaning representations rely on the distributional hypothesis, which assumes that words occurring in a similar set of contexts are also similar in meaning (Harris, 1954).","sentence2":"The former that is the most popular relies on the distributional hypothesis that puts forward the idea that words with similar meaning tend to occur in similar contexts (Harris, 1954)."},
{"id_case":"2375","class":"","sentence1":"We use Stanford parser (de Marneffe et al., 2006) to obtain parse trees and dependency relations.","sentence2":"They used Stanford parser(De Marneffe et al., 2006) to create the parse trees for all sentences."},
{"id_case":"2376","class":"","sentence1":"We contributed three systems for the 2012 Semantic Textual Similarity (STS) task (Agirre et al., 2012).","sentence2":"The task is part of the Semantic Evaluation 2012 Workshop (Agirre et al., 2012)."},
{"id_case":"2377","class":"","sentence1":"Parameters are updated using AdaGrad (Duchi et al., 2011) with a learning rate of 0.1.","sentence2":"We use the AdaGrad optimizer (Duchi et al., 2011)  with initial learning rate set to 0.1."},
{"id_case":"2378","class":"","sentence1":"Parameters are updated using AdaGrad (Duchi et al., 2011) with a learning rate of 0.1.","sentence2":"Parameters are learned using mini-batch (size=1000) stochastic gradient descent with adagrad (Duchi et al., 2011) learning schedule."},
{"id_case":"2380","class":"","sentence1":"The concrete syntax is a variant of the TimeML markup language (Pustejovsky et al., 2003).","sentence2":"TimeML is a specification for annotating human language in text (Pustejovsky et al., 2003)."},
{"id_case":"2382","class":"","sentence1":"The Grammar Matrix is couched within the Head-driven Phrase Structure Grammar (HPSG) framework (Pollard and Sag, 1994).","sentence2":"Head-driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) is a widely adopted constraint-based grammar formalism."},
{"id_case":"2383","class":"","sentence1":"Both of our systems were based on the Moses decoder (Koehn et al., 2007).","sentence2":"For this pair, all the experiments were performed using the Moses decoder (Koehn et al., 2007)."},
{"id_case":"2384","class":"","sentence1":"We have applied the well-known PageRank algorithm (Brin and Page, 1998) to score the vertices of the graph.","sentence2":"It uses the PageRank algorithm (Brin and Page, 1998) to recursively change the weight of the vertices."},
{"id_case":"2385","class":"","sentence1":"We have applied the well-known PageRank algorithm (Brin and Page, 1998) to score the vertices of the graph.","sentence2":"We apply a modified version of PageRank (Brin and Page, 1998) to the tagged mentions."},
{"id_case":"2386","class":"","sentence1":"This can be attributed to the intuitive advantage of dependency trees where the shortest path between entities captures most of the information about the relation (Bunescu and Mooney, 2005).","sentence2":"The dependency path is the shortest path between the two entities in a dependency parse graph and has been shown to be important for relation extraction (Bunescu and Mooney, 2005)."},
{"id_case":"2387","class":"","sentence1":"The word alignment was trained using GIZA++ (Och and Ney, 2003) with the configuration grow-diag-final-and.","sentence2":"The system is trained using Moses (), with Giza++ (Och and Ney, 2003) for word alignment."},
{"id_case":"2388","class":"","sentence1":"In this paper, we use MADAMIRA (Pasha et al., 2014), a system for morphological analysis and disambiguation for both MSA and DA (EGY), to identify DA words and replace MSA equivalents.","sentence2":"For our baseline, we use the morphological analysis and disambiguation tool MADAMIRA (Pasha et al., 2014), which produces a contextually ranked list of analyses for each word."},
{"id_case":"2389","class":"","sentence1":"We use Stanford parser (de Marneffe et al., 2006) to obtain parse trees and dependency relations.","sentence2":"They used Stanford parser(De Marneffe et al., 2006) to create the parse trees for all sentences."},
{"id_case":"2390","class":"","sentence1":"The segment type labeler is an SVM classifier implemented in LIBSVM (Chang and Lin, 2011).","sentence2":"We use an SVM classifier as implemented in Lib- SVM (Chang and Lin, 2011) for classification."},
{"id_case":"2391","class":"","sentence1":"We use the Giza++ tool (Och and Ney, 2003) to align words in our parallel corpora.","sentence2":"We used the mkcls tool in GIZA (Och and Ney, 2003) to learn the word classes."},
{"id_case":"2392","class":"","sentence1":"The Stanford parser (Marneffe et al., 2006) was used to produce all dependency parses.","sentence2":"They used Stanford parser(De Marneffe et al., 2006) to create the parse trees for all sentences."},
{"id_case":"2393","class":"","sentence1":"English sentences are parsed into dependency structures by Stanford parser (Marneffe et al., 2006).","sentence2":"We also obtain the dependency parse of the sentences using the Stanford parser (De Marneffe et al., 2006)."},
{"id_case":"2394","class":"","sentence1":"We use the group average agglomerative clustering package within NLTK (Bird et al., 2009).","sentence2":"We deployed these models using classifiers in the NLTK python package (Bird et al., 2009)."},
{"id_case":"2395","class":"","sentence1":"We use the standard alignment tool Giza++ (Och and Ney, 2003) to word align the parallel data.","sentence2":"We used the mkcls tool in GIZA (Och and Ney, 2003) to learn the word classes."},
{"id_case":"2397","class":"","sentence1":"The source-side features are the original pronoun , the preceding and the following nouns or pronouns, and their respective POS tags identified with the English tagger TreeTagger (Schmid, 1994).","sentence2":"All English data are POS tagged and lemmatised using the TreeTagger (Schmid, 1994)."},
{"id_case":"2400","class":"","sentence1":"The set of candidate words is called the confusion set (Golding and Roth, 1999).","sentence2":"Note that we are not dealing here with the standard models in context sensitive spelling (Golding and Roth, 1999) where the set of candidate correction is a known \" confusion set \"."},
{"id_case":"2901","class":"","sentence1":"Opinion Mining (OM), also known as Sentiment Analysis (SA) is the discipline that focuses on the computational treatment of opinion, sentiment and subjectivity in texts (Pang and Lee, 2008).","sentence2":"Opinion mining, also known as sentiment analysis (Pang and Lee, 2008), is a relatively recent area of research in natural language processing."},
{"id_case":"2902","class":"","sentence1":"Opinion Mining (OM), also known as Sentiment Analysis (SA), is the discipline that focuses on the computational treatment of opinion, sentiment and subjectivity in texts (Pang and Lee, 2008).","sentence2":"Opinion mining, also known as sentiment analysis (Pang and Lee, 2008), is a relatively recent area of research in natural language processing."},
{"id_case":"2903","class":"","sentence1":"To do so, a subset of the Simple English Wikipedia (SEW) corpus (Coster and Kauchak, 2011) was randomly chosen to build pairs of articles .","sentence2":"Table 1 shows the n-gram overlap proportions in a sentence aligned data set of 137K sentence pairs from aligning Simple English Wikipedia and English Wikipedia articles (Coster and Kauchak, 2011)."},
{"id_case":"2904","class":"","sentence1":"We used a phrase-based statistical machine translation model as implemented in the Moses toolkit (Koehn et al., 2007) for machine translation.","sentence2":"We used the Moses toolkit (Koehn et al., 2007) to build a phrase based machine translation system with a traditional 5-gram LM trained on the target side of our bitext."},
{"id_case":"2906","class":"","sentence1":"We aligned the word-pairs at character level using GIZA++ and the phrase extractor and scorer from the Moses machine translation package (Koehn et al., 2007).","sentence2":"We also ran standard phrase extraction on the same corpus using Steps 4 and 5 of the Moses statistical machine translation training script (Koehn et al., 2007)."},
{"id_case":"2906","class":"","sentence1":"We aligned the word-pairs at character level using GIZA++ and the phrase extractor and scorer from the Moses machine translation package (Koehn et al., 2007).","sentence2":"We also ran standard phrase extraction on the same corpus using Steps 4 and 5 of the Moses statistical machine translation training script (Koehn et al., 2007)."},
{"id_case":"2907","class":"","sentence1":"Traditionally, the resolution of noun phrases (NPs) has been the focus of coreference research (Ng, 2010).","sentence2":"Noun phrase coreference resolution is the task of determining which noun phrases (NPs) in a text or dialogue refer to the same real-world entities (Ng, 2010)."},
{"id_case":"2908","class":"","sentence1":"An automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been proposed in (Banerjee and Lavie, 2005).","sentence2":"A new automatic metric METEOR (Banerjee and Lavie, 2005) uses stems and synonyms of the words."},
{"id_case":"2909","class":"","sentence1":"Then we use the POS tagger from Stanford CoreNLP (Manning et al., 2014) in order to distinguish the candidate events to verbal events and nominal events.","sentence2":"To extract features, we use the constituency and dependency parser from Stanford CoreNLP (Manning et al., 2014) to identify the clause structure of sentences."},
{"id_case":"2910","class":"","sentence1":"The constituent context model (CCM) for inducing constituency parses (Klein and Manning, 2002) was the first unsupervised approach to surpass a right-branching baseline.","sentence2":"The idea of representing a constituent by its yield and (a different definition of) context is used by the CCM unsupervised parsing model (Klein and Manning, 2002)."},
{"id_case":"2911","class":"","sentence1":"We constructed the sentence corpus by first sampling sentences from a web crawl and parsing them with Malt Parser (Nivre et al., 2006).","sentence2":"This corpus is a two billion word corpus automatically harvested from the web and parsed by the Malt Parser (Nivre et al., 2006)."},
{"id_case":"2912","class":"","sentence1":"Then we use the POS tagger from Stanford CoreNLP (Manning et al., 2014) in order to distinguish the candidate events to verbal events and nominal events.","sentence2":"To extract features, we use the constituency and dependency parser from Stanford CoreNLP (Manning et al., 2014) to identify the clause structure of sentences."},
{"id_case":"2916","class":"","sentence1":"For the actions, either we shift the current word onto the stack or reduce the top two (or more) items at the top of the stack (Aho and Ullman, 1972).","sentence2":"Basically, shift-reduce parsing (Aho and Ullman, 1972) performs a left-to-right scan of the input sentence, and at each step, chooses either to shift the next word onto the stack."},
{"id_case":"2917","class":"","sentence1":"As mentioned before, we use the Stanford parser (De Marneffe et al., 2006) to obtain a typed dependency representation of the input sentence.","sentence2":"We also obtain the dependency parse of the sentences using the Stanford parser (De Marneffe et al., 2006)."},
{"id_case":"2918","class":"","sentence1":"This subsection describes the process of mining this information from the fourth edition of the English Gigaword corpus (Parker et al., 2009).","sentence2":"These were drawn from the Penn Treebank (sections 2.23) and the English Gigaword corpus (Parker et al., 2009)."},
{"id_case":"2919","class":"","sentence1":"We estimated a hierarchical MT model for the train partition with the standard configuration of the Moses toolkit (Koehn et al., 2007).","sentence2":"This was done with a specific tool provided with the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"2920","class":"","sentence1":"Document content was tokenized using the scripts provided in the Moses toolkit (Koehn et al., 2007).","sentence2":"We refer to that model as Moses en-es-100k , because it was trained using the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"2922","class":"","sentence1":"It was built with the Moses toolkit (Koehn et al., 2007), using the 14 standard core features including a 5gram language model.","sentence2":"This was done with a specific tool provided with the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"2923","class":"","sentence1":"Combinatory Categorial grammar (CCG) is a linguistic formalism that represents both the syntax and semantics of language (Steedman, 1996).","sentence2":"PAS is the sole level of representation in Combinatory Categorial Grammar (CCG) (Steedman, 1996)."},
{"id_case":"2924","class":"","sentence1":"The Grammar Matrix is couched within the Head-driven Phrase Structure Grammar (HPSG) framework (Pollard and Sag, 1994).","sentence2":"Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar (Pollard and Sag, 1994)."},
{"id_case":"2925","class":"","sentence1":"We evaluate the performance using mean average precision (MAP), which is a robust and stable metric (Manning et al., 2008).","sentence2":"We use the mean average precision MAP (Manning et al., 2008) to evaluate the quality of the system."},
{"id_case":"2927","class":"","sentence1":"We trained an SVM using the LibSVM package (Chang and Lin, 2011) and a linear kernel.","sentence2":"The STCs-based semantic parser is implemented with linear kernel SVMs trained using the LibSVM package (Chang and Lin, 2011)."},
{"id_case":"2929","class":"","sentence1":"This concatenated corpus, tokenized, POS-tagged and lemmatized with the TreeTagger (Schmid, 1995), contains about 2.83 billion tokens (excluding punctuation, digits, etc.).","sentence2":"The Web-derived ukWaC is already tokenized and POS-tagged with the TreeTagger (Schmid, 1995)."},
{"id_case":"2930","class":"","sentence1":"As mentioned before, we use the Stanford parser (De Marneffe et al., 2006) to obtain a typed dependency representation of the input sentence.","sentence2":"We also obtain the dependency parse of the sentences using the Stanford parser (De Marneffe et al., 2006)."},
{"id_case":"2931","class":"","sentence1":"We conducted two lowresource translation experiments using the phrase-based MT system Moses (Koehn et al., 2007).","sentence2":"We use the state-of-the-art phrase-based machine translation system Moses (Koehn et al., 2007) toperform our machine translation experiments."},
{"id_case":"2932","class":"","sentence1":"Model parameters for the ITG are estimated via expectation maximization (Dempster et al., 1977) .","sentence2":"The model parameters will then be estimated using the Expectation-Maximization (EM) algorithm (Dempster et al., 1977)."},
{"id_case":"2934","class":"","sentence1":"Our entry to the RTE-3 challenge is a system that takes advantage of Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997).","sentence2":"Then the Latent Semantic Analysis (Landauer and Dumais, 1997) technique is applied to reduce the space dimensionality."},
{"id_case":"2935","class":"","sentence1":"More recently, implicit relation prediction has been evaluated on annotated implicit relations from the Penn Discourse Treebank (Prasad et al., 2008).","sentence2":"The first was obtained from the Penn Discourse Treebank (PDTB) annotation manual (Prasad et al., 2008)."},
{"id_case":"2936","class":"","sentence1":"We estimated a hierarchical MT model for the train partition with the standard configuration of the Moses toolkit (Koehn et al., 2007).","sentence2":"We preprocessed the training corpora with scripts included in the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"2937","class":"","sentence1":"This was done with a specific tool provided with the Moses toolkit (Koehn et al., 2007).","sentence2":"It was built with the Moses toolkit (Koehn et al., 2007) using the 14 standard core features including a 5-gram language model."},
{"id_case":"2939","class":"","sentence1":"The goal of the Chunking task (Sang and Buchholz, 2000) is the identification of an assortment of linguistic base-phrases.","sentence2":"The goal of the task is to divide a text into syntactically related non-overlapping groups of words (Tjong Kim Sang and Buchholz, 2000)."},
{"id_case":"2941","class":"","sentence1":"In this paper, we specifically use the MADAMIRA tool (Pasha et al., 2014) for morphological analysis and disambiguation of MSA and EGY.","sentence2":"The CRF is trained using decisions from the following underlying components: @BULLET MADAMIRA: is a publicly available tool for morphological analysis and disambiguation of EDA and MSA text (Pasha et al., 2014)."},
{"id_case":"2943","class":"","sentence1":"As the source of our data, we use the training section of the NUS Corpus of Learner English (NU- CLE), which is a large corpus of essays written by non-native English speakers (Dahlmeier et al., 2013).","sentence2":"The training data for the task is from the NUCLE corpus (Dahlmeier et al., 2013), an error-tagged collection of essays written by non-native learners of English."},
{"id_case":"2946","class":"","sentence1":"Similar row vectors in T indicate similar context of two terms in the domain, and terms that occur in the same contexts tend to have similar meanings (Harris, 1954) ","sentence2":"Distributional models of meaning follow the distributional hypothesis (Harris, 1954), which states that two words that occur in similar contexts have similar meanings."},
{"id_case":"2947","class":"","sentence1":"Extraction of protein-localization event has been a subtask in BioNLP shared task from 2009 to 2013 in the Genia track (Kim et al., 2013).","sentence2":"Our participation in the Genia Event Extraction task of BioNLP 2013 (Kim et al., 2013) was motivated by the desire of testing our technologies on a more linguistically motivated task."},
{"id_case":"2948","class":"","sentence1":"Compared with the traditional approaches, such as TF-IDF, TextRank utilizes the context information of words to assign term weights (Lee et al., 2008), so it further improves the performance of CLPT.","sentence2":"Moreover, TextRank utilizes the context information of words to assign term weights (Lee et al., 2008), which makes phrase TM based crosslingual data selection model play its advantage of capturing."},
{"id_case":"2949","class":"","sentence1":"The German-to-English corpus is Europarl v7 (Koehn, 2005).","sentence2":"The source of bilingual data used in the experiments is the Europarl collection (Koehn, 2005)."},
{"id_case":"2950","class":"","sentence1":"Our experiment conducted using the Moses phrase-based decoder (Koehn et al., 2007).","sentence2":"Our baseline is a phrase-based MT system trained using the MOSES toolkit (Koehn et al., 2007)."},
{"id_case":"2951","class":"","sentence1":"For preprocessing the input text, we first process each sentence with Stanford CoreNLP (Manning et al., 2014).","sentence2":"To process the raw data, we stripped all HTML and other non-story content from each file and processed the remaining text with the Stanford CoreNLP pipeline version 3.3.1 (Manning et al., 2014)."},
{"id_case":"2952","class":"","sentence1":"L(.) stands for the Levenshtein distance (V. I. Levenshtein, 1966 ; R. A. Wagner and M. J. Fischer, 1974).","sentence2":"The Levenshtein distance gives an indication of the similarity between two strings (Levenshtein, 1966)."},
{"id_case":"2953","class":"","sentence1":"Word alignment is done using GIZA++ (Och and Ney, 2003).","sentence2":"The word-alignment is done using GIZA++ (Och and Ney, 2003) , toolkit and then growing heuristics are applied."},
{"id_case":"2954","class":"","sentence1":"We use the well-known LibSVM implementation (Chang and Lin, 2011).","sentence2":"Regression models are trained using the -SVR implementation available in the LibSVM toolkit (Chang and Lin, 2011)."},
{"id_case":"2955","class":"","sentence1":"The source of bilingual data used in the experiments is the Europarl collection (Koehn, 2005).","sentence2":"All the data come from Europarl (Koehn, 2005)."},
{"id_case":"2956","class":"","sentence1":"We use Scikit-learn (Pedregosa et al., 2011), the machine learning library for Python, for implementing the different approaches.","sentence2":"We use scikitlearn (Pedregosa et al., 2011) as machine learning library."},
{"id_case":"2957","class":"","sentence1":"We trained all models using AdaGrad (Duchi et al., 2011).","sentence2":"We train the concept identification stage using infinite ramp loss with AdaGrad (Duchi et al., 2011)."},
{"id_case":"2958","class":"","sentence1":"We used the CRFsuite (Okazaki, 2007) implementation of CRFs.","sentence2":"In this approach we used the NERsuite software based on the CRFsuite implementation (Okazaki, 2007)."},
{"id_case":"2959","class":"","sentence1":"Word alignment is done using GIZA++ (Och and Ney, 2003).","sentence2":"The word-alignment is done using GIZA++ (Och and Ney, 2003), toolkit and then growing heuristics are applied."},
{"id_case":"2960","class":"","sentence1":"We use the Stanford dependency parser (Marneffe et al., 2006).","sentence2":"We extract syntactic dependencies using Stanford Parser (de Marneffe et al., 2006), and use its collapsed dependency format."},
{"id_case":"2961","class":"","sentence1":"We use the Stanford dependency parser (Marneffe et al., 2006).","sentence2":"In our experiments , we used the Stanford parser (De Marneffe et al., 2006) to create dependency parses."},
{"id_case":"2962","class":"","sentence1":"We extract our paraphrase grammar from the French English portion of the Europarl corpus (version 5) (Koehn, 2005).","sentence2":"All the data come from Europarl (Koehn, 2005)."},
{"id_case":"2963","class":"","sentence1":"We use ROUGE score as our evaluation metric (Lin, 2004), with standard options 8 .","sentence2":"We used the ROUGE-1 evaluation metric (Lin, 2004)."},
{"id_case":"2964","class":"","sentence1":"The Stanford dependency parser (De Marneffe et al., 2006) is used for extracting features from the dependency parse trees.","sentence2":"We use the Stanford dependency parser (Marneffe et al., 2006)."},
{"id_case":"2964","class":"","sentence1":"The Stanford dependency parser (De Marneffe et al., 2006) is used for extracting features from the dependency parse trees.","sentence2":"We use the Stanford dependency parser (Marneffe et al., 2006)."},
{"id_case":"2965","class":"","sentence1":"Kneser-Ney n-grams were trained with KenLM (Heafield, 2011).","sentence2":"We built a trigram language model with Kneser-Ney smoothing using KenLM toolkit (Heafield, 2011)."},
{"id_case":"2966","class":"","sentence1":"Against the PARC 700, the hand-crafted LFG grammar reported in (Kaplan et al., 2004) achieves an fscore of 79.6%.","sentence2":"Evaluating against the PARC 700 Dependency Bank, the P-PCFG achieves an f-score of 80.24%, an overall improvement of approximately 0.6% on the result reported for the best hand-crafted grammars in (Kaplan et al., 2004)."},
{"id_case":"2968","class":"","sentence1":"For all syntactic parsers, we used the basic Stanford dependency representation (de Marneffe et al., 2006).","sentence2":"We use the Stanford dependency parser (Marneffe et al., 2006)."},
{"id_case":"2969","class":"","sentence1":"The biggest challenge in coreference resolution accounting for 42% of errors in the stateof-the-art Stanford system is the inability to reason effectively about background semantic knowledge (Lee et al., 2013).","sentence2":"The mention detection of the Stanford coreference system (Lee et al., 2013) is used for the OntoNotes data sets."},
{"id_case":"2970","class":"","sentence1":"conducted using the Moses phrase-based decoder (Koehn et al., 2007).","sentence2":"The corpora are tokenised and truecased using scripts from the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"2972","class":"","sentence1":"For preprocessing the input text, we first process each sentence with Stanford CoreNLP (Manning et al., 2014).","sentence2":"To process the raw data, we stripped all HTML and other non-story content from each file and processed the remaining text with the Stanford CoreNLP pipeline version 3.3.1 (Manning et al., 2014)."},
{"id_case":"2973","class":"","sentence1":"We found that using AdaGrad (Duchi et al., 2011) to update the parameters is very effective .","sentence2":"We trained all models using AdaGrad (Duchi et al., 2011)."},
{"id_case":"2974","class":"","sentence1":"Experiment is conducted using the Moses phrase-based decoder (Koehn et al., 2007).","sentence2":"The baseline systems are built with the opensource phrase-based SMT toolkit Moses (Koehn et al., 2007)."},
{"id_case":"2975","class":"","sentence1":"The Levenshtein distance gives an indication of the similarity between two strings (Levenshtein, 1966).","sentence2":"The error model normally use some type of approximate string matching, such as Levenshtein distance (Levenshtein, 1966), which measures the distance between two strings as the number of insertions, "},
{"id_case":"2976","class":"","sentence1":"Previous work on STSG learning has regulated the rule specificity based on elementary tree depth (Cohn and Lapata, 2009).","sentence2":"Previous approaches have modulated rule specificity by incorporating rules of varying depth in addition to the maximally general rule set (Cohn and Lapata, 2009)."},
{"id_case":"2977","class":"","sentence1":"We used a phrase-based statistical machine translation model as implemented in the Moses toolkit (Koehn et al., 2007), for machine translation.","sentence2":"As the baseline, we used a state-of-the-art phrase-based statistical machine translation model built using Moses (Koehn et al., 2007), with the standard features."},
{"id_case":"2979","class":"","sentence1":"The encoder is a bidirectional neural network with gated recurrent units (Cho et al., 2014).","sentence2":"Our model architecture is a bidirectional recurrent neural network with gated recurrent units (Bi-GRU) (Cho et al., 2014)."},
{"id_case":"2980","class":"","sentence1":"It also provides a general-purpose resource grammar library (RGL) for nearly 30 languages that implement the same abstract syntax, a shared syntactic API (Ranta, 2009).","sentence2":"Most importantly, GF provides a generalpurpose resource grammar library, RGL (Ranta, 2009), for currently 30 languages, all implementing the same abstract syntax."},
{"id_case":"2981","class":"","sentence1":"We use the Moses toolkit (Koehn et al., 2007) to create a statistical phrase-based machine translation model built on the best pre-processed data, as described above.","sentence2":"We then use the phrase extraction utility in the Moses statistical machine translation system (Koehn et al., 2007) to extract a phrase table which operates over characters ."},
{"id_case":"2984","class":"","sentence1":"The semantic representations used in the Grammar Matrix were originally derived from those used in the English Resource Grammar (Flickinger, 2000), a wide-coverage grammar of English.","sentence2":"It is used to support semantic analyses in the HPSG English Resource Grammar (Copestake and Flickinger, 2000), but also in other grammar formalism like LFG."},
{"id_case":"2985","class":"","sentence1":"The network was trained in batches (size 40) for 30 epochs with the Adam optimizer (Kingma and Ba, 2014), starting with a learning rate of 10.","sentence2":"We train our models using a mini-batch size of 128 and Adam optimizer (Kingma and Ba, 2014), with a learning rate of 2e-4 for a total of 10 epochs."},
{"id_case":"2987","class":"","sentence1":"ANEW (Bradley and Lang, 1999) is a resource developed to provide a set of normative emotional ratings for a large number of words (roughly 1 thousand) in the English language.","sentence2":"Finally the ANEW lexicon (Bradley and Lang, 1999) is used for selecting the initial set of seed words of (1)."},
{"id_case":"2988","class":"","sentence1":"For each QnA pair we applied tokenization, sentence detection, named entity tagger, parsing and coreference resolution from Stanford CoreNLP (Manning et al., 2014).","sentence2":"For preprocessing the input text, we first process each sentence with Stanford CoreNLP (Manning et al., 2014)."},
{"id_case":"2989","class":"","sentence1":"Words were downcased and lemmatized using the WordNet lemmatizer in the NLTK 2 toolkit (Bird et al., 2009).","sentence2":"Note that we are clustering lemmas rather than word forms, and lemmatization is performed using the NLTK WordNet lemmatizer (Bird et al., 2009)."},
{"id_case":"2991","class":"","sentence1":"To build the joint representation (mt#src) and to obtain source factors (mt|src), we use the word alignment model trained on src and mt pairs of the training data by using MGIZA++(Gao and Vogel, 2008).","sentence2":"For building the word alignment models we use MGIZA++ (Gao and Vogel, 2008)."},
{"id_case":"2992","class":"","sentence1":"For native data, several teams make use of the Web 1T 5-gram corpus (henceforth Web1T, (Brants and Franz, 2006)).","sentence2":"The other models are trained on native English data, the Google Web 1T 5-gram corpus (henceforth, Google, (Brants and Franz, 2006)) with the Naive Bayes (NB) algorithm."},
{"id_case":"2993","class":"","sentence1":"For syntactic features, we trained an Arabic dependency parser using Malt- Parser (Nivre et al., 2007) on the Columbia Arabic Treebank (CATiB) version of the PATB.","sentence2":"For the purposes of this paper, five dependency parsers were trained using MaltParser (Nivre et al., 2007)."},
{"id_case":"2994","class":"","sentence1":"Training is done with the Adam optimisation algorithm (Kingma and Ba, 2014) with learning rate of 10 .","sentence2":"The gradients are scaled with norm of 1.0 and the gradient update method being used is Adam (Kingma and Ba, 2014) with learning rate 0.0001."},
{"id_case":"2996","class":"","sentence1":"The same reference introduced the PARC data set, which we use in our experiments and which is based on the annotation of a database of attribution relations from the Penn Discourse Treebank B (Prasad et al., 2008).","sentence2":"We evaluate our model on the Penn Discourse Treebank (PDTB) (Prasad et al., 2008)."},
{"id_case":"2998","class":"","sentence1":"Projective trees can be found in O(n 3) time (Eisner, 2000), but cover only 63.6% of sentences in some natural language treebanks (Table 1).","sentence2":"This definition has the least coverage (as low as 63.6% for Dutch), but can be parsed in O(n 3) (Eisner, 2000)."},
{"id_case":"2999","class":"","sentence1":"Complete details of the task can be found at (Rosenthal et al., 2015).","sentence2":"An experimental evaluation of our approach and an in-depth comparison to the other participants is not included in this paper since it can be found in the task overview (Rosenthal et al., 2015)."},
{"id_case":"3000","class":"","sentence1":"For each QnA pair we applied tokenization, sentence detection, named entity tagger, parsing and coreference resolution from Stanford CoreNLP (Manning et al., 2014).","sentence2":"For preprocessing the input text, we first process each sentence with Stanford CoreNLP (Manning et al., 2014)."},
{"id_case":"3501","class":"","sentence1":"We train a log linear classifier on the features described in using scikit-learn (Pedregosa et al., 2011).","sentence2":"We used a GB implementation of the scikit-learn package (Pedregosa et al., 2011)."},
{"id_case":"3503","class":"","sentence1":"We train a log linear classifier on the features described in using scikit-learn (Pedregosa et al., 2011).","sentence2":"We used the scikit-learn toolkit to train our classifiers (Pedregosa et al., 2011)."},
{"id_case":"3504","class":"","sentence1":"Section 6.2 discusses (Gale and Church, 1991) , which is based on the statistic.","sentence2":"This issue is discussed in (Gale and Church, 1991)."},
{"id_case":"3506","class":"","sentence1":"WEKA (Hall et al., 2009) explorer was used for the classification task.","sentence2":"We used WEKA (Hall et al., 2009) for training the machine learning models and for feature selection."},
{"id_case":"3507","class":"","sentence1":"We used the TnT tagger (Brants, 2000), an implementation of the Viterbi algorithm for secondorder Markov model.","sentence2":"We used TnT (Brants, 2000), trained on the Negra training set."},
{"id_case":"3508","class":"","sentence1":"Named entity recognition (NER) is the task of finding rigid designators as they appear in free text and assigning them to coarse types such as person or geo-location (Nadeau and Sekine, 2007).","sentence2":"Named entity recognition is a well-established task (Nadeau and Sekine, 2007)."},
{"id_case":"3509","class":"","sentence1":"These features were obtained using the Stanford parser (Marneffe et al., 2006).","sentence2":"Both EN and ZH were converted to dependencies using v1.6.8 of the Stanford Converter (De Marneffe et al., 2006)."},
{"id_case":"3510","class":"","sentence1":"Statistical machine translation (SMT) (Koehn, 2010) is currently the leading paradigm in machine translation research.","sentence2":"However, it is in statistical machine translation (SMT) (Koehn, 2010) where the use of parallel corpora is more relevant."},
{"id_case":"3511","class":"","sentence1":"To reduce, (Hochreiter and Schmidhuber, 1997) has introduced long shortterm memory (LSTM).","sentence2":"We use long shortterm memory (Hochreiter and Schmidhuber, 1997 , LSTM) for our experiments using our own implementation ."},
{"id_case":"3512","class":"","sentence1":"We use GIZA++ (Och and Ney, 2003) with the grow-diag-final-and heuristic for word alignment.","sentence2":"We then run word alignment with GIZA++ (Och and Ney, 2003) in both directions, with the default parameters used in Moses."},
{"id_case":"3513","class":"","sentence1":"ECAL is used by (Habash et al., 2012) to produce the CALIMA morphological analyzer for EGY.","sentence2":"CALIMA Lexicon (CALIMA-LEX) CALIMA (aka CALIM-ARZ) is an EGY morphological analyzer (Habash et al., 2012)."},
{"id_case":"3514","class":"","sentence1":"We calculated significance using paired bootstrap resampling (Koehn, 2004).","sentence2":"The improvement is statistically significant according to paired bootstrap resampling test (Koehn, 2004)."},
{"id_case":"3515","class":"","sentence1":"The significance tests were performed using the bootstrap resampling method (Koehn, 2004).","sentence2":"Significance tests are conducted using bootstrap sampling (Koehn, 2004)."},
{"id_case":"3516","class":"","sentence1":"This demonstrates that we are competitive with the methods described in (Och and Ney, 2000).","sentence2":"First, we word-align the corpus with Giza++ (Och and Ney, 2000)."},
{"id_case":"3517","class":"","sentence1":"Our word pairs are lemmatized using the Wordnetbased lemmatizer of NLTK (Loper and Bird, 2002).","sentence2":"Both corpora are preprocessed using NLTK (Loper and Bird, 2002)."},
{"id_case":"3518","class":"","sentence1":"We used Adam (Kingma and Ba, 2014) with a learning rate of 0.0002.","sentence2":"The NMT models are trained using Adam optimizer (Kingma and Ba, 2014) with an initial learning rate of 0.0001."},
{"id_case":"3519","class":"","sentence1":"We used to measure the reliability of the annotation (Cohen, 1960).","sentence2":"We have used the familiar measure of Cohen \" s kappa (Cohen, 1960) for assessing the quality of annotation."},
{"id_case":"3520","class":"","sentence1":"For all experiments, we used the Moses SMT system (Koehn et al., 2007).","sentence2":"For building our SMT systems, the open-source SMT toolkit Moses (Koehn et al., 2007) was used in its standard setup."},
{"id_case":"3521","class":"","sentence1":"We built a source-to-target PB-SMT model from the bilingual domain corpus using the Moses toolkit (Koehn et al., 2007).","sentence2":"The English side was tokenized using the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"3523","class":"","sentence1":"We develop translation models using the phrase-based Moses (Koehn et al., 2007) SMT system.","sentence2":"We trained a model using Moses toolkit (Koehn et al., 2007) on the training data as our baseline system."},
{"id_case":"3524","class":"","sentence1":"For language model, we use SRI Language Modeling Toolkit 6 to train a trigram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) on the target side of training corpus.","sentence2":"A trigram language model with modified interpolated Kneser- Ney smoothing (Chen and Goodman, 1998) was trained by the SRILM toolkit on the Xinhua portion of the Gigaword corpus and the English side "},
{"id_case":"3535","class":"","sentence1":"In order to compare our method to a well understood phrase baseline, we present a method that tracts phrases by harvesting the Viterbi path from an HMM alignment model (Vogel et al., 1996).","sentence2":"In this section, we briefly review the HMM alignment model (Vogel et al., 1996)."},
{"id_case":"3539","class":"","sentence1":"As in other uses of parallel corpora, good alignment is essential in order for the results to be meaningful (Och and Ney, 2000).","sentence2":"Och is the HMM alignment model of (Och and Ney, 2000)."},
{"id_case":"3540","class":"","sentence1":"All systems were tuned using batch MIRA (Cherry and Foster, 2012).","sentence2":"SMT systems are trained with Moses 3.0, using default settings unless mentioned otherwise , and tuned with MIRA (Cherry and Foster, 2012)."},
{"id_case":"3541","class":"","sentence1":"We apply the Stanford coreference resolution system (Lee et al., 2013).","sentence2":"Preliminary work on Coreference for Basque was done by (Soraluze et al., 2015) where they adapt the Stanford coreference resolution system (Lee et al., 2013) to Basque."},
{"id_case":"3542","class":"","sentence1":"We calculated significance using paired bootstrap resampling (Koehn, 2004).","sentence2":"Statistical significance is tested on the BLEU metric using paired bootstrap resampling (Koehn, 2004) with n = 1000 and p = 0.05."},
{"id_case":"3543","class":"","sentence1":"A string similarity measure such as the Levenshtein distance (Levenshtein, 1966) is commonly used to measure the similarity of source texts in TM systems.","sentence2":"We use the Levenshtein distance (Levenshtein, 1966) as the similarity metric."},
{"id_case":"3550","class":"","sentence1":"In line with tree kernels over structures (Collins and Duffy, 2002), we introduce the set S(t) of the subtrees t i of a given lexicalized tree t.","sentence2":"As for the tree kernels (Collins and Duffy, 2002), the set S(t) contains all CSSTs derived from the subtrees of t such that if a node n belongs to a subtree t s , all the siblings of n in t belongs "},
{"id_case":"3556","class":"","sentence1":"We experiment with the phrase-based statistical machine translation toolkit Moses (Koehn et al., 2007) in order to train a Japanese -English system and to show the influence of the expanded parallel","sentence2":"We used the Moses toolkit (Koehn et al., 2007) to train standard phrase-based systems with default configurations."},
{"id_case":"3557","class":"","sentence1":"The training set, which is used to train the phrase-based translation model and language model for the-state-of-the-art phrase-based system Moses (Koehn et al., 2007), contains 1.21M Chinese-English","sentence2":"For training the translation model and for decoding we used the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"3558","class":"","sentence1":"and domains of application, we experimented with a product reviews dataset (Hu and Liu, 2004) and additional lexicons as follows.","sentence2":"We use the OpinionFinder and General Inquirer lexicons (OFL and GIL) as before, as well as the lexicon of positive and negative sentiment and opinion words available along with (Hu and Liu, 2004) pr"},
{"id_case":"3560","class":"","sentence1":"We use the OpinionFinder and General Inquirer lexicons (OFL and GIL) as before, as well as the lexicon of positive and negative sentiment and opinion words available along with (Hu and Liu, 2004).","sentence2":"and domains of application, we experimented with a product reviews dataset (Hu and Liu, 2004) and additional lexicons as follows."},
{"id_case":"3562","class":"","sentence1":"To train a mention-pair classifier, we use the SVM learning algorithm from the SVM light package (Joachims, 2002), converting all multi-valued features into an equivalent set of binary-valued feature.","sentence2":"We train our first baseline, the mention-pair coreference classifier, using the SVM learning algorithm as implemented in the SVM light package (Joachims, 2002)."},
{"id_case":"3563","class":"","sentence1":"The tagger we use is TnT (Brants, 2000) , a hidden Markov trigram tagger, which was trained on the Spoken Dutch Corpus (CGN), Internal Release 6.","sentence2":"We employed the TnT tagger (Brants, 2000) which was trained on the spective CoNLL training data."},
{"id_case":"3564","class":"","sentence1":"We conducted baseline experiments for phrasebased machine translation using the Moses toolkit (Koehn et al., 2007).","sentence2":"We preprocess the Gigaword data with the following tools from the Moses machine translation toolkit (Koehn et al., 2007): the data is tokenized using tokenizer.perl; truecase.perl4 is used to standard."},
{"id_case":"3565","class":"","sentence1":"Backward feature selection 3 in order to perform feature selection, we used the Random Forest algorithm, as implemented in the scikit-learn toolkit (Pedregosa et al., 2011), to rank the features .","sentence2":"Specifically, we used the standard Gradient Boosting Regressor in the scikit-learn toolkit 4 (Pedregosa et al., 2011)."},
{"id_case":"3566","class":"","sentence1":"Both systems are based on the Moses SMT toolkit (Koehn et al., 2007) and constructed as follows.","sentence2":"The Edinburgh\/JHU phrase-based translation systems for our participation in the WMT 2015 shared translation task 1 are based on the open source Moses toolkit (Koehn et al., 2007)."},
{"id_case":"3569","class":"","sentence1":"We preprocess the Gigaword data with the following tools from the Moses machine translation toolkit (Koehn et al., 2007): the data is tokenized using tokenizer.perl; truecase.perl4 is used to standard.","sentence2":"We used the Moses toolkit (Koehn et al., 2007) to train standard phrase-based systems with default configurations."},
{"id_case":"3570","class":"","sentence1":"We build our PB-SMT systems in a standard way using the Moses system , KenLM for language modelling (Heafield, 2011), and standard lexical reordering model.","sentence2":"We built a trigram language model with Kneser-Ney smoothing using KenLM toolkit (Heafield, 2011)."},
{"id_case":"3571","class":"","sentence1":"We preprocess the Gigaword data with the following tools from the Moses machine translation toolkit (Koehn et al., 2007): the data is tokenized using tokenizer.perl; truecase.perl4 is used to standard.","sentence2":"We built phrase-based machine translation systems using the open software toolkit Moses (Koehn et al., 2007)."},
{"id_case":"3572","class":"","sentence1":"We preprocess the Gigaword data with the following tools from the Moses machine translation toolkit (Koehn et al., 2007): the data is tokenized using tokenizer.perl; truecase.perl4 is used to standard.","sentence2":"We tokenize and frequent-case the data with the standard scripts from the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"3573","class":"","sentence1":"Among the different possible distance metrics that can be applied for calculating the phonetic similarity between two pronunciation strings, we have chosen the Levenshtein distance (Levenshtein, 1966).","sentence2":"The Levenshtein distance gives an indication of the similarity between two strings (Levenshtein, 1966)."},
{"id_case":"3575","class":"","sentence1":"In this paper, we present the different systems we developed as part of our participation in SemEval-2017 Task 4 on Sentiment Analysis in Twitter (Rosenthal et al., 2017).","sentence2":"Task 4 of SemEval 2017, Sentiment Analysis in Twitter (Rosenthal et al., 2017), has included some new subtasks this year."},
{"id_case":"3577","class":"","sentence1":"The SMT systems are trained with the Moses toolkit (Koehn et al., 2007), according to the WMT 2011 guidelines 6 .","sentence2":"The Edinburgh\/JHU phrase-based translation systems for our participation in the WMT 2015 shared translation task 1 are based on the open source Moses toolkit (Koehn et al., 2007)."},
{"id_case":"3578","class":"","sentence1":"All of our experiments were based on the RCV1 corpus which contains one year of Reuters English newswire from August 1996 to August 1997 (Lewis et al., 2004).","sentence2":"We conducted our experiments on the RCV1- v2 (Lewis et al., 2004) corpus, which is a corrected version of RCV1 (Reuters Corpus Volume 1)."},
{"id_case":"3579","class":"","sentence1":"As bilingual data, we used the provided training set, the Autodesk Post-Editing Data 2 and a collection of corpora from OPUS (Tiedemann, 2012) in the IT domain.","sentence2":"For Portuguese-Brazilian, we used the OPUS corpus (Tiedemann, 2012) which is a growing collection of translated texts from the web."},
{"id_case":"3583","class":"","sentence1":"We use the Moses (Koehn et al., 2007) toolkit with a phrase-based baseline to extract the QE features for the x l , x u , and testing.","sentence2":"We use the Moses phrase-based translation system (Koehn et al., 2007) to implement our models."},
{"id_case":"3585","class":"","sentence1":"Word alignments are created by aligning the data in both directions with GIZA++ and symmetrizing the two trained alignments (Och and Ney, 2003).","sentence2":"The parallel sentences are aligned in both directions with Giza++ (Och and Ney, 2003)."},
{"id_case":"3587","class":"","sentence1":"It is a phrase-based system built using the Moses toolkit (Koehn et al., 2007) and trained\/tuned using only the preprocessed (tokenised, lower-cased) parallel data provided for the shared task.","sentence2":"The questions are translated using a phrase-based system built using Moses (Koehn et al., 2007) (the Mo set)."},
{"id_case":"3588","class":"","sentence1":"The word alignment was trained using GIZA++ (Och and Ney, 2003) with the configuration grow-diag-final-and.","sentence2":"We trained an IBM model 4 using GIZA++ (Och and Ney, 2003) with the in-domain corpus and computed the alignment scores over the United Nations sentences."},
{"id_case":"3590","class":"","sentence1":"This new direction is in part inspired by a desire for greater interoperability with the Abstract Meaning Representation (AMR) project (Banarescu et al., 2013).","sentence2":"Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013)."},
{"id_case":"3592","class":"","sentence1":"Both of our systems were based on the Moses decoder (Koehn et al., 2007).","sentence2":"This paper describes the development of a statistical machine translation system based on the Moses decoder (Koehn et al., 2007) for the 2007 WMT shared tasks."},
{"id_case":"3593","class":"","sentence1":"This new direction is in part inspired by a desire for greater interoperability with the Abstract Meaning Representation (AMR) project (Banarescu et al., 2013).","sentence2":"Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013)."},
{"id_case":"3594","class":"","sentence1":"For Reuters we segmented and tokenized the data using NLTK (Bird et al., 2009).","sentence2":"Note that we are clustering lemmas rather than word forms, and lemmatization is performed using the NLTK WordNet lemmatizer (Bird et al., 2009)."},
{"id_case":"3597","class":"","sentence1":"Two clinicians (RB and SW) annotated the reports using BRAT (Stenetorp et al., 2012), a collaborative tool for text annotation that was configured to use our own schema.","sentence2":"The annotation was performed using the BRAT tool (Stenetorp et al., 2012)."},
{"id_case":"3598","class":"","sentence1":"In addition, the corpus was lemmatised using the TreeTagger lemmatiser (Schmid, 1994).","sentence2":"The corpus was converted from XML to raw text, various string normalization operations were then applied, and the corpus was lemmatized using TreeTagger (Schmid, 1994)."},
{"id_case":"3601","class":"","sentence1":"For language modelling we use the KenLM implementation (Heafield, 2011).","sentence2":"For language modeling, we trained a separate 5-gram Kneser-Ney smoothed LM model on the target (i.e. English) side of the training bi-text using KenLM (Heafield, 2011)."},
{"id_case":"3602","class":"","sentence1":"We now discuss the modifications made to the parameter files generated by TnT (Brants, 2000), our HMM POS tagger, after being trained on Farsi.","sentence2":"We used TnT (Brants, 2000), trained on the Negra training set."},
{"id_case":"3603","class":"","sentence1":"For the summarization task, we compare results using ROUGE (Lin, 2004).","sentence2":"Because our goal is to focus on the viability of summarization units for content selection, we evaluated system-generated summaries using ROUGE (Lin, 2004)."},
{"id_case":"3604","class":"","sentence1":"MRS is integrated with the HPSG English Resource Grammar (ERG) (Flickinger, 2000).","sentence2":"It is used to support semantic analyses in the HPSG English Resource Grammar (Copestake and Flickinger, 2000), but also in other grammar formalisms like LFG."},
{"id_case":"3605","class":"","sentence1":"The phrase translation probabilities are smoothed with Good-Turing smoothing (Foster et al., 2006).","sentence2":"The raw rel-ative frequency estimates found in the phrase translation tables are then smoothed by applying modified Kneser-Ney discounting as explained in (Foster et al., 2006)."},
{"id_case":"3609","class":"","sentence1":"The edit distance kernel was trained with LIBSVM (Chang and Lin, 2011).","sentence2":"The STCs-based semantic parser is implemented with linear kernel SVMs trained using the Lib- SVM package (Chang and Lin, 2011)."},
{"id_case":"3610","class":"","sentence1":"The NJU-Parser is based on the state-of-theart MSTParser (McDonald, 2006).","sentence2":"The second algorithm, denoted GloTr, is the Chu-Liu-Edmonds algorithm for maximal spanning tree implemented in the MSTParser (McDonald, 2006)."},
{"id_case":"3612","class":"","sentence1":"We introduce a fully automated judge for semantic similarity that performs well in the semantic textual similarity (STS) task (Agirre et al., 2013).","sentence2":"The semantic textual similarity (STS) task (Agirre et al., 2013) addresses the following problem."},
{"id_case":"3613","class":"","sentence1":"Word alignment was done with GIZA++ (Och and Ney, 2003) for both systems.","sentence2":"GIZA++ (Och and Ney, 2003) implementation of IBM word alignment model 4 with the grow-diagonalfinal-and heuristic was used for performing word alignment."},
{"id_case":"3614","class":"","sentence1":"The statistics on the distributions of words, alignments and lexical probabilities were extracted from the Europarl corpus (Koehn, 2005).","sentence2":"We used the English side of the Europarl corpus (Koehn, 2005)."},
{"id_case":"3615","class":"","sentence1":"We use the support vector machine (SVM) rank algorithm (Joachims, 2002) to predict a rank order for each list of comments.","sentence2":"We use SVM rank (Joachims, 2002) for training the candidate-ranking model."},
{"id_case":"3616","class":"","sentence1":"Finally, we used the GIZA++ toolkit (Och and Ney, 2003) to induce word alignments in both directions for each language pair.","sentence2":"We used the GIZA++ software (Och and Ney, 2003) to do the word alignments."},
{"id_case":"3617","class":"","sentence1":"We used the Moses toolkit (Koehn et al., 2007) with its default settings.","sentence2":"For the extraction of translation tables, we use the de facto standard SMT toolbox Moses (Koehn et al., 2007) with default settings."},
{"id_case":"3618","class":"","sentence1":"We estimated a hierarchical MT model for the train partition with the standard configuration of the Moses toolkit (Koehn et al., 2007).","sentence2":"We used the Moses toolkit (Koehn et al., 2007) with its default settings."},
{"id_case":"3619","class":"","sentence1":"We used GIZA++ (Och and Ney, 2003) to align the words in the corpus.","sentence2":"Finally, we used the GIZA++ toolkit (Och and Ney, 2003) to induce word alignments in both directions for each language pair."},
{"id_case":"3624","class":"","sentence1":"We built a 5-gram language model on the English side of QCA-train using KenLM (Heafield, 2011).","sentence2":"Language Model: For all 3-SCFG systems we use a 4-gram Kneser-Ney smoothed language model trained using the KenLM toolkit (Heafield, 2011)."},
{"id_case":"3625","class":"","sentence1":"Vector space models represent the semantics of natural language using vectors and operations on vectors (Turney and Pantel, 2010).","sentence2":"Vector space models of words have been very successful in capturing the semantic and syntactic characteristics of individual lexical items (Turney and Pantel, 2010)."},
{"id_case":"3627","class":"","sentence1":"Training is again done with the Adam optimisation algorithm (Kingma and Ba, 2014) with learning rate of 10 .","sentence2":"The gradients are scaled with norm of 1.0 and the gradient update method being used is Adam (Kingma and Ba, 2014) with learning rate 0.0001."},
{"id_case":"3629","class":"","sentence1":"Word alignment is computed using the GIZA++ toolkit (Och and Ney, 2003), only one-to-one word alignments are employed.","sentence2":"We perform word alignment by using GIZA++ (Och and Ney, 2003), while utilizing IBM model 4 for creating the Viterbi word alignments for parallel corpora."},
{"id_case":"3630","class":"","sentence1":"We first split each document in the corpus into sentences and create a shallow Discourse Representation Structure (following Discourse Representation Theory (Kamp and Reyle, 1993)) of each sentence.","sentence2":"The standard output is a meaning representation in the form of a Discourse Representation Structure (Kamp and Reyle, 1993)."},
{"id_case":"3631","class":"","sentence1":"In (Teufel and Moens, 2002) the authors use rhetorical status of statements in a scientific article to produce a summary.","sentence2":"AZ is an approach to information structure which provides an analysis of the rhetorical progression of the scientific argument in a document (Teufel and Moens, 2002)."},
{"id_case":"3633","class":"","sentence1":"We first split each document in the corpus into sentences and create a shallow Discourse Representation Structure (following Discourse Representation Theory (Kamp and Reyle, 1993)) of each sentence.","sentence2":"The standard output is a meaning representation in the form of a Discourse Representation Structure (Kamp and Reyle, 1993)."},
{"id_case":"3637","class":"","sentence1":"We use the Stanford dependency parser (Marneffe et al., 2006).","sentence2":"The grammatical relations are all the collapsed dependencies produced by the Stanford Dependency parser (Marneffe et al., 2006)."},
{"id_case":"3638","class":"","sentence1":"As for EJ translation, we use the Stanford parser (de Marneffe et al., 2006) to obtain English abstraction trees.","sentence2":"We use the Stanford dependency parser (Marneffe et al., 2006)."},
{"id_case":"3639","class":"","sentence1":"MIRA and RM were run with 15 parallel learners using iterative parameter mixing (McDonald et al., 2010).","sentence2":"We resort to iterative parameter mixing (McDonald et al., 2010)."},
{"id_case":"3640","class":"","sentence1":"Word alignments on the parallel corpus are performed using GIZA++ (Och and Ney, 2003) with the \" grow-diag-final \" refinement.","sentence2":"Word alignments were created using GIZA++ (Och and Ney, 2003)."},
{"id_case":"3641","class":"","sentence1":"This follows from the distributional hypothesis (Harris, 1954).","sentence2":"Distributional similarity relies on the distributional hypothesis that similar terms appear in similar contexts (Harris, 1954)."},
{"id_case":"3642","class":"","sentence1":"conducted using the Moses phrase-based decoder (Koehn et al., 2007).","sentence2":"The decoder is built on top of an open-source phrase-based SMT decoder, Moses (Koehn et al., 2007)."},
{"id_case":"3644","class":"","sentence1":"Co-training (Blum and Mitchell, 1998) is a semi-supervised learning technique that requires two different views of the data.","sentence2":"Co-training(Blum and Mitchell, 1998) is a powerful unsupervised learning method."},
{"id_case":"3645","class":"","sentence1":"We minimize the cross entropy loss using gradient-based optimization and the Adam update rule (Kingma and Ba, 2014).","sentence2":"We used Adam as the optimizer (Kingma and Ba, 2014)."},
{"id_case":"3646","class":"","sentence1":"Word alignments on the parallel corpus are performed using GIZA++ (Och and Ney, 2003) with the \" grow-diag-final \" refinement.","sentence2":"Word alignment is performed using GIZA++ (Och and Ney, 2003)."},
{"id_case":"3647","class":"","sentence1":"In this paper, in order to be comparable with Wang et al. (2011) , we evaluate our system against the corpora from the Second International Chinese Word Segmentation Bakeoff (Emerson, 2005).","sentence2":"The evaluation presented here uses the corpora from the 2005 Chinese Word Segmentation Bakeoff (Emerson, 2005)."},
{"id_case":"3648","class":"","sentence1":"Word alignments for the parallel documents were computed using Giza++ (Och and Ney, 2003) run in both directions.","sentence2":"Word alignments were created using GIZA++ (Och and Ney, 2003)."},
{"id_case":"3649","class":"","sentence1":"We train an SVM classifier (Cortes and Vapnik, 1995) to assign a score to the ground atom ETARGET(y,t).","sentence2":"We use the SVM classifier (Cortes and Vapnik, 1995) from ScikitLearn."},
{"id_case":"3651","class":"","sentence1":"We train an SVM classifier (Cortes and Vapnik, 1995) to assign a score to the ground atom ETARGET(y,t).","sentence2":"We use the SVM classifier (Cortes and Vapnik, 1995) from ScikitLearn."},
{"id_case":"3655","class":"","sentence1":"We use the implementation provided by CRFsuite 7 (Okazaki, 2007) for both training and classification tasks.","sentence2":"We used the CRFsuite (Okazaki, 2007) implementation of CRFs."},
{"id_case":"3656","class":"","sentence1":"Statistics are shown in  We built unpruned modified Kneser-Ney language models using lmplz (Heafield et al., 2013).","sentence2":"Language model (LM) are built using KENLM (Heafield et al., 2013)."},
{"id_case":"3659","class":"","sentence1":"The SMT systems are trained with the Moses toolkit (Koehn et al., 2007), according to the WMT 2011 guidelines 6 .","sentence2":"We used the Moses toolkit (Koehn et al., 2007) to train standard phrase-based systems with default configurations."},
{"id_case":"3660","class":"","sentence1":"We select as a general-purpose corpus Europarl v7 (Koehn, 2005), with 1.97M parallel sentences.","sentence2":"We worked with the Europarl corpus (Koehn, 2005) in order to have a parallel comparative corpus for Italian and Spanish."},
{"id_case":"3661","class":"","sentence1":"For a pair of words, WordNet provides a series of measures of the semantic similarity (Pedersen et al., 2004).","sentence2":"We use the popular WordNet::Similarity (Pedersen et al., 2004) package which measures the semantic similarity and relatedness between a pairs of concepts."},
{"id_case":"3662","class":"","sentence1":"We use the Stanford parser to generate a DG for each sentence (de Marneffe et al., 2006).","sentence2":"As mentioned before, we use the Stanford parser (De Marneffe et al., 2006) to obtain a typed dependency representation of the input sentence."},
{"id_case":"3663","class":"","sentence1":"The statistics on the distributions of words, alignments and lexical probabilities were extracted from the Europarl corpus (Koehn, 2005).","sentence2":"We extract our paraphrase grammar from the French-English portion of the Europarl corpus (version 5) (Koehn, 2005)."},
{"id_case":"3664","class":"","sentence1":"By adding a max pooling layer to the network, the TDNN can be adopted as a sentence model (Collobert and Weston, 2008).","sentence2":"The Max-TDNN sentence model is based on the architecture of a TDNN (Collobert and Weston, 2008)."},
{"id_case":"3665","class":"","sentence1":"We use the ROUGE evaluation metrics which has shown consistent correlation with manually evaluated summarization scores (Lin, 2004).","sentence2":"We use ROUGE score as our evaluation metric (Lin, 2004) with standard options 8 ."},
{"id_case":"3667","class":"","sentence1":"We estimated a hierarchical MT model for the train partition with the standard configuration of the Moses toolkit (Koehn et al., 2007).","sentence2":"For training the translation model and for decoding we used the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"3668","class":"","sentence1":"use the Stanford Parser (de Marneffe et al., 2006) to extract a set of dependencies from each comment.","sentence2":"As mentioned before, we use the Stanford parser (De Marneffe et al., 2006) to obtain a typed dependency representation of the input sentence."},
{"id_case":"3669","class":"","sentence1":"TERp identifies words in the hypothesis and reference that share the same stem using the Porter stemming algorithm (Porter, 1980).","sentence2":"We then reduced each word to its stem using the Porter algorithm (Porter, 1980)."},
{"id_case":"3670","class":"","sentence1":"As mentioned before, we use the Stanford parser (De Marneffe et al., 2006) to obtain a typed dependency representation of the input sentence.","sentence2":"In our experiments , we used the Stanford parser (De Marneffe et al., 2006) to create dependency parses."},
{"id_case":"3671","class":"","sentence1":"We use the Stanford parser to generate a DG for each sentence (de Marneffe et al., 2006).","sentence2":"We used a rule-based method to extract temporal expressions and used Stanford parser (De Marneffe et al., 2006) to analyze association between the temporal expressions and the extractions."},
{"id_case":"3672","class":"","sentence1":"As mentioned before, we use the Stanford parser (De Marneffe et al., 2006) to obtain a typed dependency representation of the input sentence.","sentence2":"In our experiments , we used the Stanford parser (De Marneffe et al., 2006) to create dependency parses."},
{"id_case":"3673","class":"","sentence1":"use the Stanford Parser (de Marneffe et al., 2006) to extract a set of dependencies from each comment.","sentence2":"In our experiments, we use the Stanford parser (Marneffe et al., 2006) to automatically extract the grammatical role for each entity mention."},
{"id_case":"3674","class":"","sentence1":"We conducted baseline experiments for phrasebased machine translation using the Moses toolkit (Koehn et al., 2007).","sentence2":"We refer to that model as Moses en-es-100k , because it was trained using the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"3675","class":"","sentence1":"We use the Stanford parser to generate a DG for each sentence (de Marneffe et al., 2006).","sentence2":"As mentioned before, we use the Stanford parser (De Marneffe et al., 2006) to obtain a typed dependency representation of the input sentence."},
{"id_case":"3677","class":"","sentence1":"The SMT systems are trained with the Moses toolkit (Koehn et al., 2007), according to the WMT 2011 guidelines 6 .","sentence2":"Both systems are based on the Moses SMT toolkit (Koehn et al., 2007) and constructed as follows."},
{"id_case":"3678","class":"","sentence1":"The tectogrammatical annotation layer is based on the Functional Generative Description theory (Sgall et al., 1986).","sentence2":"The present paper describes a tentative semantic representation of NSUs in the Functional Generative Description (FGD) framework (Sgall et al., 1986)."},
{"id_case":"3679","class":"","sentence1":"We conducted baseline experiments for phrasebased machine translation using the Moses toolkit (Koehn et al., 2007).","sentence2":"We estimated a hierarchical MT model for the train partition with the standard configuration of the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"3680","class":"","sentence1":"The SMT systems are trained with the Moses toolkit (Koehn et al., 2007), according to the WMT 2011 guidelines 6 .","sentence2":"The corpora are tokenised and truecased using scripts from the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"3681","class":"","sentence1":"To test our method, we conducted two lowresource translation experiments using the phrase-based MT system Moses (Koehn et al., 2007).","sentence2":"Our baseline is a phrase-based MT system trained using the MOSES toolkit (Koehn et al., 2007)."},
{"id_case":"3682","class":"","sentence1":"To test our method, we conducted two lowresource translation experiments using the phrase-based MT system Moses (Koehn et al., 2007).","sentence2":"We built phrase-based machine translation systems using the open software toolkit Moses (Koehn et al., 2007)."},
{"id_case":"3683","class":"","sentence1":"We estimated a hierarchical MT model for the train partition with the standard configuration of the Moses toolkit (Koehn et al., 2007).","sentence2":"We build a state of the art phrase-based SMT system using Moses (Koehn et al., 2007)."},
{"id_case":"3684","class":"","sentence1":"We use the Stanford parser to generate a DG for each sentence (de Marneffe et al., 2006).","sentence2":"We used a rule-based method to extract temporal expressions and used Stanford parser (De Marneffe et al., 2006) to analyze association between the temporal expressions and the extractions."},
{"id_case":"3685","class":"","sentence1":"All experiments are conducted using the Moses phrase-based SMT system (Koehn et al., 2007) with a maximum phrase length of 8.","sentence2":"The baseline systems are built with the opensource phrase-based SMT toolkit Moses (Koehn et al., 2007)."},
{"id_case":"3687","class":"","sentence1":"We report results for the method on the NIST 2006 evaluation data, using the MOSES phrase-based SMT system (Koehn et al., 2007).","sentence2":"Our baseline is a phrase-based MT system trained using the MOSES toolkit (Koehn et al., 2007)."},
{"id_case":"3688","class":"","sentence1":"The statistics on the distributions of words, alignments and lexical probabilities were extracted from the Europarl corpus (Koehn, 2005).","sentence2":"Europarl (Koehn, 2005) is a multilingual parallel corpus extracted from the proceedings of the European Parliament."},
{"id_case":"3689","class":"","sentence1":"Word alignment is performed by GIZA++ (Och and Ney, 2000) in both directions with the default setting.","sentence2":"Specifically, GIZA++ toolkit (Och and Ney, 2000) with default setting is used for word alignment on the JRC-Acquis parallel corpora (Steinberger et al., 2006)."},
{"id_case":"3690","class":"","sentence1":"Our previous MLN-based approach for joint disambiguation and clustering of concepts (Fahrni and Strube, 2012).","sentence2":"Table 5 compares the overall accurracy of the scope-ignorant joint disambiguation and clustering approach (Fahrni and Strube, 2012) with the accurracy of the corresponding joint scope-aware approach"},
{"id_case":"3691","class":"","sentence1":"To test our method, we conducted two lowresource translation experiments using the phrase-based MT system Moses (Koehn et al., 2007).","sentence2":"We build a state of the art phrase-based SMT system using Moses (Koehn et al., 2007)."},
{"id_case":"3692","class":"","sentence1":"use the Stanford Parser (de Marneffe et al., 2006) to extract a set of dependencies from each comment.","sentence2":"As mentioned before, we use the Stanford parser (De Marneffe et al., 2006) to obtain a typed dependency representation of the input sentence."},
{"id_case":"3694","class":"","sentence1":"We use the Moses software package 5 to train a PBMT model (Koehn et al., 2007).","sentence2":"We refer to that model as Moses en-es-100k , because it was trained using the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"3695","class":"","sentence1":"We built phrase-based machine translation systems using the open software toolkit Moses (Koehn et al., 2007).","sentence2":"We refer to that model as Moses en-es-100k , because it was trained using the Moses toolkit (Koehn et al., 2007)."},
{"id_case":"3696","class":"","sentence1":"We measure statistical significance using 95% confidence intervals computed with paired bootstrap resampling (Koehn, 2004).","sentence2":"We perform bootstrap resampling with bounds estimation as described by (Koehn, 2004)."},
{"id_case":"3697","class":"","sentence1":"We perform bootstrap resampling with bounds estimation as described in (Koehn, 2004).","sentence2":"We measure statistical significance using 95% confidence intervals computed with paired bootstrap resampling (Koehn, 2004)."},
{"id_case":"3698","class":"","sentence1":"The Weka SMO implementation of SVM (Hall et al., 2009) was used as classifier with default parameter settings.","sentence2":"We used the SMO module in Weka (version 3.6.9) (Hall et al., 2009) as an SVM implementation."},
{"id_case":"3699","class":"","sentence1":"We transformed the parse trees in OntoNotes into syntactic dependencies using Stanford CoreNLP (Manning et al., 2014).","sentence2":"We use the Stanford CoreNLP caseless tagger for part-of-speech tagging (Manning et al., 2014)."},
{"id_case":"3701","class":"","sentence1":"A chunk is a minimal , non-recursive structure consisting of correlated groups of words (Bharati et al., 2006).","sentence2":"The chunk label tagset is a coarser version of AnnCorra tagset (Bharati et al., 2006)."},
{"id_case":"3702","class":"","sentence1":"MC-30: A subset of RG-65 dataset with 30 word pairs (Miller and Charles, 1991).","sentence2":"We use a set of 30 word pairs from a study carried out by (Miller and Charles, 1991)."},
{"id_case":"3703","class":"","sentence1":"We obtained news-peg judgments using the Brat annotation tool (Stenetorp et al., 2012) from two annotators 3 .","sentence2":"All annotations were done using the brat rapid annotation tool (Stenetorp et al., 2012)."},
{"id_case":"3704","class":"","sentence1":"We transformed the parse trees in OntoNotes into syntactic dependencies using Stanford CoreNLP (Manning et al., 2014).","sentence2":"We use the Stanford CoreNLP caseless tagger for part-of-speech tagging (Manning et al., 2014)."},
{"id_case":"3705","class":"","sentence1":"Gradient clipping heuristic to prevent the \" exploding gradient \" problem (Graves, 2013).","sentence2":"For the exploding gradient problem, numerical stability can be achieved by clipping the gradients (Graves, 2013)."},
{"id_case":"3706","class":"","sentence1":"We obtained news-peg judgments using the Brat annotation tool (Stenetorp et al., 2012) from two annotators 3 .","sentence2":"The annotations were made using the BRAT rapid annotation tool (Stenetorp et al., 2012)."},
{"id_case":"3707","class":"","sentence1":"We then describe in more detail a modern Chinese corpus, the Penn Chinese Treebank (Xue et al., 2005).","sentence2":"For Chinese, we use the Penn Chinese Treebank Version 5.1 (CTB) (Xue et al., 2005)."},
{"id_case":"3708","class":"","sentence1":"In order to assess statistical significance of the obtained results, we use the paired bootstrap resampling method (Koehn, 2004) which estimates the probability (p-value) that a measured difference. ","sentence2":"For statistical significance testing, we use a paired bootstrap resampling method proposed in (Koehn, 2004)."},
{"id_case":"3709","class":"","sentence1":"Barrachina et al., (Barrachina et al., 2009) presented a prefix-based ITP approach in which the user is assumed to proof-read each automatic translation correcting each time the first error, if any, ","sentence2":"We have presented a new ITP approach where the user is not longer bound to interact with the system in a prefix-based fashion (Barrachina et al., 2009)."},
{"id_case":"3710","class":"","sentence1":"Statistical significance in BLEU score difference was measured by using paired bootstrap re-sampling (Koehn, 2004).","sentence2":"Significance was tested using a paired bootstrap (Koehn, 2004) with 1000 samples (p<0.02)."},
{"id_case":"3711","class":"","sentence1":"The article system builds on the elements of the system described in (Rozovskaya and Roth, 2010).","sentence2":"The preposition classifier uses a combined system, building on work described in (Rozovskaya and Roth, 2010)."},
{"id_case":"3712","class":"","sentence1":"For calculating the required frequencies, we use the Web1T corpus 6 (Brants and Franz, 2006).","sentence2":"As a practical approximation, we use bigram counts from the Web 1T corpus (Brants and Franz, 2006)."},
{"id_case":"3713","class":"","sentence1":"Phrase pairs were extracted from symmetrized word alignments and distortions generated by GIZA++ (Och and Ney, 2003) using the combination of heuristics \" grow-diagfinal-and \" and \" msd-bidirectional.","sentence2":"Word alignments were created using GIZA++ (Och and Ney, 2003)."},
{"id_case":"3714","class":"","sentence1":"We ran all of our experiments in Weka (Hall et al., 2009) using Logistic Regression.","sentence2":"Weka (Hall et al., 2009) which contains the implementation of all three algorithms was used in our study."},
{"id_case":"3715","class":"","sentence1":"Both corpora were extracted from the open parallel corpus OPUS (Tiedemann, 2012).","sentence2":"The parallel data were taken from OPUS (Tiedemann, 2012), which provides sentencealigned corpora with annotation."},
{"id_case":"3716","class":"","sentence1":"We ran all of our experiments in Weka (Hall et al., 2009) using Logistic Regression.","sentence2":"We conducted experiments using Multinomial Naive Bayes classifier implemented in the Weka toolkit (Hall et al., 2009)."},
{"id_case":"3717","class":"","sentence1":"The Spanish-English (S2E) training corpus was drawn from the Europarl collection (Koehn, 2005).","sentence2":"The experiments focus on translation from German to English, using the Europarl data (Koehn, 2005)."},
{"id_case":"3718","class":"","sentence1":"Type System extends the type system that is built into the UIMA 6 framework (Ferrucci and Lally, 2004).","sentence2":"This architecture is very similar to the framework of UIMA (Ferrucci and Lally, 2004)."},
{"id_case":"3719","class":"","sentence1":"For other languages we use the corpora made available for the CoNLL-X Shared Task (Buchholz and Marsi, 2006).","sentence2":"We use the CoNLL-X (Buchholz and Marsi, 2006) distribution data from seven different languages (Arabic, Bulgarian, Dutch, Portuguese, Slovene, Spanish and Swedish)."},
{"id_case":"3720","class":"","sentence1":"Table 5 compares our reordering model with a reimplementation of the reordering model proposed in (Tromble and Eisner, 2009).","sentence2":"We note that our model outperforms the model proposed in (Tromble and Eisner, 2009) in all cases."},
{"id_case":"3722","class":"","sentence1":"For example, OntoNotes (Hovy et al., 2006), a large-scale annotation project, chose this option.","sentence2":"To this end, a recent large-scale annotation effort called the OntoNotes project (Hovy et al., 2006) was started."},
{"id_case":"3723","class":"","sentence1":"As for EJ translation, we use the Stanford parser (de Marneffe et al., 2006) to obtain English abstraction trees.","sentence2":"We use Stanford parser (de Marneffe et al., 2006) to obtain parse trees and dependency relations."},
{"id_case":"3724","class":"","sentence1":"For both English and German we used the part-of speech tagger TreeTagger (Schmid, 1994) to obtain POS-tags.","sentence2":"Only for German data did we used the TreeTagger (Schmid, 1994) tokenizer ."},
{"id_case":"3725","class":"","sentence1":"We train our model on a subset of the WaCkypedia EN 6 corpus (Baroni et al., 2009).","sentence2":"We built a knowledge base (V 2 R) 1 using the frWaC corpus(Baroni et al., 2009)."},
{"id_case":"3726","class":"","sentence1":"The questions are translated using a phrase-based system built using Moses (Koehn et al., 2007) (the Mo set).","sentence2":"The first two baselines are standard systems using PBMT or Hiero trained using Moses (Koehn et al., 2007)."},
{"id_case":"3727","class":"","sentence1":"The questions are translated using a phrase-based system built using Moses (Koehn et al., 2007) (the Mo set).","sentence2":"The decoder is built on top of an open-source phrase-based SMT decoder, Moses (Koehn et al., 2007)."},
{"id_case":"3728","class":"","sentence1":"We measure statistical significance using 95% confidence intervals computed with paired bootstrap resampling (Koehn, 2004).","sentence2":"Significance was tested using a paired bootstrap (Koehn, 2004) with 1000 samples (p<0.02)."},
{"id_case":"3729","class":"","sentence1":"As for EJ translation, we use the Stanford parser (de Marneffe et al., 2006) to obtain English abstraction trees.","sentence2":"We use Stanford parser (de Marneffe et al., 2006) to obtain parse trees and dependency relations."},
{"id_case":"3730","class":"","sentence1":"Words were downcased and lemmatized using the WordNet lemmatizer in the NLTK 2 toolkit (Bird et al., 2009).","sentence2":"For Reuters we segmented and tokenized the data using NLTK (Bird et al., 2009)."},
{"id_case":"3731","class":"","sentence1":"English sentences are parsed into dependency structures by Stanford parser (Marneffe et al., 2006).","sentence2":"The grammatical relations are all the collapsed dependencies produced by the Stanford Dependency parser (Marneffe et al., 2006)."},
{"id_case":"3732","class":"","sentence1":"Both of our systems were based on the Moses decoder (Koehn et al., 2007).","sentence2":"The decoder is built on top of an open-source phrase-based SMT decoder, Moses (Koehn et al., 2007)."},
{"id_case":"3733","class":"","sentence1":"We also used ANEW (Bradley and Lang, 1999) for bootstrapping the affective lexicon expansion process.","sentence2":"Finally the ANEW lexicon (Bradley and Lang, 1999) is used for selecting the initial set of seed words of (1)."},
{"id_case":"3734","class":"","sentence1":"translation at the DiscoMT 2015 workshop (Hardmeier et al., 2015).","sentence2":"The NLP Group of the Idiap Research Institute participated in both sub-tasks of the DiscoMT 2015 Shared Task: pronoun-focused translation and pronoun prediction (Hardmeier et al., 2015)."},
{"id_case":"3735","class":"","sentence1":"Establishing and maintaining common ground is a complicated process, even for human interlocutors (Clark, 1996).","sentence2":"An example of such a pragmatic factor is common ground (Clark, 1996)."},
{"id_case":"3736","class":"","sentence1":"SentiWordNet score (senti) We used the Senti- WordNet (Baccianella et al., 2010) lexical resource to assign scores for each word based on three sentiments i.e positive, negative and objective respec","sentence2":"We then have assigned a sentiment score using the SentiWordNet (Baccianella et al., 2010) lexical resource to each word in the set of retrieved snippets."},
{"id_case":"3737","class":"","sentence1":"Europarl 2 (Koehn, 2005): it is a corpus of parallel texts in 11 languages from the proceedings of the European Parliament .","sentence2":"We used a subset of the data provided for the Second Workshop on Statistical Machine Translation 2 , which consists mainly of texts from the Europarl corpus (Koehn, 2005)."},
{"id_case":"3739","class":"","sentence1":"WSI is generally considered as an unsupervised clustering task under the distributional hypothesis (Harris, 1954) that the word meaning is reflected by the set of contexts in which it appears.","sentence2":"Distributional models of meaning follow the distributional hypothesis (Harris, 1954), which states that two words that occur in similar contexts have similar meanings."},
{"id_case":"3740","class":"","sentence1":"The data used for the experiments described in this paper comes predominantly from Bible translations , Wikipedia, and the Europarl corpus of European parliamentary proceedings (Koehn, 2005).","sentence2":"Europarl 2 (Koehn, 2005): it is a corpus of parallel texts in 11 languages from the proceedings of the European Parliament ."},
{"id_case":"3741","class":"","sentence1":"In a second step, we applied Pointwise Mutual Information (Church and Hanks, 1990) as a weighting function to discover informative semantic similarity relations between words.","sentence2":"Therefore, we propose a second baseline where pairs are rated according to their Pointwise Mutual Information (PMI) (Church and Hanks, 1990), which measures the statistical association between two words."},
{"id_case":"3748","class":"","sentence1":"The most famous example would probably be the Europarl corpus (Koehn, 2005).","sentence2":"For this purpose we use the Europarl corpus (Koehn, 2005)."},
{"id_case":"3749","class":"","sentence1":"For language modeling, we trained a separate 5-gram Kneser-Ney smoothed LM model on the target (i.e. English) side of the training bi-text using KenLM (Heafield, 2011).","sentence2":"In order to evaluate the fluency of each system, we train 5-gram language models for each language using KenLM (Heafield, 2011)."},
{"id_case":"3750","class":"","sentence1":"They used the Web-based annotation tool brat (Stenetorp et al., 2012) for the annotation .","sentence2":"The annotations were made using the BRAT rapid annotation tool (Stenetorp et al., 2012)."},
{"id_case":"3751","class":"","sentence1":"The classification was conducted, using different Scikit-learn algorithms (Pedregosa et al., 2011).","sentence2":"The SVM models were trained using the Scikit-learn toolkit 4 (Pedregosa et al., 2011)."},
{"id_case":"3752","class":"","sentence1":"The Europarl corpus (Koehn, 2005) is built from the proceedings of the European Parliament.","sentence2":"We used the English side of the Europarl corpus (Koehn, 2005)."},
{"id_case":"3753","class":"","sentence1":"For language modeling, we trained a separate 5-gram Kneser-Ney smoothed LM model on the target (i.e. English) side of the training bi-text using KenLM (Heafield, 2011).","sentence2":"The language model is a 5-gram KenLM (Heafield, 2011) model, trained using lmplz, with modified Kneser-Ney smoothing and no pruning."},
{"id_case":"3754","class":"","sentence1":"The Spanish-English (S2E) training corpus was drawn from the Europarl collection (Koehn, 2005).","sentence2":"We used the English side of the Europarl corpus (Koehn, 2005)."},
{"id_case":"3756","class":"","sentence1":"We use the Stanford CoreNLP caseless tagger for part-of-speech tagging (Manning et al., 2014).","sentence2":"We also lemmatized all words using Stanford CoreNLP (Manning et al., 2014)."},
{"id_case":"3757","class":"","sentence1":"All of our models are trained using Nematus (Sennrich et al., 2017).","sentence2":"We trained our basic neural machine translation systems (labeled base in Table 3) with Nematus (Sennrich et al., 2017)."},
{"id_case":"3758","class":"","sentence1":"We use GIZA++ (Och and Ney, 2003) with its default parameters to produce phrase alignments.","sentence2":"We used the GIZA++ software (Och and Ney, 2003) to do the word alignments."},
{"id_case":"3759","class":"","sentence1":"We are working with standard tools as DISSECT (Dinu et al., 2013).","sentence2":"The matrix is weighted with PPMI as implemented in DISSECT (Dinu et al., 2013)."},
{"id_case":"3760","class":"","sentence1":"They are based on Distributional Hypothesis which works under the assumption that similar words occur in similar contexts (Harris, 1954).","sentence2":"Corpus-based VSMs follow the standard \" distributional hypothesis, \" which states that words appearing in the same contexts tend to have similar meaning (Harris, 1954)."},
{"id_case":"3761","class":"","sentence1":"Its segmentation model is a class-based hidden Markov model (HMM) model (Zhang et al., 2003).","sentence2":"For Chinese, a segmentation model (Zhang et al., 2003) is used for detecting word boundaries."},
{"id_case":"3762","class":"","sentence1":"The edit distance kernel was trained with LIBSVM (Chang and Lin, 2011).","sentence2":"As our learner, we use LIBSVM with a linear kernel (Chang and Lin, 2011)."},
{"id_case":"3763","class":"","sentence1":"The corpora are first tokenized and lowercased using the Moses scripts, then lemmatized and tagged by part-of-speech (PoS) using the TreeTagger (Schmid, 1994).","sentence2":"The corpus was converted from XML to raw text, various string normalization operations were then applied, and the corpus was lemmatized using TreeTagger (Schmid, 1994)."},
{"id_case":"3764","class":"","sentence1":"For example, OntoNotes (Hovy et al., 2006), a large-scale annotation project, chose this option.","sentence2":"For example, the OntoNotes (Hovy et al., 2006) project opted for this approach."},
{"id_case":"3766","class":"","sentence1":"We use the open-source Moses toolkit (Koehn et al., 2007) to build a standard phrase-based SMT system which extracts up to 8 words phrases in the Moses phrase table.","sentence2":"The Moses toolkit (Koehn et al., 2007) is used to train a phrase based SMT system with the parallel data previously introduced."},
{"id_case":"3767","class":"","sentence1":"In our experiments, we use the LIBLINEAR package (Fan et al., 2008) to solve the primal problem with L 2 -regularization and L 2 -loss.","sentence2":"Specifically, we use the LIBLINEAR SVM package (Fan et al., 2008) as it is well-suited to text classification tasks with large numbers of features and texts."},
{"id_case":"3768","class":"","sentence1":"The Spanish-English (S2E) training corpus was drawn from the Europarl collection (Koehn, 2005).","sentence2":"For this purpose we use the Europarl corpus (Koehn, 2005)."},
{"id_case":"3769","class":"","sentence1":"The ICSI meeting corpus (Janin et al., 2003) is a corpus of text transcripts of research meetings.","sentence2":"The ICSI Meeting Corpus: The ICSI Meeting Corpus (Janin et al., 2003) is 75 transcribed meetings."},
{"id_case":"3770","class":"","sentence1":"We use ROUGE (Lin, 2004) for evaluating the content of summaries.","sentence2":"We use ROUGE metric (Lin, 2004) to evaluate generated timelines against reference summaries."},
{"id_case":"3771","class":"","sentence1":"The measure selected is the normalised Pearson correlation (Agirre et al., 2012).","sentence2":"The task is part of the Semantic Evaluation 2012 Workshop (Agirre et al., 2012)."},
{"id_case":"3772","class":"","sentence1":"The main corpora we use are Europarl (Koehn, 2005) and the Canadian Hansard.","sentence2":"For this purpose we use the Europarl corpus (Koehn, 2005)."},
{"id_case":"3773","class":"","sentence1":"We exploit this monolingual data for training as described in (Sennrich et al., 2016).","sentence2":"We also used automatically back-translated in-domain monolingual data (Sennrich et al., 2016)."},
{"id_case":"3774","class":"","sentence1":"We use the Stanford CoreNLP caseless tagger for part-of-speech tagging (Manning et al., 2014).","sentence2":"We also lemmatized all words using Stanford CoreNLP (Manning et al., 2014)."},
{"id_case":"3775","class":"","sentence1":"This is a corpus-based metric relying on the distributional hypothesis of meaning suggesting that similarity of context implies similarity of meaning (Z. Harris, 1954).","sentence2":"Corpus-based meaning representations rely on the distributional hypothesis, which assumes that words occurring in a similar set of contexts are also similar in meaning (Harris, 1954)."},
{"id_case":"3777","class":"","sentence1":"Reference System We compare a number of analogical devices to the state-of-the-art statistical translation engine Moses (Koehn et al., 2007).","sentence2":"We then use the phrase extraction utility in the Moses statistical machine translation system (Koehn et al., 2007) to extract a phrase table which operates over characters ."},
{"id_case":"3778","class":"","sentence1":"For training SVM classifiers we used LIBSVM package (Chang and Lin, 2001).","sentence2":"We used LIBSVM to implement our own SVM for regression (Chang and Lin, 2001)."},
{"id_case":"3779","class":"","sentence1":"The word alignment was trained using GIZA++ (Och and Ney, 2003) with the configuration grow-diag-final-and.","sentence2":"The word alignment was obtained by running Giza++ (Och and Ney, 2003)."},
{"id_case":"3781","class":"","sentence1":"The corpus is first word-aligned using a word alignment heuristic (Och and Ney, 2003).","sentence2":"The word alignment was obtained by running Giza++ (Och and Ney, 2003)."},
{"id_case":"3782","class":"","sentence1":"We perform bootstrap resampling with bounds estimation as described in (Koehn, 2004).","sentence2":"We used bootstrap resampling for testing statistical significance (Koehn, 2004)."},
{"id_case":"3784","class":"","sentence1":"We perform bootstrap resampling with bounds estimation as described by (Koehn, 2004).","sentence2":"We used bootstrap resampling for testing statistical significance (Koehn, 2004)."},
{"id_case":"3785","class":"","sentence1":"Markov Logic Networks (MLN) (Richardson and Domingos, 2006) is adopted for learning and predication.","sentence2":"Markov Logic Networks (MLN) (Richardson and Domingos, 2006) are one of the statistical relational learning frameworks."},
{"id_case":"3789","class":"","sentence1":"For this purpose we use the Europarl corpus (Koehn, 2005).","sentence2":"We evaluate our method by means of the Europarl corpus (Koehn, 2005)."},
{"id_case":"3790","class":"","sentence1":"We exploit this monolingual data for training as described in (Sennrich et al., 2016).","sentence2":"We also used automatically back-translated in-domain monolingual data (Sennrich et al., 2016)."},
{"id_case":"3792","class":"","sentence1":"For the English- Spanish and French-English systems, we used parallel training data from the Europarl and News Commentary corpora, as well as the TED corpus (Cettolo et al., 2012).","sentence2":"In our low-resource condition, we trained an SMT system using only training data from the TED corpus (Cettolo et al., 2012)."},
{"id_case":"3794","class":"","sentence1":"The word alignment was trained using GIZA++ (Och and Ney, 2003) with the configuration grow-diag-final-and.","sentence2":"Word alignment was done with GIZA++ (Och and Ney, 2003) for both systems."},
{"id_case":"3795","class":"","sentence1":"Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) represents the coherence structure of a text by a labeled tree, called discourse tree (DT) as shown in Figure 1 .","sentence2":"The closest area to our work consists of investigations of discourse relations in the context of Rhetorical Structure Theory (Mann and Thompson, 1988)."},
{"id_case":"3797","class":"","sentence1":"A good data source for this is the Europarl Corpus (Koehn, 2005).","sentence2":"For this purpose we use the Europarl corpus (Koehn, 2005)."},
{"id_case":"3798","class":"","sentence1":"The most famous example would probably be the Europarl corpus (Koehn, 2005).","sentence2":"We used the English side of the Europarl corpus (Koehn, 2005)."},
{"id_case":"3806","class":"","sentence1":"We follow the state of the art word-based system (Yatbaz et al., 2012) and use probable substitutes of a word instance as its contextual features.","sentence2":"We represent the context of a word by constructing substitute vectors using possible substitutes of the word as (Yatbaz et al., 2012) suggests."},
{"id_case":"3807","class":"","sentence1":"Our system is a fully characterto-character neural MT (Lee et al., 2016) system with additional rescoring from the inverse direction model.","sentence2":"Our system uses the architecture from (Lee et al., 2016) where a character-level neural MT model maps the source character sequence to the target character sequence."}
]
}
]
